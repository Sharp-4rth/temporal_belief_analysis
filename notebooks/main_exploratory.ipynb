{
  "cells": [
    {
      "metadata": {
        "id": "a6134511ae0d06fa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Clone Repo\n",
        "!git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git"
      ],
      "id": "a6134511ae0d06fa"
    },
    {
      "metadata": {
        "id": "ca9d6e7e2597e920"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Need to restart after:\n",
        "!pip install convokit[llm]\n",
        "!pip install convokit\n",
        "!pip install statsmodels"
      ],
      "id": "ca9d6e7e2597e920"
    },
    {
      "metadata": {
        "id": "8da55431d4e44ab8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import sys\n",
        "import os\n",
        "os.chdir('/content/temporal_belief_analysis/notebooks')\n",
        "print(\"Changed working directory to:\", os.getcwd())\n",
        "# Absolute path to src directory\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ],
      "id": "8da55431d4e44ab8"
    },
    {
      "metadata": {
        "id": "58e4db9f6ee972cd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import time\n",
        "!pip install gdown\n",
        "import zipfile\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from convokit import Corpus, download\n",
        "import convokit\n",
        "from temporal_belief.core.timeline_building import TimelineBuilder\n",
        "from temporal_belief.core.change_detection import ChangeDetector\n",
        "from temporal_belief.core.window_extraction import WindowExtractor\n",
        "from temporal_belief.core.op_path_pairing import OpPathPairer\n",
        "from temporal_belief.data.preprocessors import ChangeDetectorPreprocessor\n",
        "from temporal_belief.data.preprocessors import PairPreprocessor\n",
        "from temporal_belief.data.preprocessors import ExtractFeatures\n",
        "from temporal_belief.data.preprocessors import GroupPreprocessor\n",
        "from temporal_belief.core.interplay import Interplay\n",
        "import numpy as np\n",
        "nltk.download('stopwords')"
      ],
      "id": "58e4db9f6ee972cd"
    },
    {
      "metadata": {
        "id": "d80eb0bf377b9a19"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Download 'r/PoliticalDiscussion' corpus with stance labels to root, unzip with python:\n",
        "!gdown \"https://drive.google.com/file/d/1AIrstrzE259fcVyxJQW4-RwvAkoUyK1x/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\" --fuzzy\n",
        "zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\").extractall(\"/content/temporal_belief_analysis\")"
      ],
      "id": "d80eb0bf377b9a19"
    },
    {
      "metadata": {
        "id": "d35b2192da8b17fe"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "CORPUS_PATH = \"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned\"\n",
        "corpus = Corpus(filename=CORPUS_PATH)"
      ],
      "id": "d35b2192da8b17fe"
    },
    {
      "metadata": {
        "id": "c6ac08ef2dd321bc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 13,
      "source": [
        "# Build per-user topic timelines:\n",
        "timeline_builder = TimelineBuilder(corpus)\n",
        "timelines = timeline_builder.build_timelines()"
      ],
      "id": "c6ac08ef2dd321bc"
    },
    {
      "cell_type": "code",
      "source": [
        "# A user to test with:\n",
        "user_id = \"HardCoreModerate\"\n",
        "topic = \"Process & Actors (Meta)\"\n",
        "topic_timeline = timelines[user_id][topic]\n",
        "topic_timeline = list(topic_timeline.items())"
      ],
      "metadata": {
        "id": "jCOmvdmna8i8"
      },
      "id": "jCOmvdmna8i8",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "change_detector = ChangeDetector()\n",
        "change_point = change_detector.detect_cusum_changes(topic_timeline)['change_points']\n",
        "print(change_point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIUwW1bIWoej",
        "outputId": "16bfc94f-ae3d-4192-a54d-9141b616f627"
      },
      "id": "pIUwW1bIWoej",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c4w912m']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_extractor = WindowExtractor(corpus, timelines)\n",
        "# window_extractor.build_global_user_conversations_index()\n",
        "candidate_convos = window_extractor.get_conversations_around_change_point(change_point=change_point, corpus=corpus)\n",
        "# print(f\"Candidate convos: {[convo for convo in candidate_convos]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "CRcQABhbWq4V",
        "outputId": "303101e3-b10e-4e21-9b5d-d6a60620ed3f"
      },
      "id": "CRcQABhbWq4V",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1827766899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindowExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimelines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# window_extractor.build_global_user_conversations_index()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcandidate_convos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_conversations_around_change_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchange_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(f\"Candidate convos: {[convo for convo in candidate_convos]}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/temporal_belief_analysis/src/temporal_belief/core/window_extraction.py\u001b[0m in \u001b[0;36mget_conversations_around_change_point\u001b[0;34m(self, corpus, change_point, test, window)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_conversations_around_change_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Get first change (probably only one I need)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mutterance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Find the convo this utterance belongs to:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/convokit/model/corpus.py\u001b[0m in \u001b[0;36mget_utterance\u001b[0;34m(self, utt_id)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUtterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutterances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mutt_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvo_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mConversation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op_path_pairer = OpPathPairer(corpus, timelines)\n",
        "pair_preprocessor = PairPreprocessor()"
      ],
      "metadata": {
        "id": "Ze4NJJFSZROC"
      },
      "id": "Ze4NJJFSZROC",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a27bafbebffd6590"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "groups_preprocessor = GroupPreprocessor()\n",
        "groups = change_detector.get_two_groups(timelines)\n",
        "groups_tuple = (groups['with_changes'], groups['no_changes'])\n",
        "groups_tuple = groups_preprocessor.filter_groups(groups, groups_tuple)\n",
        "target_utterances = groups_preprocessor.get_target(groups_tuple)"
      ],
      "id": "a27bafbebffd6590"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = ExtractFeatures()\n",
        "persuasion_analyzer = Interplay()"
      ],
      "metadata": {
        "id": "7xCwjX-5ZYd5"
      },
      "id": "7xCwjX-5ZYd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5efbbbf0a1d8f24b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from tqdm import tqdm\n",
        "stop_words_set = set(stopwords.words('english'))\n",
        "\n",
        "group_means = []\n",
        "group_scores = []\n",
        "\n",
        "# For each group\n",
        "for group_idx, group in enumerate(tqdm(groups_tuple, desc=\"Processing groups\")):\n",
        "    # Initialize dictionary for this group's scores (one score per utterance)\n",
        "    current_group_scores = {\n",
        "        'interplay': [],\n",
        "        'politeness': [],\n",
        "        'argument_complexity': [],\n",
        "        'evidence_markers': [],\n",
        "        'hedging': []\n",
        "    }\n",
        "\n",
        "    utterances_processed = 0\n",
        "    target_reached = False  # Flag to control all nested loops\n",
        "\n",
        "    for user_id, topic_timelines in group.items():\n",
        "        if target_reached:  # Check flag at user level\n",
        "            break\n",
        "\n",
        "        user_start_time = time.time()\n",
        "        user_change_points = 0\n",
        "\n",
        "        for topic_timeline in topic_timelines.values():\n",
        "            if target_reached:  # Check flag at topic level\n",
        "                break\n",
        "\n",
        "            for change_point in topic_timeline.keys():  # Each utterance/change point\n",
        "                if utterances_processed >= target_utterances:\n",
        "                    target_reached = True  # Set flag instead of just breaking\n",
        "                    break\n",
        "\n",
        "                # utts_num += 1\n",
        "                user_change_points += 1\n",
        "                utterances_processed += 1\n",
        "\n",
        "                # Window extraction\n",
        "                start_time = time.time()\n",
        "                try:\n",
        "                    candidate_convos = window_extractor.get_conversations_around_change_point(\n",
        "                        change_point=change_point, corpus=corpus, test=True\n",
        "                    )\n",
        "                    window_time = time.time() - start_time\n",
        "                    print(f'Window extraction: {window_time:.3f}s')\n",
        "                except ValueError as e:\n",
        "                    print(f\"Skipping change point {change_point}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Path extraction\n",
        "                start_time = time.time()\n",
        "                timeout_duration = 0.25\n",
        "                op_path_pairs = []\n",
        "\n",
        "                for candidate_convo in candidate_convos:\n",
        "                    if time.time() - start_time > timeout_duration:\n",
        "                        print(f\"Path extraction timeout reached ({timeout_duration}s)\")\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        op_path_pairs.extend(op_path_pairer.extract_rooted_path_from_candidate_convos(\n",
        "                            [candidate_convo], user_id\n",
        "                        ))\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Skipping conversation {candidate_convo.id}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                path_time = time.time() - start_time\n",
        "                print(f'Path extraction: {path_time:.3f}s')\n",
        "\n",
        "                # Preprocessing\n",
        "                start_time = time.time()\n",
        "                preprocessed_pairs = pair_preprocessor.concatenate_path_in_all_pairs(op_path_pairs)\n",
        "                preprocess_time = time.time() - start_time\n",
        "                print(f'Preprocessing: {preprocess_time:.3f}s')\n",
        "\n",
        "                # Feature extraction - collect ALL scores for this utterance\n",
        "                start_time = time.time()\n",
        "                utterance_interplay_scores = []\n",
        "                utterance_politeness_scores = []\n",
        "                utterance_complexity_scores = []\n",
        "                utterance_evidence_scores = []\n",
        "                utterance_hedging_scores = []\n",
        "\n",
        "                for op, paths in preprocessed_pairs:\n",
        "                    for k, concatenated_utts in paths.items():\n",
        "                        # Extract features\n",
        "                        interplay_features = persuasion_analyzer.calculate_interplay_features(\n",
        "                            op.text, concatenated_utts, stop_words_set\n",
        "                        )\n",
        "                        politeness_features = feature_extractor.get_politeness_features(concatenated_utts)\n",
        "                        complexity_features = feature_extractor.extract_argument_complexity_features(concatenated_utts)\n",
        "                        evidence_features = feature_extractor.extract_evidence_features(concatenated_utts)\n",
        "                        hedging_features = feature_extractor.extract_hedging_features(concatenated_utts)\n",
        "\n",
        "                        # Calculate scores\n",
        "                        interplay_score = persuasion_analyzer.calculate_persuasion_score(interplay_features)\n",
        "                        politeness_score = sum(politeness_features.values())\n",
        "                        complexity_score = feature_extractor.calculate_complexity_score(complexity_features)\n",
        "                        evidence_score = feature_extractor.calculate_evidence_score(evidence_features)\n",
        "                        hedging_score = feature_extractor.calculate_hedging_score_from_features(hedging_features)\n",
        "\n",
        "                        # Collect all scores for this utterance\n",
        "                        utterance_interplay_scores.append(interplay_score)\n",
        "                        utterance_politeness_scores.append(politeness_score)\n",
        "                        utterance_complexity_scores.append(complexity_score)\n",
        "                        utterance_evidence_scores.append(evidence_score)\n",
        "                        utterance_hedging_scores.append(hedging_score)\n",
        "\n",
        "                feature_time = time.time() - start_time\n",
        "\n",
        "                # Take mean across all paths for this single utterance\n",
        "                start_time = time.time()\n",
        "                if utterance_interplay_scores:  # Only if we have scores\n",
        "                    # One score per utterance (mean of all conversation paths)\n",
        "                    utterance_mean_interplay = np.mean(utterance_interplay_scores)\n",
        "                    utterance_mean_politeness = np.mean(utterance_politeness_scores)\n",
        "                    utterance_mean_complexity = np.mean(utterance_complexity_scores)\n",
        "                    utterance_mean_evidence = np.mean(utterance_evidence_scores)\n",
        "                    utterance_mean_hedging = np.mean(utterance_hedging_scores)\n",
        "\n",
        "                    # Add ONE score per utterance to group scores\n",
        "                    current_group_scores['interplay'].append(utterance_mean_interplay)\n",
        "                    current_group_scores['politeness'].append(utterance_mean_politeness)\n",
        "                    current_group_scores['argument_complexity'].append(utterance_mean_complexity)\n",
        "                    current_group_scores['evidence_markers'].append(utterance_mean_evidence)\n",
        "                    current_group_scores['hedging'].append(utterance_mean_hedging)\n",
        "\n",
        "                    print(f\"Utterance {change_point}: {len(utterance_interplay_scores)} paths -> 1 mean score\")\n",
        "                    print(f\"Group {group_idx + 1}: {utterances_processed}/{target_utterances} utterances processed\")\n",
        "                else:\n",
        "                    print(f\"Utterance {change_point}: No valid paths found, skipping\")\n",
        "\n",
        "                scoring_time = time.time() - start_time\n",
        "                print(f'Scoring: {scoring_time:.3f}s')\n",
        "\n",
        "                # Print total time for this change point\n",
        "                total_time = window_time + path_time + preprocess_time + feature_time + scoring_time\n",
        "                print(f'TOTAL for utterance: {total_time:.3f}s\\n')\n",
        "\n",
        "        user_total_time = time.time() - user_start_time\n",
        "        if user_change_points > 0:  # Only print if user had utterances\n",
        "            print(f'USER {user_id}: {user_total_time:.3f}s ({user_change_points} utterances)')\n",
        "\n",
        "    # Calculate means for each predictor for this group\n",
        "    group_mean = {}\n",
        "    for predictor_name, scores in current_group_scores.items():\n",
        "        if scores:\n",
        "            group_mean[predictor_name] = np.mean(scores)\n",
        "        else:\n",
        "            group_mean[predictor_name] = 0\n",
        "\n",
        "    print(f\"\\nGroup {group_idx + 1} final sample sizes:\")\n",
        "    for predictor_name, scores in current_group_scores.items():\n",
        "        print(f\"  {predictor_name}: n={len(scores)}\")\n",
        "\n",
        "    print(f\"Group {group_idx + 1}: Processed exactly {utterances_processed} utterances\")\n",
        "\n",
        "    group_means.append(group_mean)\n",
        "    group_scores.append(current_group_scores)"
      ],
      "id": "5efbbbf0a1d8f24b"
    },
    {
      "metadata": {
        "id": "1ba2bc1fa33d0696"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "groups_preprocessor.run_statistical_comparison(group_scores)"
      ],
      "id": "1ba2bc1fa33d0696"
    },
    {
      "metadata": {
        "id": "68b11687c9dc32be"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "68b11687c9dc32be"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}