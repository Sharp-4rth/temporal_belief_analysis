{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T08:09:51.278612Z",
     "start_time": "2025-07-14T08:08:08.657634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Need to restart after:\n",
    "!pip install convokit"
   ],
   "id": "bbad0be784b4869a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting convokit\r\n",
      "  Using cached convokit-3.3.0.tar.gz (206 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting matplotlib>=3.0.0 (from convokit)\r\n",
      "  Using cached matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting scipy>1.14 (from convokit)\r\n",
      "  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "Collecting pandas>=1.5.0 (from convokit)\r\n",
      "  Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\r\n",
      "Collecting numpy>=2.0.0 (from convokit)\r\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\r\n",
      "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\r\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Collecting spacy>=3.8.2 (from convokit)\r\n",
      "  Using cached spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\r\n",
      "Collecting scikit-learn>=1.0 (from convokit)\r\n",
      "  Using cached scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\r\n",
      "Collecting nltk>=3.4 (from convokit)\r\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting dill>=0.2.9 (from convokit)\r\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting joblib>=0.13.2 (from convokit)\r\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\r\n",
      "Collecting clean-text>=0.6.0 (from convokit)\r\n",
      "  Using cached clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\r\n",
      "Collecting unidecode>=1.1.1 (from convokit)\r\n",
      "  Using cached Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting tqdm>=4.64.0 (from convokit)\r\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting pymongo>=4.0 (from convokit)\r\n",
      "  Using cached pymongo-4.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\r\n",
      "Collecting dnspython>=1.16.0 (from convokit)\r\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from convokit)\r\n",
      "  Using cached thinc-8.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\r\n",
      "Collecting h5py==3.12.1 (from convokit)\r\n",
      "  Using cached h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\r\n",
      "Collecting numexpr>=2.8.0 (from convokit)\r\n",
      "  Using cached numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\r\n",
      "Collecting ruff>=0.4.8 (from convokit)\r\n",
      "  Using cached ruff-0.12.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting bottleneck (from convokit)\r\n",
      "  Using cached bottleneck-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\r\n",
      "Collecting accelerate (from convokit)\r\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting peft (from convokit)\r\n",
      "  Using cached peft-0.16.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting bitsandbytes (from convokit)\r\n",
      "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\r\n",
      "Collecting transformers (from convokit)\r\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\r\n",
      "Collecting unsloth (from convokit)\r\n",
      "  Using cached unsloth-2025.7.3-py3-none-any.whl.metadata (47 kB)\r\n",
      "Collecting trl>=0.12.2 (from convokit)\r\n",
      "  Using cached trl-0.19.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting tensorflow>=2.18.0 (from convokit)\r\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting tf-keras<3.0.0,>=2.17.0 (from convokit)\r\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\r\n",
      "  Using cached emoji-1.7.0.tar.gz (175 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\r\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.0.0->convokit)\r\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.0.0->convokit)\r\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.0.0->convokit)\r\n",
      "  Using cached fonttools-4.58.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\r\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.0.0->convokit)\r\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\r\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.0.0->convokit) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\r\n",
      "Collecting msgpack>=0.5.2 (from msgpack-numpy>=0.4.3.2->convokit)\r\n",
      "  Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\r\n",
      "Collecting click (from nltk>=3.4->convokit)\r\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.4->convokit)\r\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\n",
      "Collecting pytz>=2020.1 (from pandas>=1.5.0->convokit)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.5.0->convokit)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0->convokit)\r\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\r\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\r\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\r\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\r\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\r\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (2.31.0)\r\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.8.2->convokit) (68.2.2)\r\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\r\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\r\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\r\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\r\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow>=2.18.0->convokit) (1.16.0)\r\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.18.0->convokit) (4.4.0)\r\n",
      "Collecting wrapt>=1.11.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\r\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting keras>=3.5.0 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\r\n",
      "Collecting numpy>=2.0.0 (from convokit)\r\n",
      "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\r\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\r\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.0->convokit)\r\n",
      "  Using cached blis-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\r\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->convokit)\r\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting datasets>=3.0.0 (from trl>=0.12.2->convokit)\r\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->convokit) (5.9.6)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->convokit) (2.1.0+cu118)\r\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate->convokit)\r\n",
      "  Using cached huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from accelerate->convokit)\r\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->convokit) (3.9.0)\r\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->convokit)\r\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting torch>=2.0.0 (from accelerate->convokit)\r\n",
      "  Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\r\n",
      "Collecting unsloth_zoo>=2025.7.4 (from unsloth->convokit)\r\n",
      "  Using cached unsloth_zoo-2025.7.4-py3-none-any.whl.metadata (8.1 kB)\r\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth->convokit)\r\n",
      "  Using cached xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\r\n",
      "Collecting triton>=3.0.0 (from unsloth->convokit)\r\n",
      "  Using cached triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting tyro (from unsloth->convokit)\r\n",
      "  Using cached tyro-0.9.26-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting datasets>=3.0.0 (from trl>=0.12.2->convokit)\r\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth->convokit)\r\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n",
      "Collecting wheel>=0.42.0 (from unsloth->convokit)\r\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting hf_transfer (from unsloth->convokit)\r\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting diffusers (from unsloth->convokit)\r\n",
      "  Using cached diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth->convokit) (0.16.0+cu118)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill>=0.2.9 (from convokit)\r\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy>=3.8.2->convokit)\r\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting xxhash (from datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\r\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit) (2023.4.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.9)\r\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.21.0->accelerate->convokit)\r\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\r\n",
      "Collecting rich (from keras>=3.5.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting namex (from keras>=3.5.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\r\n",
      "Collecting optree (from keras>=3.5.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\r\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit)\r\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit)\r\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit)\r\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit)\r\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (1.26.13)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2022.12.7)\r\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\r\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate->convokit) (3.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.0.0->accelerate->convokit)\r\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit)\r\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\r\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.7.4->unsloth->convokit)\r\n",
      "  Using cached cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\r\n",
      "Collecting msgspec (from unsloth_zoo>=2025.7.4->unsloth->convokit)\r\n",
      "  Using cached msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\r\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit)\r\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit)\r\n",
      "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\r\n",
      "Requirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth->convokit) (4.6.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.8.2->convokit) (2.1.2)\r\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting torchvision (from unsloth->convokit)\r\n",
      "  Using cached torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\r\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth->convokit)\r\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth->convokit)\r\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\r\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth->convokit)\r\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\r\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit)\r\n",
      "  Using cached marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\r\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.18.0->convokit) (2.16.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate->convokit) (1.3.0)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit) (23.1.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.12.2->convokit)\r\n",
      "  Using cached yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\r\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.18.0->convokit)\r\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Using cached h5py-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\r\n",
      "Using cached clean_text-0.6.0-py3-none-any.whl (11 kB)\r\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\r\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\r\n",
      "Using cached matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\r\n",
      "Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\r\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "Using cached numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (399 kB)\r\n",
      "Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\r\n",
      "Using cached pymongo-4.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "Using cached ruff-0.12.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\r\n",
      "Using cached scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\r\n",
      "Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\r\n",
      "Using cached spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\r\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\r\n",
      "Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\r\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\r\n",
      "Using cached thinc-8.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\r\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Using cached trl-0.19.1-py3-none-any.whl (376 kB)\r\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\r\n",
      "Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\r\n",
      "Using cached Unidecode-1.4.0-py3-none-any.whl (235 kB)\r\n",
      "Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\r\n",
      "Using cached bottleneck-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (357 kB)\r\n",
      "Using cached peft-0.16.0-py3-none-any.whl (472 kB)\r\n",
      "Using cached unsloth-2025.7.3-py3-none-any.whl (297 kB)\r\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\r\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Using cached blis-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\r\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\r\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\r\n",
      "Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\r\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Using cached cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\r\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\r\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\r\n",
      "Using cached fonttools-4.58.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\r\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\r\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "Using cached grpcio-1.73.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\r\n",
      "Using cached huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\r\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\r\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\r\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\r\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\r\n",
      "Using cached ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\r\n",
      "Using cached msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\r\n",
      "Using cached murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\r\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\r\n",
      "Using cached preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\r\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\r\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\r\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\r\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\r\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\r\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\r\n",
      "Using cached srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\r\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\r\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\r\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\r\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "Using cached torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\r\n",
      "Using cached triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\r\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\r\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\r\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\r\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\r\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\r\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\r\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\r\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\r\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\r\n",
      "Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m216.6/216.6 MB\u001B[0m \u001B[31m65.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m156.8/156.8 MB\u001B[0m \u001B[31m81.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m201.3/201.3 MB\u001B[0m \u001B[31m68.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.7/19.7 MB\u001B[0m \u001B[31m228.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m89.3/89.3 kB\u001B[0m \u001B[31m51.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.3/46.3 kB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.2/102.2 kB\u001B[0m \u001B[31m61.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.9/43.9 kB\u001B[0m \u001B[31m27.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m347.8/347.8 kB\u001B[0m \u001B[31m147.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading unsloth_zoo-2025.7.4-py3-none-any.whl (162 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.8/162.8 kB\u001B[0m \u001B[31m92.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m229.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\r\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.3/50.3 kB\u001B[0m \u001B[31m18.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached wheel-0.45.1-py3-none-any.whl (72 kB)\r\n",
      "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.8/82.8 kB\u001B[0m \u001B[31m47.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m117.1/117.1 MB\u001B[0m \u001B[31m95.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading diffusers-0.34.0-py3-none-any.whl (3.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.8/3.8 MB\u001B[0m \u001B[31m255.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m262.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl (7.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.5/7.5 MB\u001B[0m \u001B[31m274.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tyro-0.9.26-py3-none-any.whl (128 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m129.0/129.0 kB\u001B[0m \u001B[31m68.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m52.8/52.8 kB\u001B[0m \u001B[31m31.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\r\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.6/193.6 kB\u001B[0m \u001B[31m104.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m250.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m267.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.8/106.8 kB\u001B[0m \u001B[31m57.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m75.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.3/42.3 MB\u001B[0m \u001B[31m183.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m243.2/243.2 kB\u001B[0m \u001B[31m110.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\r\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\r\n",
      "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.9/61.9 kB\u001B[0m \u001B[31m36.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m276.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.6/6.6 MB\u001B[0m \u001B[31m278.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\r\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m224.5/224.5 kB\u001B[0m \u001B[31m107.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\r\n",
      "Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.6/211.6 kB\u001B[0m \u001B[31m112.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\r\n",
      "Downloading optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m405.8/405.8 kB\u001B[0m \u001B[31m161.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m107.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m244.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m239.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.5/87.5 kB\u001B[0m \u001B[31m59.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\r\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m222.9/222.9 kB\u001B[0m \u001B[31m118.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\n",
      "Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m241.6/241.6 kB\u001B[0m \u001B[31m120.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m198.3/198.3 kB\u001B[0m \u001B[31m106.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m326.1/326.1 kB\u001B[0m \u001B[31m142.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: convokit, emoji\r\n",
      "  Building wheel for convokit (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for convokit: filename=convokit-3.3.0-py3-none-any.whl size=244589 sha256=b60a5c1e42408402527295171d1aa9867e635826e214b555592a0b711ba1b55a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/27/aa/45df7fb557bf6c9e88f6a1052906159af648ec6cf26f8ae7f0\r\n",
      "  Building wheel for emoji (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171031 sha256=083b407c4021a203b587ee6341271943b8969ecb4714effaa0521e90980d27be\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\r\n",
      "Successfully built convokit emoji\r\n",
      "Installing collected packages: sentencepiece, pytz, nvidia-cusparselt-cu12, namex, libclang, flatbuffers, emoji, cymem, xxhash, wrapt, wheel, werkzeug, wasabi, unidecode, tzdata, typing-extensions, triton, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, sympy, spacy-loggers, spacy-legacy, shtab, shellingham, safetensors, ruff, requests, regex, pyarrow, protobuf, propcache, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, murmurhash, msgspec, msgpack, mdurl, markdown, marisa-trie, kiwisolver, joblib, hf-xet, hf_transfer, grpcio, google-pasta, gast, ftfy, fsspec, frozenlist, fonttools, docstring-parser, dnspython, dill, cycler, click, catalogue, async-timeout, annotated-types, aiohappyeyeballs, absl-py, typing-inspection, typeguard, tensorboard, srsly, smart-open, scipy, pymongo, pydantic-core, preshed, pandas, optree, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numexpr, nltk, multiprocess, multidict, msgpack-numpy, ml-dtypes, markdown-it-py, language-data, huggingface_hub, h5py, contourpy, cloudpathlib, clean-text, bottleneck, blis, astunparse, aiosignal, yarl, tokenizers, scikit-learn, rich, pydantic, nvidia-cusolver-cu12, matplotlib, langcodes, diffusers, tyro, typer, transformers, torch, keras, confection, aiohttp, xformers, weasel, torchvision, thinc, tensorflow, cut_cross_entropy, bitsandbytes, accelerate, tf-keras, spacy, peft, datasets, trl, unsloth_zoo, unsloth, convokit\r\n",
      "  Attempting uninstall: wheel\r\n",
      "    Found existing installation: wheel 0.41.3\r\n",
      "    Uninstalling wheel-0.41.3:\r\n",
      "      Successfully uninstalled wheel-0.41.3\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.4.0\r\n",
      "    Uninstalling typing_extensions-4.4.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.4.0\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 2.1.0\r\n",
      "    Uninstalling triton-2.1.0:\r\n",
      "      Successfully uninstalled triton-2.1.0\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.12\r\n",
      "    Uninstalling sympy-1.12:\r\n",
      "      Successfully uninstalled sympy-1.12\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.31.0\r\n",
      "    Uninstalling requests-2.31.0:\r\n",
      "      Successfully uninstalled requests-2.31.0\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.24.1\r\n",
      "    Uninstalling numpy-1.24.1:\r\n",
      "      Successfully uninstalled numpy-1.24.1\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2023.4.0\r\n",
      "    Uninstalling fsspec-2023.4.0:\r\n",
      "      Successfully uninstalled fsspec-2023.4.0\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.0+cu118\r\n",
      "    Uninstalling torch-2.1.0+cu118:\r\n",
      "      Successfully uninstalled torch-2.1.0+cu118\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.16.0+cu118\r\n",
      "    Uninstalling torchvision-0.16.0+cu118:\r\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed absl-py-2.3.1 accelerate-1.8.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 annotated-types-0.7.0 astunparse-1.6.3 async-timeout-5.0.1 bitsandbytes-0.46.1 blis-1.3.0 bottleneck-1.5.0 catalogue-2.0.10 clean-text-0.6.0 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 contourpy-1.3.2 convokit-3.3.0 cut_cross_entropy-25.1.1 cycler-0.12.1 cymem-2.0.11 datasets-3.6.0 diffusers-0.34.0 dill-0.3.8 dnspython-2.7.0 docstring-parser-0.16 emoji-1.7.0 flatbuffers-25.2.10 fonttools-4.58.5 frozenlist-1.7.0 fsspec-2025.3.0 ftfy-6.3.1 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.12.1 hf-xet-1.1.5 hf_transfer-0.1.9 huggingface_hub-0.33.4 joblib-1.5.1 keras-3.10.0 kiwisolver-1.4.8 langcodes-3.5.0 language-data-1.3.0 libclang-18.1.1 marisa-trie-1.2.1 markdown-3.8.2 markdown-it-py-3.0.0 matplotlib-3.10.3 mdurl-0.1.2 ml-dtypes-0.5.1 msgpack-1.1.1 msgpack-numpy-0.4.8 msgspec-0.19.0 multidict-6.6.3 multiprocess-0.70.16 murmurhash-1.0.13 namex-0.1.0 nltk-3.9.1 numexpr-2.11.0 numpy-2.1.3 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opt-einsum-3.4.0 optree-0.16.0 pandas-2.3.1 peft-0.16.0 preshed-3.0.10 propcache-0.3.2 protobuf-3.20.3 pyarrow-20.0.0 pydantic-2.11.7 pydantic-core-2.33.2 pymongo-4.13.2 pytz-2025.2 regex-2024.11.6 requests-2.32.4 rich-14.0.0 ruff-0.12.3 safetensors-0.5.3 scikit-learn-1.7.0 scipy-1.15.3 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.2 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.14.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 tf-keras-2.19.0 thinc-8.3.6 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.7.1 torchvision-0.22.1 tqdm-4.67.1 transformers-4.53.2 triton-3.3.1 trl-0.19.1 typeguard-4.4.4 typer-0.16.0 typing-extensions-4.14.1 typing-inspection-0.4.1 tyro-0.9.26 tzdata-2025.2 unidecode-1.4.0 unsloth-2025.7.3 unsloth_zoo-2025.7.4 wasabi-1.1.3 weasel-0.4.1 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2 xformers-0.0.31.post1 xxhash-3.5.0 yarl-1.20.1\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For runpod-jupyter or local (run twice)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Change to the correct working directory (same as Jupyter)\n",
    "os.chdir('/workspace/temporal_belief_analysis/notebooks')\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "# Absolute path to src directory\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from temporal_belief.core.timeline_building import TimelineBuilder"
   ],
   "id": "d36660ce8a4d736b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run twice\n",
    "# import unsloth\n",
    "# import unsloth_zoo\n",
    "from convokit import Corpus, download\n",
    "import convokit"
   ],
   "id": "22220bf09f61753c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T10:26:40.933464Z",
     "start_time": "2025-07-14T10:23:26.387524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load a corpus:\n",
    "# corpus = Corpus(filename=\"/Users/leonidas/.convokit/saved-corpora/pd_corpus_with_stances1000_chronological\")\n",
    "corpus = Corpus(filename=\"/workspace/temporal_belief_analysis/pd_corpus_with_stances100000_chronological\")"
   ],
   "id": "781d4e0509ad3c21",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# OPTIONAL TO ADD LATER:\n",
    "# # 1. Better logging/reporting\n",
    "# def generate_analysis_report(self, results):\n",
    "#     \"\"\"Generate analysis summary\"\"\"\n",
    "#\n",
    "# # 2. Visualization\n",
    "# def plot_belief_timeline(self, user_timeline, detected_changes):\n",
    "#     \"\"\"Plot belief evolution with change points marked\"\"\"\n",
    "#\n",
    "# # 3. Effect size calculation\n",
    "# def calculate_effect_size(self, left_window, right_window):\n",
    "#     \"\"\"Cohen's d or similar for practical significance\"\"\"\n",
    "#\n",
    "# # 4. Confidence intervals\n",
    "# def belief_change_confidence_interval(self, change_data):\n",
    "#     \"\"\"95% CI for magnitude of belief change\"\"\""
   ],
   "id": "bf129eb03b4e3bc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T08:17:28.390010Z",
     "start_time": "2025-07-14T08:17:20.788691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install scipy\n",
    "!pip install statsmodels"
   ],
   "id": "ad611eb7998835c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.15.3)\r\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (2.1.3)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\r\n",
      "Collecting statsmodels\r\n",
      "  Downloading statsmodels-0.14.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\r\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.1.3)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.15.3)\r\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.3.1)\r\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\r\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (23.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\r\n",
      "Downloading statsmodels-0.14.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.7/10.7 MB\u001B[0m \u001B[31m193.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m:01\u001B[0m\r\n",
      "\u001B[?25hDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m232.9/232.9 kB\u001B[0m \u001B[31m117.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: patsy, statsmodels\r\n",
      "Successfully installed patsy-1.0.1 statsmodels-0.14.5\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T10:26:41.170080Z",
     "start_time": "2025-07-14T10:26:41.164333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def filter_for_change_detection(timelines, min_posts_per_topic=5, min_topics_per_user=2, min_confidence=0.0):\n",
    "    \"\"\"Filter timelines to only include users/topics suitable for change detection\"\"\"\n",
    "    filtered_timelines = {}\n",
    "\n",
    "    for user_id, user_timeline in timelines.items():\n",
    "        filtered_user_timeline = {}\n",
    "\n",
    "        for topic, topic_posts in user_timeline.items():\n",
    "            # Filter by confidence (if you have access to corpus here)\n",
    "            reliable_posts = {}\n",
    "            for utt_id, stance in topic_posts.items():\n",
    "                # You'd need to pass corpus or confidence scores here\n",
    "                # For now, assume all posts are reliable\n",
    "                reliable_posts[utt_id] = stance\n",
    "\n",
    "            # Check minimum posts per topic\n",
    "            if len(reliable_posts) >= min_posts_per_topic:\n",
    "                filtered_user_timeline[topic] = reliable_posts\n",
    "\n",
    "        # Check minimum topics per user\n",
    "        if len(filtered_user_timeline) >= min_topics_per_user:\n",
    "            filtered_timelines[user_id] = filtered_user_timeline\n",
    "\n",
    "    return filtered_timelines"
   ],
   "id": "de0d75b2335f7dce",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T10:26:41.492540Z",
     "start_time": "2025-07-14T10:26:41.473273Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "class BeliefChangeDetector:\n",
    "    \"\"\"Sliding window change detection with proper statistical significance.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=3, significance_level=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.alpha = significance_level\n",
    "        self.stance_values = {\n",
    "            'strongly_against': -2, 'moderately_against': -1,\n",
    "            'neutral': 0, 'moderately_favor': 1, 'strongly_favor': 2\n",
    "        }\n",
    "\n",
    "    def detect_simple_stance_changes(self, topic_timeline):\n",
    "\n",
    "        if len(topic_timeline) < 2:\n",
    "            return []\n",
    "\n",
    "        changes = []\n",
    "        timeline_items = list(topic_timeline.items())  # Convert to list of (utterance_id, stance) pairs\n",
    "\n",
    "        for i in range(1, len(timeline_items)):\n",
    "            current_utterance_id, current_stance = timeline_items[i]\n",
    "            previous_utterance_id, previous_stance = timeline_items[i-1]\n",
    "\n",
    "            # Check if stance changed\n",
    "            if current_stance != previous_stance:\n",
    "                change = {\n",
    "                    'position': i,\n",
    "                    'current_utterance_id': current_utterance_id,\n",
    "                    'previous_utterance_id': previous_utterance_id,\n",
    "                    'from_stance': previous_stance,\n",
    "                    'to_stance': current_stance,\n",
    "                    'change_type': self._classify_change_direction(previous_stance, current_stance),\n",
    "                    'change_magnitude': self._calculate_simple_magnitude(previous_stance, current_stance)\n",
    "                }\n",
    "                changes.append(change)\n",
    "\n",
    "        return changes\n",
    "\n",
    "    def _classify_change_direction(self, from_stance, to_stance):\n",
    "        \"\"\"Classify the direction of stance change.\"\"\"\n",
    "        from_value = self.stance_values.get(from_stance, 0)\n",
    "        to_value = self.stance_values.get(to_stance, 0)\n",
    "\n",
    "        if to_value > from_value:\n",
    "            return 'more_favorable'\n",
    "        elif to_value < from_value:\n",
    "            return 'less_favorable'\n",
    "        else:\n",
    "            return 'neutral_shift'\n",
    "\n",
    "    def _calculate_simple_magnitude(self, from_stance, to_stance):\n",
    "        \"\"\"Calculate the magnitude of stance change.\"\"\"\n",
    "        from_value = self.stance_values.get(from_stance, 0)\n",
    "        to_value = self.stance_values.get(to_stance, 0)\n",
    "        return abs(to_value - from_value)\n",
    "\n",
    "    def detect_changes_with_significance(self, topic_timeline):\n",
    "        \"\"\"Detect changes with statistical significance testing.\"\"\"\n",
    "\n",
    "        if len(topic_timeline) < self.window_size * 2:\n",
    "            return [], [], []\n",
    "\n",
    "        # Convert to lists to maintain order and get IDs\n",
    "        timeline_items = list(topic_timeline.items())  # [(utterance_id, stance), ...]\n",
    "        stance_sequence = [self.stance_values.get(stance, 0) for _, stance in timeline_items]\n",
    "\n",
    "        potential_changes = []\n",
    "        p_values = []\n",
    "\n",
    "        # Sliding window approach\n",
    "        for i in range(self.window_size, len(stance_sequence) - self.window_size):\n",
    "\n",
    "            # Left window (before potential change)\n",
    "            left_window = stance_sequence[i - self.window_size:i]\n",
    "\n",
    "            # Right window (after potential change)\n",
    "            right_window = stance_sequence[i:i + self.window_size]\n",
    "\n",
    "            # Statistical test: Are these two windows significantly different?\n",
    "            statistic, p_value = self.two_sample_test(left_window, right_window)\n",
    "\n",
    "            p_values.append(p_value)\n",
    "\n",
    "            # Store potential change info with just the key utterance ID\n",
    "            change_magnitude = abs(np.mean(right_window) - np.mean(left_window))\n",
    "            potential_changes.append({\n",
    "                'position': i,\n",
    "                'utterance_id': timeline_items[i][0],  # The utterance where change detected\n",
    "                'p_value': p_value,\n",
    "                'test_statistic': statistic,\n",
    "                'magnitude': change_magnitude,\n",
    "                'left_mean': np.mean(left_window),\n",
    "                'right_mean': np.mean(right_window),\n",
    "                'left_window': left_window.copy(),\n",
    "                'right_window': right_window.copy()\n",
    "            })\n",
    "\n",
    "        # Apply FDR correction to all p-values\n",
    "        if not p_values:\n",
    "            return [], [], []\n",
    "\n",
    "        rejected, p_corrected = self.multiple_testing_correction(p_values)\n",
    "\n",
    "        # Keep only changes that survive FDR correction\n",
    "        significant_changes = []\n",
    "        for i, change in enumerate(potential_changes):\n",
    "            if rejected[i]:  # Survives FDR correction\n",
    "                change.update({\n",
    "                    'p_corrected': p_corrected[i],\n",
    "                    'statistically_significant': True,\n",
    "                    'survives_fdr_correction': True,\n",
    "                    'significance_level': self.alpha\n",
    "                })\n",
    "                significant_changes.append(change)\n",
    "\n",
    "        return significant_changes, p_values, p_corrected\n",
    "\n",
    "    def two_sample_test(self, left_window, right_window):\n",
    "        \"\"\"Statistical test for difference between two windows.\"\"\"\n",
    "        # Use Mann-Whitney U test (non-parametric, more robust)\n",
    "        try:\n",
    "            statistic, p_value = mannwhitneyu(left_window, right_window,\n",
    "                                            alternative='two-sided')\n",
    "            return statistic, p_value\n",
    "        except ValueError:\n",
    "            # Fallback to t-test if Mann-Whitney fails\n",
    "            statistic, p_value = ttest_ind(left_window, right_window)\n",
    "            return statistic, p_value\n",
    "\n",
    "    def multiple_testing_correction(self, p_values):\n",
    "        \"\"\"Correct for multiple testing using Benjamini-Hochberg.\"\"\"\n",
    "        rejected, p_corrected = fdrcorrection(p_values, alpha=self.alpha)\n",
    "        return rejected, p_corrected\n",
    "\n",
    "    # def analyze_user_belief_changes(self, user_timeline):\n",
    "    #     \"\"\"Analyze belief changes across all topics for a user.\"\"\"\n",
    "    #     all_changes = {}\n",
    "    #\n",
    "    #     for topic, topic_timeline in user_timeline.items():\n",
    "    #         changes = self.detect_changes_with_significance(topic_timeline)\n",
    "    #         all_changes[topic] = changes\n",
    "    #\n",
    "    #     return all_changes\n",
    "\n",
    "    def analyze_user_belief_changes(self, user_timeline):\n",
    "        \"\"\"Analyze belief changes across all topics for a user.\n",
    "\n",
    "        Args:\n",
    "            user_timeline: Dict of {topic: {utterance_id: stance}}\n",
    "\n",
    "        Returns:\n",
    "            Dict with changes by topic and total count\n",
    "        \"\"\"\n",
    "        all_changes = {}\n",
    "        total_changes = 0\n",
    "\n",
    "        for topic, topic_timeline in user_timeline.items():\n",
    "            significant_changes, p_values, p_corrected = self.detect_changes_with_significance(topic_timeline)\n",
    "            all_changes[topic] = significant_changes\n",
    "            total_changes += len(significant_changes)\n",
    "\n",
    "        return {\n",
    "            'changes_by_topic': all_changes,\n",
    "            'total_changes': total_changes\n",
    "        }\n",
    "\n",
    "    def analyze_all_users_belief_changes(self, timelines):\n",
    "        \"\"\"Analyze belief changes across all users.\n",
    "\n",
    "        Args:\n",
    "            timelines: Dict of {user_id: {topic: {utterance_id: stance}}}\n",
    "\n",
    "        Returns:\n",
    "            Dict with changes by user and total count\n",
    "        \"\"\"\n",
    "        all_user_changes = {}\n",
    "        total_changes = 0\n",
    "\n",
    "        for user_id, user_timeline in timelines.items():\n",
    "            user_result = self.analyze_user_belief_changes(user_timeline)\n",
    "            all_user_changes[user_id] = user_result\n",
    "            total_changes += user_result['total_changes']\n",
    "\n",
    "        return {\n",
    "            'changes_by_user': all_user_changes,\n",
    "            'total_changes': total_changes\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T10:27:22.794077Z",
     "start_time": "2025-07-14T10:27:10.717627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Detect changes with significance:\n",
    "timeline_builder = TimelineBuilder(corpus, min_posts_per_topic=0, min_topics_per_user=0)\n",
    "timelines = timeline_builder.build_timelines()\n",
    "\n",
    "# Filter for analysis\n",
    "filtered_timelines = filter_for_change_detection(timelines, min_posts_per_topic=5, min_topics_per_user=2)\n",
    "\n",
    "# Get a specific user's timeline for a specific topic\n",
    "user_id = \"HardCoreModerate\"\n",
    "topic = \"media and political commentary\"\n",
    "topic_timeline = filtered_timelines[user_id][topic]  # This is {utterance_id: stance}\n",
    "\n",
    "# Initialize detector and detect changes\n",
    "detector = BeliefChangeDetector()\n",
    "significant_changes, p_values, p_corrected = detector.detect_changes_with_significance(topic_timeline)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Detected {len(significant_changes)} statistically significant stance changes for user {user_id} on topic {topic}:\")\n",
    "for change in significant_changes:\n",
    "    print(f\"  {change['stance_before']} → {change['stance_after']} (magnitude: {change['magnitude']:.3f}, p={change['p_corrected']:.4f})\")"
   ],
   "id": "9e5cd23a3a61604c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 0 statistically significant stance changes for user HardCoreModerate on topic media and political commentary:\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T10:46:54.285111Z",
     "start_time": "2025-07-14T10:46:54.278201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Most populated topic for a user\n",
    "def topic_with_most_contributions(user_id):\n",
    "    posts_in_topic = {}\n",
    "    for topic in timelines[user_id].keys():\n",
    "      posts_in_topic[topic] = len(list(timelines[user_id][topic]))\n",
    "    # key with the largest value\n",
    "    topic = max(posts_in_topic, key=posts_in_topic.get)\n",
    "\n",
    "    return topic, posts_in_topic[topic]\n",
    "\n",
    "# Yea the number came cause the posts_in_topic was not encapsulated\n",
    "user_id = 'HardCoreModerate'\n",
    "topic, number = topic_with_most_contributions(user_id)\n",
    "print(f\"{topic}: {number}\")\n",
    "# print(posts_in_topic)"
   ],
   "id": "d6a1cb6bb6b62c85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media and political commentary: 145\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Total number of users with metadata (unfiltered)\n",
    "print(len(timelines))"
   ],
   "id": "cd56563c479c5f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T10:51:13.890752Z",
     "start_time": "2025-07-14T10:51:13.850561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOT WORKING\n",
    "# user with the most utterances:\n",
    "# I have to find the max between their topics and then find the overall max\n",
    "users = {}\n",
    "for user_id, data in timelines.items():\n",
    "    topic, number = topic_with_most_contributions(user_id)\n",
    "    users[user_id] = topic\n",
    "    users[user_id][topic] = number\n",
    "\n",
    "for user in users:\n",
    "    print(user)\n",
    "# user_id = max(users, key=users.get)\n",
    "# print(f\"{user_id}: {users[user_id]}\")"
   ],
   "id": "484f1d1c47361a1f",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m     topic, number \u001B[38;5;241m=\u001B[39m topic_with_most_contributions(user_id)\n\u001B[1;32m      6\u001B[0m     users[user_id] \u001B[38;5;241m=\u001B[39m topic\n\u001B[0;32m----> 7\u001B[0m     \u001B[43musers\u001B[49m\u001B[43m[\u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtopic\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m number\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m user \u001B[38;5;129;01min\u001B[39;00m users:    \n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(user)\n",
      "\u001B[0;31mTypeError\u001B[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T14:02:24.843970Z",
     "start_time": "2025-07-13T14:02:12.281770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Detect simple stance change:\n",
    "timeline_builder = TimelineBuilder(corpus, min_posts_per_topic=3, min_topics_per_user=1)\n",
    "timelines = timeline_builder.build_timelines()\n",
    "\n",
    "# Get a specific user's timeline for a specific topic\n",
    "user_id = \"HardCoreModerate\"\n",
    "topic = \"taxation and government spending\"\n",
    "topic_timeline = timelines[user_id][topic]  # This is {utterance_id: stance}\n",
    "\n",
    "# Initialize detector and detect changes\n",
    "detector = BeliefChangeDetector()\n",
    "changes = detector.detect_simple_stance_changes(topic_timeline)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Detected {len(changes)} stance changes for user {user_id} on topic {topic}:\")\n",
    "for change in changes:\n",
    "    print(f\"  {change['from_stance']} → {change['to_stance']} (magnitude: {change['change_magnitude']})\")"
   ],
   "id": "fa8987174cfc2918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 stance changes for user HardCoreModerate on topic taxation and government spending:\n",
      "  moderately_against → neutral (magnitude: 1)\n",
      "  neutral → moderately_against (magnitude: 1)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# for user_id in",
   "id": "f414b5573c4a4ca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run detection for all topics for a user - NOT TESTED:\n",
    "# Get complete user timeline\n",
    "user_timeline = timelines[\"pixel8\"]  # All topics for this user\n",
    "\n",
    "# Analyze changes across all topics\n",
    "detector = BeliefChangeDetector()\n",
    "all_changes = detector.analyze_user_belief_changes(user_timeline)\n",
    "\n",
    "# Results\n",
    "for topic, changes in all_changes.items():\n",
    "    print(f\"Topic: {topic}\")\n",
    "    for change in changes:\n",
    "        print(f\"  Change at position {change['position']}: magnitude {change['magnitude']}\")"
   ],
   "id": "ad5e6484d5f466e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# All users that meet the criteria:\n",
    "print(\"Available users:\")\n",
    "print(list(timelines.keys())[:20])"
   ],
   "id": "76049e46c5223f2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# What topics the users have posted about:\n",
    "for user_id in list(timelines.keys())[:20]:  # Check first 5 users\n",
    "    topics = list(timelines[user_id].keys())\n",
    "    print(f\"{user_id}: {topics}\")\n",
    "    break"
   ],
   "id": "ba90763c4b3d61ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T12:45:16.821978Z",
     "start_time": "2025-07-07T12:45:16.653806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# confidence score:\n",
    "utterances = list(corpus.iter_utterances())\n",
    "print(utterances[1].meta)"
   ],
   "id": "1fee47509c799599",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvoKitMeta({'score': 29, 'top_level_comment': None, 'retrieved_on': -1, 'gilded': -1, 'gildings': None, 'subreddit': 'PoliticalDiscussion', 'stickied': False, 'permalink': '/r/PoliticalDiscussion/comments/nz1xu/congrats_rpoliticaldiscussion_you_are_turning/', 'author_flair_text': '', 'detected_stance': 'moderately_against', 'stance_confidence': 0.8540321985880533, 'stance_scores': {'strongly_favor': 0.0016047263949682626, 'moderately_favor': 0.5134096046288809, 'neutral': 0.0072105322033166885, 'moderately_against': 0.8540321985880533, 'strongly_against': 0.3021060957883795}})\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a81d7e7fa2b05740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
