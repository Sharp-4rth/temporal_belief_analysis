{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Need to restart after:\n",
        "!pip install convokit[llm]\n",
        "!pip install convokit"
      ],
      "metadata": {
        "id": "HNCzJ0As_8H3"
      },
      "id": "HNCzJ0As_8H3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir('/content/temporal_belief_analysis/notebooks')\n",
        "print(\"Changed working directory to:\", os.getcwd())\n",
        "\n",
        "# Absolute path to src directory\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ],
      "metadata": {
        "id": "A-KqnzYq_9Ct",
        "outputId": "8010bd18-afd2-4578-bbcd-7bc632bee156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A-KqnzYq_9Ct",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed working directory to: /content/temporal_belief_analysis/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "!pip install gdown\n",
        "import zipfile\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from convokit import Corpus, download\n",
        "import convokit\n",
        "from temporal_belief.core.timeline_building import TimelineBuilder"
      ],
      "metadata": {
        "id": "JoF3EEYTAB5j",
        "outputId": "e0fd377b-ac07-4479-c415-43ef38b81809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JoF3EEYTAB5j",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip with python (Dataloading):\n",
        "# !gdown \"https://drive.google.com/file/d/1N0U_jUJlOYjdaju2FaU8p87uB22YBxJ0/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/1DLFY6JLMZqNjwvNRZmhlV4-rnoQP_eyH/view?usp=sharing\" -O \"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/15NMRXEkGRoGjK6TXFBHIMOPjkTyZ0keP/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances200000_llm.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/15nVf6Js0KsDxA9HaB0zCXhc8VdcC-Fy-/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances75000_llm.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/1dOUvQmtjFrXq0hJvnOsP_ZLqdGEyOVNJ/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances100000_llm.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/10HNQHIsFe386oBu_o5s9JOuo-wiaofXC/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances150000_llm.zip\" --fuzzy\n",
        "!gdown \"https://drive.google.com/file/d/1nWaj5N8nsG7u5homv_kAh4CLPDv01M_Z/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_topics.zip\" --fuzzy\n",
        "\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances200000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances75000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances150000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_topics.zip\").extractall(\"/content/temporal_belief_analysis\")"
      ],
      "metadata": {
        "id": "KbhaVaGwADs4",
        "outputId": "9133a91a-6d23-4d9f-d481-636115905097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KbhaVaGwADs4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1nWaj5N8nsG7u5homv_kAh4CLPDv01M_Z\n",
            "From (redirected): https://drive.google.com/uc?id=1nWaj5N8nsG7u5homv_kAh4CLPDv01M_Z&confirm=t&uuid=118793da-003b-41f5-9571-5fd97fe192da\n",
            "To: /content/temporal_belief_analysis/pd_corpus_with_topics.zip\n",
            "100% 833M/833M [00:12<00:00, 67.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/file/d/1hgSaV4VgTMzkQ9omjJHVnd-OJkA8UGxX/view?usp=sharing\" -O \"/content/temporal_belief_analysis/domain_adapted_model.zip\" --fuzzy\n",
        "zipfile.ZipFile(\"/content/temporal_belief_analysis/domain_adapted_model.zip\").extractall(\"/content/temporal_belief_analysis\")"
      ],
      "metadata": {
        "id": "bqTvD2aHNmUj",
        "outputId": "9e11cd6b-768e-4d8f-a330-d2afab0590a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bqTvD2aHNmUj",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1hgSaV4VgTMzkQ9omjJHVnd-OJkA8UGxX\n",
            "From (redirected): https://drive.google.com/uc?id=1hgSaV4VgTMzkQ9omjJHVnd-OJkA8UGxX&confirm=t&uuid=ea43ae6d-3119-4a50-b26d-43c016804c3e\n",
            "To: /content/temporal_belief_analysis/domain_adapted_model.zip\n",
            "100% 481M/481M [00:07<00:00, 67.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus(filename=\"/content/temporal_belief_analysis/pd_corpus_with_topics\")"
      ],
      "metadata": {
        "id": "Hy9knMDXAI3g",
        "outputId": "76da9f96-52a8-4037-e58f-6dda35ca9ab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Hy9knMDXAI3g",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "model_directory: ~/.convokit/saved-models\n",
            "default_backend: mem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Stance detection with your fine-tuned model + ConvoKit integration ===\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# If you're already in a notebook where 'Corpus' is available, keep this import.\n",
        "# Otherwise, uncomment the next line and ensure convokit is installed.\n",
        "# from convokit import Corpus\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "MODEL_PATH = \"/content/temporal_belief_analysis/stance-clf-best-domain-adapted\"   # or \"username/your-model\"\n",
        "MAX_LENGTH = 128                                    # speed/accuracy tradeoff\n",
        "BATCH_SIZE = 32                                     # adjust to your VRAM\n",
        "SAVE_PATH  = None                                   # e.g., \"/content/corpus_with_stance\"\n",
        "FILTER_REMOVED = True                               # skip removed/deleted/empty\n",
        "# ----------------------------\n",
        "\n",
        "# Device selection (CUDA > MPS > CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = 0\n",
        "    torch_dtype = torch.float16   # fp16 on GPU if supported\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "    torch_dtype = None\n",
        "else:\n",
        "    device = -1\n",
        "    torch_dtype = None\n",
        "\n",
        "# Load tokenizer/model and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, torch_dtype=torch_dtype)\n",
        "\n",
        "clf = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    return_all_scores=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "def is_valid_text(txt: str) -> bool:\n",
        "    if not txt:\n",
        "        return False\n",
        "    s = txt.strip()\n",
        "    if FILTER_REMOVED and (s in (\"[removed]\", \"[deleted]\") or s == \".\"):\n",
        "        return False\n",
        "    return len(s) > 0\n",
        "\n",
        "def run_batched_stance(corpus, batch_size=BATCH_SIZE, max_len=MAX_LENGTH, save_path=SAVE_PATH, max_utts=0):\n",
        "    # Collect utterances & texts to score\n",
        "    utts = []\n",
        "    texts = []\n",
        "    utts = list(corpus.iter_utterances())\n",
        "    utts = utts[:max_utts] if max_utts > 0 else utts\n",
        "    for utt in utts:\n",
        "        if is_valid_text(utt.text):\n",
        "            utts.append(utt)\n",
        "            texts.append(utt.text)\n",
        "\n",
        "    if not texts:\n",
        "        print(\"No valid utterances to process.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Scoring {len(texts)} utterances (batch_size={batch_size}, max_length={max_len})...\")\n",
        "\n",
        "    # Run batched inference\n",
        "    # The pipeline accepts a list; we add padding/truncation and a max_length cap for speed.\n",
        "    all_results = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Batches\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        res = clf(\n",
        "            batch,\n",
        "            batch_size=batch_size,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len\n",
        "        )\n",
        "        # `res` is List[List[{'label','score'}]] of length len(batch)\n",
        "        all_results.extend(res)\n",
        "\n",
        "    # Attach results back to utterances\n",
        "    for utt, scores in zip(utts, all_results):\n",
        "        # scores: [{'label': 'neutral', 'score': ...}, ...]\n",
        "        # sort descending, get top\n",
        "        scores_sorted = sorted(scores, key=lambda d: d[\"score\"], reverse=True)\n",
        "        top = scores_sorted[0]\n",
        "        per_label = {d[\"label\"]: float(d[\"score\"]) for d in scores_sorted}\n",
        "\n",
        "        utt.add_meta(\"detected_stance\", top[\"label\"])\n",
        "        utt.add_meta(\"stance_confidence\", float(top[\"score\"]))\n",
        "        utt.add_meta(\"stance_scores\", per_label)\n",
        "        # optional tag to record which model produced these\n",
        "        utt.add_meta(\"stance_model\", MODEL_PATH)\n",
        "\n",
        "    if save_path:\n",
        "        corpus.dump(save_path)\n",
        "        print(f\"Saved processed corpus to: {save_path}\")\n",
        "\n",
        "# ---- Usage example ----\n",
        "# corpus = Corpus(\"/path/or/url\")\n"
      ],
      "metadata": {
        "id": "-mLiRTsaCQYV",
        "outputId": "42a06eca-85fb-4552-9654-7e1de65f7451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-mLiRTsaCQYV",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, math, random\n",
        "\n",
        "texts = [utt.text for utt in corpus.iter_utterances() if utt.text]\n",
        "\n",
        "# pick a representative subset of your texts\n",
        "SAMPLE_N = 10000  # 10k usually enough for a stable rate\n",
        "subset = random.sample(texts, min(SAMPLE_N, len(texts)))\n",
        "\n",
        "# warmup (GPU + graph compile, caches)\n",
        "_ = clf(subset[:512], batch_size=32, padding=True, truncation=True, max_length=128)\n",
        "\n",
        "t0 = time.time()\n",
        "_ = clf(subset, batch_size=32, padding=True, truncation=True, max_length=128)\n",
        "dt = time.time() - t0\n",
        "\n",
        "rate = len(subset) / dt                 # utterances per second\n",
        "total = 4_000_000\n",
        "eta_hours = (total / rate) / 3600\n",
        "\n",
        "print(f\"Throughput: {rate:.1f} utt/s\")\n",
        "print(f\"ETA for 4,000,000: {eta_hours:.1f} hours\")"
      ],
      "metadata": {
        "id": "i9IkGc5qLqeN",
        "outputId": "185139bc-7f33-4ce3-ad00-93c3712dc13f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "i9IkGc5qLqeN",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Throughput: 661.6 utt/s\n",
            "ETA for 4,000,000: 1.7 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_batched_stance(corpus, batch_size=32, max_len=128, save_path=\"/content/temporal_belief_analysis\", max_utts=5000)"
      ],
      "metadata": {
        "id": "WhEsNdLZCLUQ",
        "outputId": "13ad4f35-cd6a-413c-f9dc-5b87f45f2ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "id": "WhEsNdLZCLUQ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3493488647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_batched_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/temporal_belief_analysis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_utts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-692862175.py\u001b[0m in \u001b[0;36mrun_batched_stance\u001b[0;34m(corpus, batch_size, max_len, save_path, max_utts)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mutts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_utts\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_utts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mutts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mutts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-692862175.py\u001b[0m in \u001b[0;36mis_valid_text\u001b[0;34m(txt)\u001b[0m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_valid_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mqnihpdOgQe"
      },
      "id": "2mqnihpdOgQe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}