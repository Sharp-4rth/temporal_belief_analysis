{
 "cells": [
  {
   "metadata": {
    "id": "c3fbd6a099944b04"
   },
   "cell_type": "code",
   "source": [
    "# Need to restart after:\n",
    "!pip install convokit"
   ],
   "id": "c3fbd6a099944b04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "37001152dd406570"
   },
   "cell_type": "code",
   "source": [
    "# Download file from Google Drive to colab directory\n",
    "!pip install gdown\n",
    "import zipfile"
   ],
   "id": "37001152dd406570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "56c7674bc45a38b2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3b6eabb3-86d0-40d3-93cf-9573c560767a"
   },
   "cell_type": "code",
   "source": [
    "# Download and unzip with python:\n",
    "!gdown \"https://drive.google.com/file/d/1N0U_jUJlOYjdaju2FaU8p87uB22YBxJ0/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\" --fuzzy\n",
    "!gdown \"https://drive.google.com/file/d/1DLFY6JLMZqNjwvNRZmhlV4-rnoQP_eyH/view?usp=sharing\" -O \"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\" --fuzzy\n",
    "zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
    "zipfile.ZipFile(\"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\").extractall(\"/content/temporal_belief_analysis\")\n"
   ],
   "id": "56c7674bc45a38b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e946106a2e92b1d0",
    "outputId": "bee26bcc-c188-41d6-ea8d-2c8335b0473f"
   },
   "cell_type": "code",
   "source": [
    "# For runpod-jupyter or local (run twice)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Change to the correct working directory (workspace if runpod, content if colab)\n",
    "os.chdir('/content/temporal_belief_analysis/notebooks')\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "# Absolute path to src directory\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Comment out if in colab:\n",
    "from temporal_belief.core.timeline_building import TimelineBuilder\n",
    "from temporal_belief.core.persistence_change_detection import ChangeDetector\n",
    "from temporal_belief.core.window_extraction import WindowExtractor\n",
    "from temporal_belief.core.op_path_pairing import OpPathPairer\n",
    "from temporal_belief.data.preprocessors import ChangeDetectorPreprocessor"
   ],
   "id": "e946106a2e92b1d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "30c48534c952a7fc"
   },
   "cell_type": "code",
   "source": [
    "# Run twice\n",
    "# import unsloth\n",
    "# import unsloth_zoo\n",
    "from convokit import Corpus, download\n",
    "import convokit"
   ],
   "id": "30c48534c952a7fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e79372f7a96844b0",
    "outputId": "98d77e36-9342-4857-a847-9868bab10daa"
   },
   "cell_type": "code",
   "source": [
    "# Load a corpus:\n",
    "corpus = Corpus(filename=\"/content/temporal_belief_analysis/merged_corpus_checkpoint_5\")"
   ],
   "id": "e79372f7a96844b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1094315e0f0a6e2b",
    "outputId": "3e522c07-b893-4364-b60b-0624578a6bd3"
   },
   "cell_type": "code",
   "source": [
    "print(corpus.meta)"
   ],
   "id": "1094315e0f0a6e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "utts = list(corpus.iter_utterances())\n",
    "print(utts[0].meta)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRryqd_8mTrQ",
    "outputId": "926c21dd-4498-4ec3-f365-36e74b75f7ee"
   },
   "id": "WRryqd_8mTrQ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if the 300k labeled utterances are the LAST 300k chronologically\n",
    "sorted_utts = sorted(list(corpus.iter_utterances()), key=lambda u: getattr(u, 'timestamp', 0))\n",
    "total_utts = len(sorted_utts)\n",
    "\n",
    "print(f\"Total utterances: {total_utts}\")\n",
    "\n",
    "# Check from the end backwards\n",
    "consecutive_from_end = 0\n",
    "for i in range(total_utts - 1, -1, -1):  # Count backwards from end\n",
    "    utt = sorted_utts[i]\n",
    "    if 'detected_stance' in utt.meta:\n",
    "        consecutive_from_end += 1\n",
    "    else:\n",
    "        break  # First utterance (going backwards) without stance\n",
    "\n",
    "print(f\"Consecutive utterances with stance from END: {consecutive_from_end}\")\n",
    "\n",
    "# Also check where the labels START (going forward)\n",
    "first_labeled_position = None\n",
    "for i, utt in enumerate(sorted_utts):\n",
    "    if 'detected_stance' in utt.meta:\n",
    "        first_labeled_position = i\n",
    "        break\n",
    "\n",
    "print(f\"First labeled utterance is at position: {first_labeled_position}\")\n",
    "print(f\"Labels span positions {first_labeled_position} to {total_utts - 1}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXwUCd8n4cKL",
    "outputId": "ea18c8bd-52cf-4f61-b19f-12adc279d11c"
   },
   "id": "EXwUCd8n4cKL",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Find where your 300k labels actually are\n",
    "labeled_positions = []\n",
    "for i, utt in enumerate(sorted_utts):\n",
    "    if 'detected_stance' in utt.meta:\n",
    "        labeled_positions.append(i)\n",
    "\n",
    "if labeled_positions:\n",
    "    print(f\"First labeled position: {labeled_positions[0]}\")\n",
    "    print(f\"Last labeled position: {labeled_positions[-1]}\")\n",
    "    print(f\"Total labeled: {len(labeled_positions)}\")\n",
    "\n",
    "    # Check if they're consecutive\n",
    "    is_consecutive = all(labeled_positions[i] == labeled_positions[i-1] + 1\n",
    "                        for i in range(1, len(labeled_positions)))\n",
    "    print(f\"Are they consecutive? {is_consecutive}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "6Jxrh3SCGI1c",
    "outputId": "7f82eb99-5d8d-4e72-feb5-0121263d92d7"
   },
   "id": "6Jxrh3SCGI1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "found_stances = []\n",
    "i = 0\n",
    "for utt in corpus.iter_utterances():\n",
    "    if 'detected_stance' in utt.meta:\n",
    "        found_stances.append((utt.id, utt.meta['detected_stance']))\n",
    "        i += 1\n",
    "\n",
    "print(\"Utterances WITH stance labels:\", found_stances)\n",
    "print(i)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4YZH27T5Gsj",
    "outputId": "320b8fd7-dfc8-487a-e840-6a430ec7142b"
   },
   "id": "Z4YZH27T5Gsj",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Check first x utterances specifically\n",
    "utts = list(corpus.iter_utterances())\n",
    "for i in range(min(300000, len(utts))):\n",
    "    utt = utts[i]\n",
    "    has_stance = 'detected_stance' in utt.meta\n",
    "    print(f\"Utterance {i}: {utt.id} - Has stance: {has_stance}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQiNqLp25QiE",
    "outputId": "b0aa71f2-2292-4198-b428-dd1af34520e0"
   },
   "id": "VQiNqLp25QiE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from collections import Counter\n",
    "import logging\n",
    "\n",
    "class ChangeDetector:\n",
    "    \"\"\"Sliding window change detection with persistence threshold.\"\"\"\n",
    "\n",
    "    def __init__(self, window_size=3, persistence_threshold=4, significance_level=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.persistence_threshold = persistence_threshold\n",
    "        self.alpha = significance_level\n",
    "        self.stance_values = {\n",
    "            'strongly_against': -2, 'moderately_against': -1,\n",
    "            'neutral': 0, 'moderately_favor': 1, 'strongly_favor': 2\n",
    "        }\n",
    "        self.all_change_points = []\n",
    "        self.all_no_change_points = []\n",
    "\n",
    "    def detect_persistent_changes(self, topic_timeline):\n",
    "        \"\"\"\n",
    "        Detect persistent changes using sliding window with numerical averages.\n",
    "        This is your main detection method.\n",
    "        \"\"\"\n",
    "        if len(topic_timeline) < self.window_size * 2:\n",
    "            return {'change_points': [], 'no_change_points': []}\n",
    "\n",
    "        # Convert stances to numerical values\n",
    "        numerical_stances = []\n",
    "        for utt_id, stance in topic_timeline:\n",
    "            numerical_stances.append(self.stance_values.get(stance, 0))\n",
    "\n",
    "        change_points = []\n",
    "        no_change_points = []\n",
    "\n",
    "        # Calculate sliding window averages\n",
    "        for i in range(self.window_size, len(numerical_stances) - self.window_size):\n",
    "\n",
    "            # Get before and after windows\n",
    "            before_window = numerical_stances[i-self.window_size:i]\n",
    "            after_window = numerical_stances[i:i+self.window_size]\n",
    "\n",
    "            # Calculate means\n",
    "            before_mean = np.mean(before_window)\n",
    "            after_mean = np.mean(after_window)\n",
    "\n",
    "            # Check for significant change (simple threshold approach)\n",
    "            change_magnitude = abs(after_mean - before_mean)\n",
    "\n",
    "            # Require both magnitude and direction consistency\n",
    "            if change_magnitude > 0.5:  # Adjust threshold as needed\n",
    "                # Check if change persists\n",
    "                future_window = numerical_stances[i+self.window_size:i+2*self.window_size]\n",
    "                if len(future_window) >= self.window_size:\n",
    "                    future_mean = np.mean(future_window)\n",
    "\n",
    "                    # If the change direction is maintained\n",
    "                    if (after_mean - before_mean) * (future_mean - before_mean) > 0:\n",
    "                        utt_id = topic_timeline[i][0]\n",
    "                        change_points.append(utt_id)\n",
    "                        print(f\"Sliding window change at index {i}: \"\n",
    "                              f\"before={before_mean:.2f}, after={after_mean:.2f}, \"\n",
    "                              f\"future={future_mean:.2f}\")\n",
    "\n",
    "        # Add non-change points\n",
    "        for i, (utt_id, stance) in enumerate(topic_timeline):\n",
    "            if utt_id not in change_points:\n",
    "                no_change_points.append(utt_id)\n",
    "\n",
    "        return {\n",
    "            'change_points': change_points,\n",
    "            'no_change_points': no_change_points\n",
    "        }\n",
    "\n",
    "    def detect_persistent_changes_simple(self, topic_timeline):\n",
    "        \"\"\"\n",
    "        Alternative: Simple persistence detection (your original approach, but fixed).\n",
    "        Call this method if you want the simpler approach.\n",
    "        \"\"\"\n",
    "        change_points = []\n",
    "        no_change_points = []\n",
    "\n",
    "        if len(topic_timeline) < self.persistence_threshold + 1:\n",
    "            # Timeline too short for meaningful analysis\n",
    "            return {'change_points': change_points, 'no_change_points': no_change_points}\n",
    "\n",
    "        # Track detected changes to avoid duplicates\n",
    "        already_detected = set()\n",
    "\n",
    "        for i in range(1, len(topic_timeline) - self.persistence_threshold):\n",
    "            current_stance = topic_timeline[i][1]\n",
    "            previous_stance = topic_timeline[i-1][1]\n",
    "\n",
    "            # Check if stance changed\n",
    "            if current_stance != previous_stance and i not in already_detected:\n",
    "\n",
    "                # Check if new stance persists for required threshold\n",
    "                persistence_count = 1  # Current post counts as 1\n",
    "\n",
    "                for j in range(i + 1, min(i + self.persistence_threshold, len(topic_timeline))):\n",
    "                    if topic_timeline[j][1] == current_stance:\n",
    "                        persistence_count += 1\n",
    "                    else:\n",
    "                        break  # Persistence broken\n",
    "\n",
    "                # If new stance persists for threshold, mark as change point\n",
    "                if persistence_count >= self.persistence_threshold:\n",
    "                    utt_id = topic_timeline[i][0]\n",
    "                    change_points.append(utt_id)\n",
    "\n",
    "                    # Mark this range as detected to avoid overlapping detections\n",
    "                    for k in range(i, min(i + self.persistence_threshold, len(topic_timeline))):\n",
    "                        already_detected.add(k)\n",
    "\n",
    "                    print(f\"Change detected at index {i}: {previous_stance} → {current_stance} \"\n",
    "                          f\"(persisted for {persistence_count} posts)\")\n",
    "\n",
    "        # Add non-change points (utterances that didn't cause changes)\n",
    "        for i, (utt_id, stance) in enumerate(topic_timeline):\n",
    "            if utt_id not in change_points:\n",
    "                no_change_points.append(utt_id)\n",
    "\n",
    "        # Store for global analysis\n",
    "        self.all_change_points.extend(change_points)\n",
    "        self.all_no_change_points.extend(no_change_points)\n",
    "\n",
    "        return {\n",
    "            'change_points': change_points,\n",
    "            'no_change_points': no_change_points\n",
    "        }\n",
    "\n",
    "    def get_two_groups(self, timelines, method='sliding_window'):\n",
    "        \"\"\"\n",
    "        Categorize users into those with/without changes using specified method.\n",
    "\n",
    "        Args:\n",
    "            timelines: User timeline data\n",
    "            method: 'sliding_window' (default) or 'simple'\n",
    "        \"\"\"\n",
    "        with_changes = {}\n",
    "        no_changes = {}\n",
    "\n",
    "        # Choose detection method\n",
    "        if method == 'sliding_window':\n",
    "            detect_func = self.detect_persistent_changes  # Uses numerical sliding windows\n",
    "        elif method == 'simple':\n",
    "            detect_func = self.detect_persistent_changes_simple  # Your original approach\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}. Use 'sliding_window' or 'simple'\")\n",
    "\n",
    "        for user_id, topic_timelines in timelines.items():\n",
    "            user_has_changes = False\n",
    "\n",
    "            for topic_name, topic_timeline in topic_timelines.items():\n",
    "                topic_timeline_list = list(topic_timeline.items())\n",
    "                changes = detect_func(topic_timeline_list)\n",
    "\n",
    "                if changes['change_points']:\n",
    "                    user_has_changes = True\n",
    "                    # Store change-causing utterances\n",
    "                    if user_id not in with_changes:\n",
    "                        with_changes[user_id] = {}\n",
    "                    with_changes[user_id][topic_name] = {\n",
    "                        utt_id: topic_timeline[utt_id]\n",
    "                        for utt_id in changes['change_points']\n",
    "                    }\n",
    "\n",
    "            # If user had no changes in any topic, add to no_changes group\n",
    "            if not user_has_changes:\n",
    "                no_changes[user_id] = topic_timelines\n",
    "\n",
    "        return {\n",
    "            'with_changes': with_changes,\n",
    "            'no_changes': no_changes\n",
    "        }"
   ],
   "metadata": {
    "id": "Ydu4Kt8arll0"
   },
   "id": "Ydu4Kt8arll0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f41c4501acad35af",
    "outputId": "abdc6202-c068-41bd-e7b5-f90968a5454c"
   },
   "cell_type": "code",
   "source": [
    "# Test timeline builder:\n",
    "timeline_builder = TimelineBuilder(corpus, min_posts_per_topic=0, min_topics_per_user=0)\n",
    "timelines = timeline_builder.build_timelines()\n",
    "\n",
    "# Filter for analysis\n",
    "change_detector_preprocessor = ChangeDetectorPreprocessor()\n",
    "\n",
    "# Use filtered ones for detecting changes but the full ones for interplay score. Although maybe it doesn't matter.\n",
    "filtered_timelines = change_detector_preprocessor.filter_for_change_detection(timelines, min_posts_per_topic=5, min_topics_per_user=2)\n",
    "\n",
    "# Get a specific user's timeline for a specific topic\n",
    "user_id = \"HardCoreModerate\"\n",
    "topic = \"media and political commentary\"\n",
    "topic_timeline = filtered_timelines[user_id][topic]  # This is {utterance_id: stance}\n",
    "\n",
    "# Convert to list of tuples\n",
    "# topic_timeline_list = list(topic_timeline.items())"
   ],
   "id": "f41c4501acad35af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(timelines))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSL7KxJJaSD2",
    "outputId": "21f38d08-e501-43da-92f6-7d6a23131870"
   },
   "id": "DSL7KxJJaSD2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(topic_timeline)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CVTC02y0lUu",
    "outputId": "e81dabe7-5eef-4d08-a0ab-475cd99f63ab"
   },
   "id": "6CVTC02y0lUu",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Can I iterate over all timelines of this user?\n",
    "persistence_detector_new = ChangeDetector()\n",
    "for user_id, topic_timelines in timelines.items():\n",
    "    for topic_timeline in topic_timelines.values():\n",
    "        topic_timeline_list = list(topic_timeline.items())\n",
    "        changes = persistence_detector_new.detect_persistent_changes(topic_timeline_list)\n",
    "        # self.detect_persistent_changes(topic_timeline)"
   ],
   "metadata": {
    "id": "UacsukiTx-DX"
   },
   "id": "UacsukiTx-DX",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5df301bbdf9bfab3"
   },
   "cell_type": "code",
   "source": [
    "# Test the change detector:\n",
    "persistence_detector_new = ChangeDetector()\n",
    "topic_timeline_list = list(topic_timeline.items())\n",
    "change_points = persistence_detector_new.detect_persistent_changes(topic_timeline_list)\n",
    "# with_changes, no_changes = persistence_detector_new.get_two_groups()"
   ],
   "id": "5df301bbdf9bfab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Groups:\n",
    "persistence_detector_new = ChangeDetector()\n",
    "groups = persistence_detector_new.get_two_groups(timelines)\n",
    "\n",
    "print(len(groups['with_changes']))\n",
    "print(len(groups['no_changes']))\n",
    "\n",
    "print(groups['with_changes'])\n",
    "\n",
    "# Numbers don't add up cause some users could appear in both groups (had changes in some topics but not in others).\n",
    "\n",
    "# works!!!!"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzjIRLyNsAb_",
    "outputId": "928b6c8e-23dd-4d42-881e-a2fd07191a4e"
   },
   "id": "fzjIRLyNsAb_",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "9e2b5729ab13ef41",
    "outputId": "e611430e-f146-435e-b2d3-609a7f3efcf7"
   },
   "cell_type": "code",
   "source": [
    "# Test the window extractor:\n",
    "window_extractor = WindowExtractor(corpus, timelines=timelines)\n",
    "candidate_convos = window_extractor.get_conversations_around_change_point(change_points=change_points, corpus=corpus)\n",
    "for convo in candidate_convos:\n",
    "  print(f'ID:{convo.id}')"
   ],
   "id": "9e2b5729ab13ef41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "class PairPreprocessor:\n",
    "\n",
    "    def tokenize_quotes(self, utterance_text):\n",
    "        lines = utterance_text.split('\\n')\n",
    "        processed_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('&gt;') or line.startswith('>'):\n",
    "                processed_lines.append('[QUOTE]')\n",
    "            else:\n",
    "                processed_lines.append(line)\n",
    "\n",
    "        return '\\n'.join(processed_lines)\n",
    "\n",
    "    def concatenate_path(self, paths):\n",
    "        concatenated_paths = {}\n",
    "        for key, utt_list in paths.items():\n",
    "            path_text = ''\n",
    "            for utt in utt_list:\n",
    "                utt_text_quoted = self.tokenize_quotes(utt.text)\n",
    "                path_text += utt_text_quoted + ' '\n",
    "            concatenated_paths[key] = path_text.strip()\n",
    "        return concatenated_paths\n",
    "\n",
    "    def tokenize_and_lower(op_text, reply_path_text, stop_words_set):\n",
    "        op_words = op_text.lower().split()\n",
    "        reply_words = reply_path_text.lower().split()\n",
    "\n",
    "        return (op_words, reply_words)\n",
    "\n",
    "    # This pattern keeps letters, numbers, whitespace, and apostrophes (for contractions)\n",
    "    def remove_punctuation(op_text, reply_path_text):\n",
    "        op_text = re.sub(r\"[^\\w\\s']\", '', op_text)\n",
    "        reply_path_text = re.sub(r\"[^\\w\\s']\", '', reply_path_text)\n",
    "\n",
    "        return op_text, reply_path_text\n",
    "\n",
    "    def remove_quotes_from_all(self, op_path_pairs):\n",
    "        marked_pairs = []\n",
    "        for op_path_pair in op_path_pairs:\n",
    "            # Process the OP utterance\n",
    "            op_text = self.tokenize_quotes(op_path_pair[0].text)\n",
    "\n",
    "            # Process each utterance path\n",
    "            processed_paths = []\n",
    "            for utterances in op_path_pair[1].values():\n",
    "                path = [self.tokenize_quotes(utt.text) for utt in utterances]\n",
    "                processed_paths.append(path)\n",
    "\n",
    "            marked_pairs.append((op_text, processed_paths))\n",
    "\n",
    "        return marked_pairs\n",
    "\n",
    "    def concatenate_path_in_pair(self, pair):\n",
    "        op = pair[0]\n",
    "        paths = pair[1]\n",
    "\n",
    "        concatenated_paths = self.concatenate_path(paths)\n",
    "\n",
    "        return (op, concatenated_paths)\n",
    "\n",
    "    def concatenate_path_in_all_pairs(self, op_path_pairs):\n",
    "        # op_path_pairs_quoted = self.remove_quotes_from_all(op_path_pairs)\n",
    "        preprocessed_pairs = []\n",
    "        for pair in op_path_pairs:\n",
    "            pair = self.concatenate_path_in_pair(pair)\n",
    "            preprocessed_pairs.append(pair)\n",
    "\n",
    "        return preprocessed_pairs\n",
    "\n",
    "    def clean_and_tokenize(self, op_text, reply_path_text):\n",
    "        # Step 1: Remove punctuation\n",
    "        op_text, reply_path_text = self.remove_punctuation(op_text, reply_path_text)\n",
    "\n",
    "        # Step 2: Tokenize and lowercase\n",
    "        op_words, reply_words = self.tokenize_and_lower(op_text, reply_path_text)\n",
    "\n",
    "        return op_words, reply_words"
   ],
   "metadata": {
    "id": "8xz63RKkNM4m"
   },
   "id": "8xz63RKkNM4m",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_interplay_features(op_text, reply_path_text, stop_words_set):\n",
    "    \"\"\"Calculate 12 interplay features between OP and reply path.\"\"\"\n",
    "\n",
    "    # Remove punctuation\n",
    "    op_text = re.sub(r\"[^\\w\\s']\", '', op_text)\n",
    "    reply_path_text = re.sub(r\"[^\\w\\s']\", '', reply_path_text)\n",
    "\n",
    "    # Tokenize and clean\n",
    "    op_words = op_text.lower().split()\n",
    "    reply_words = reply_path_text.lower().split()\n",
    "\n",
    "    # Create word sets\n",
    "    op_all = set(op_words)\n",
    "    reply_all = set(reply_words)\n",
    "    op_stop = set(w for w in op_words if w in stop_words_set)\n",
    "    reply_stop = set(w for w in reply_words if w in stop_words_set)\n",
    "    op_content = set(w for w in op_words if w not in stop_words_set)\n",
    "    reply_content = set(w for w in reply_words if w not in stop_words_set)\n",
    "\n",
    "    # Calculate 4 metrics for each word type\n",
    "    features = {}\n",
    "\n",
    "    for word_type, (op_set, reply_set) in [\n",
    "        ('all', (op_all, reply_all)),\n",
    "        ('stop', (op_stop, reply_stop)),\n",
    "        ('content', (op_content, reply_content))\n",
    "    ]:\n",
    "        intersection = len(op_set & reply_set)\n",
    "        union = len(op_set | reply_set)\n",
    "\n",
    "        features[f'common_words_{word_type}'] = intersection\n",
    "        features[f'sim_frac_reply_{word_type}'] = intersection / len(reply_set) if reply_set else 0\n",
    "        features[f'sim_frac_op_{word_type}'] = intersection / len(op_set) if op_set else 0\n",
    "        features[f'jaccard_{word_type}'] = intersection / union if union else 0\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "id": "MPYqGMj-iLrX"
   },
   "id": "MPYqGMj-iLrX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_persuasion_score(interplay_features):\n",
    "    \"\"\"\n",
    "    Calculate persuasion score based on Tan et al.'s CMV findings.\n",
    "    Higher scores indicate higher persuasion likelihood.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the key predictive features\n",
    "    reply_frac_content = interplay_features.get('sim_frac_reply_content', 0)\n",
    "    jaccard_content = interplay_features.get('jaccard_content', 0)\n",
    "    op_frac_stop = interplay_features.get('sim_frac_op_stop', 0)\n",
    "    reply_frac_all = interplay_features.get('sim_frac_reply_all', 0)\n",
    "\n",
    "    # Apply their findings (↓↓↓↓ means negative correlation, ↑↑↑↑ means positive)\n",
    "    score = 0\n",
    "\n",
    "    # Strongest predictor: less content word similarity → more persuasive\n",
    "    score += (1 - reply_frac_content) * 0.4  # Weight of 0.4 for strongest predictor\n",
    "\n",
    "    # Less content overlap → more persuasive\n",
    "    score += (1 - jaccard_content) * 0.3     # Weight of 0.3\n",
    "\n",
    "    # More stopword similarity → more persuasive\n",
    "    score += op_frac_stop * 0.2              # Weight of 0.2\n",
    "\n",
    "    # Less overall similarity → more persuasive\n",
    "    score += (1 - reply_frac_all) * 0.1      # Weight of 0.1\n",
    "\n",
    "    return score"
   ],
   "metadata": {
    "id": "tJoU24EMoBd6"
   },
   "id": "tJoU24EMoBd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1d9c6199351dc388",
    "outputId": "d3d34172-293b-410b-b948-808e20ca416b"
   },
   "cell_type": "code",
   "source": [
    "# Test the OP and path pairer:\n",
    "utt_id = change_points[0][1]\n",
    "user_id = corpus.get_utterance(change_points[0][1]).speaker.id\n",
    "op_path_pairer = OpPathPairer(corpus, timelines=timelines)\n",
    "\n",
    "# List of tuples:\n",
    "op_path_pairs = op_path_pairer.extract_rooted_path_from_candidate_convos(candidate_convos, user_id)\n",
    "\n",
    "# def print_user_path_utterances(utterances):\n",
    "#   for i, utt in enumerate(utterances):\n",
    "#     print(f'{i}, {utt.text}\\n')\n",
    "\n",
    "for op_path_pair in op_path_pairs:\n",
    "  print(1000*'=')\n",
    "  print(f'\\nop: {op_path_pair[0].id}\\n')\n",
    "  for path, utterances in op_path_pair[1].items():\n",
    "    print(f'path: {path}, utterances: {[utt.text for utt in utterances]}\\n')"
   ],
   "id": "1d9c6199351dc388",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(op_path_pairs[3])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "o9_WlwAH7WVq",
    "outputId": "b704dc85-4753-4e4e-8161-cb590d36c66e"
   },
   "id": "o9_WlwAH7WVq",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the preprocessor:\n",
    "pair_preprocessor = PairPreprocessor()\n",
    "pair = op_path_pairs[3]\n",
    "# So it should take a tuple, where the second part of the tuple is a dictionary of path_key, list of utterance pairs\n",
    "\n",
    "# WHY THIS WORKS????\n",
    "# for k, v in pair[1].items():\n",
    "#     print(k, v)\n",
    "\n",
    "preprocessed_pairs = pair_preprocessor.concatenate_path_in_all_pairs(op_path_pairs)\n",
    "\n",
    "for pair in preprocessed_pairs:\n",
    "    for k, utt_text in pair[1].items():\n",
    "        print(1000*'=')\n",
    "        print(f'Text: {utt_text}\\n')\n",
    "\n",
    "# Now do it for all pairs\n",
    "\n",
    "# DONT FORGET TO REMOVE DELETED ETC.\n",
    "# SHIT BUT CAN I KEEP THEM TO ANALYSE THEM SOMEHOW?\n",
    "# AS LONG AS THEY ARE IN THE SAME PATH I THINK I CAN KEEP THEM?\n"
   ],
   "metadata": {
    "id": "OEmCUli_WtEn"
   },
   "id": "OEmCUli_WtEn",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# REMEMBER TO EXTRACT THIS FOR PYCAHRM\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stop words\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "print(len(stop_words_set))\n",
    "print(list(stop_words_set)[:20])  # Show first 20 stop words"
   ],
   "metadata": {
    "id": "d9mRlswpfbtX",
    "outputId": "e758dd45-1caa-41e1-e549-7e8a523c173e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d9mRlswpfbtX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test interplay features extraction\n",
    "features_list = []\n",
    "for op, paths in preprocessed_pairs:\n",
    "    for k, concatenated_utts in paths.items():\n",
    "        interplay_features = calculate_interplay_features(op.text, concatenated_utts, stop_words_set)\n",
    "        features_list.append(interplay_features)\n",
    "\n",
    "for interplay_features in features_list:\n",
    "    print(interplay_features)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QGUIAFdGW3Ak",
    "outputId": "1a562659-4dd6-4055-c7b5-1a614e66d4e5"
   },
   "id": "QGUIAFdGW3Ak",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test interplay scoring\n",
    "scores = []\n",
    "for interplay_features in features_list:\n",
    "    score = calculate_persuasion_score(interplay_features)\n",
    "    scores.append(score)\n",
    "\n",
    "for score in scores:\n",
    "  print(score)"
   ],
   "metadata": {
    "id": "1SNob2xVn7SJ",
    "outputId": "db821d8a-269f-4c37-b410-9f7424d778d6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "1SNob2xVn7SJ",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "973891d8aa6df83c"
   },
   "cell_type": "code",
   "source": [
    "# So now thnk of all things that need to be tidied up.\n",
    "# Need to score things properly according to what they said.\n",
    "# Need to find that previous chat where I was given the interplay code and in general tighten up the interplay code\n",
    "# Need to look into potentially more preprocessing if needed\n",
    "# Need to find proper stop words\n",
    "# Need to make it so that it runs on the entire dataset."
   ],
   "id": "973891d8aa6df83c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Build once at the start\n",
    "def build_global_user_conversations_index(corpus):\n",
    "    \"\"\"Build sorted conversations for ALL users upfront\"\"\"\n",
    "    print(\"Building global user conversations index...\")\n",
    "    user_conversations = {}\n",
    "\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # Get all speakers in this conversation\n",
    "        speakers = {utt.speaker.id for utt in convo.iter_utterances()}\n",
    "\n",
    "        # Add this conversation to each speaker's list\n",
    "        for speaker_id in speakers:\n",
    "            if speaker_id not in user_conversations:\n",
    "                user_conversations[speaker_id] = []\n",
    "            user_conversations[speaker_id].append(convo)\n",
    "\n",
    "    # Sort each user's conversations once\n",
    "    for speaker_id in user_conversations:\n",
    "        user_conversations[speaker_id].sort(\n",
    "            key=lambda convo: min(utt.timestamp for utt in convo.iter_utterances())\n",
    "        )\n",
    "\n",
    "    print(f\"Index built for {len(user_conversations)} users!\")\n",
    "    return user_conversations\n",
    "\n",
    "# I can also have it as a global variable instead:\n",
    "# global_user_convos = build_global_user_conversations_index(corpus)"
   ],
   "metadata": {
    "id": "lMdEFDN5M1_P"
   },
   "id": "lMdEFDN5M1_P",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class WindowExtractor:\n",
    "    \"\"\" Find the conversations around the change point \"\"\"\n",
    "    def __init__(self, corpus, timelines):\n",
    "        self.corpus = corpus\n",
    "        self.timelines = timelines\n",
    "        self.user_conversations_cache = {}  # Add cache\n",
    "\n",
    "    def get_user_conversations_chronological_old(self, corpus, speaker_id):\n",
    "        \"\"\"Get all conversations for a user in chronological order.\"\"\"\n",
    "\n",
    "        # Check cache first\n",
    "        if speaker_id in self.user_conversations_cache:\n",
    "            return self.user_conversations_cache[speaker_id]\n",
    "\n",
    "        # Get all conversations where the speaker participated\n",
    "        user_conversations = [convo for convo in corpus.iter_conversations()\n",
    "                              if speaker_id in [utt.speaker.id for utt in convo.iter_utterances()]]\n",
    "\n",
    "        # Sort conversations by their earliest timestamp\n",
    "        user_conversations.sort(key=lambda convo: min(utt.timestamp for utt in convo.iter_utterances()))\n",
    "\n",
    "        # Cache the result\n",
    "        self.user_conversations_cache[speaker_id] = user_conversations\n",
    "\n",
    "        return user_conversations\n",
    "\n",
    "    def get_user_conversations_chronological(self, corpus, speaker_id):\n",
    "        return self.user_conversations_cache.get(speaker_id, [])\n",
    "\n",
    "    def get_conversations_around_change_point(self, corpus, change_point):\n",
    "        # Get first change (probably only one I need)\n",
    "        utterance = corpus.get_utterance(change_point)\n",
    "\n",
    "        # Find the convo this utterance belongs to:\n",
    "        conversation = utterance.get_conversation()\n",
    "\n",
    "        # Put all user's convos in a list\n",
    "        speaker_id = utterance.speaker.id\n",
    "        user_conversations = self.get_user_conversations_chronological(corpus, speaker_id)\n",
    "\n",
    "        candidate_convos = []\n",
    "        # find the index of the convo, and return the convo id of the 3 prior convos\n",
    "        for i, convo in enumerate(user_conversations):\n",
    "            if conversation.id == user_conversations[i].id:\n",
    "                # Check if there are at least two conversations before the current one\n",
    "                if i >= 2:\n",
    "                    candidate_convos.append(user_conversations[i - 2])\n",
    "                    candidate_convos.append(user_conversations[i - 1])\n",
    "                elif i == 1:\n",
    "                     # If only one conversation before, append that one\n",
    "                     candidate_convos.append(user_conversations[i-1])\n",
    "\n",
    "                # Append the current conversation with the change point\n",
    "                candidate_convos.append(conversation)\n",
    "                break # Found the conversation, no need to continue the loop\n",
    "\n",
    "        return candidate_convos"
   ],
   "metadata": {
    "id": "5_gKC9RS9gbq"
   },
   "id": "5_gKC9RS9gbq",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "timelines = timeline_builder.build_timelines()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDIwj95hC1PK",
    "outputId": "dad81622-f2ae-4e60-bdae-6feb98cf1d55"
   },
   "id": "EDIwj95hC1PK",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "speakers = list(corpus.iter_speakers())\n",
    "print(len(speakers))"
   ],
   "metadata": {
    "id": "fRn1SRffhZhK",
    "outputId": "f18efbce-b3bb-4226-cec0-7d18292629d3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "fRn1SRffhZhK",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "window_extractor = WindowExtractor(corpus, timelines=timelines)\n",
    "# Build the cache\n",
    "print(\"Pre-building user conversation index...\")\n",
    "window_extractor.user_conversations_cache = build_global_user_conversations_index(corpus)"
   ],
   "metadata": {
    "id": "sZuw1DPyQtJ8",
    "outputId": "856abf19-e02c-4e28-f6c2-9cdc652aca6b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "sZuw1DPyQtJ8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Test persuation analysis coordinator\n",
    "import time\n",
    "\n",
    "\n",
    "# For topic_timeline in timelines:\n",
    "pair_preprocessor = PairPreprocessor()\n",
    "persistence_detector_new = ChangeDetector()\n",
    "window_extractor = WindowExtractor(corpus, timelines=timelines)\n",
    "op_path_pairer = OpPathPairer(corpus, timelines=timelines)\n",
    "\n",
    "# use the groups\n",
    "groups = persistence_detector_new.get_two_groups(timelines)\n",
    "groups_tuple = (groups['with_changes'], groups['no_changes'])\n",
    "\n",
    "# Init\n",
    "i = 0\n",
    "group_means = [] # Initialize as a list to append means\n",
    "group_scores = []\n",
    "utts_num = 0\n",
    "\n",
    "# For each group\n",
    "for group in groups_tuple:\n",
    "    current_group_scores = []\n",
    "\n",
    "    for user_id, topic_timelines in group.items():\n",
    "        user_start_time = time.time()\n",
    "        user_change_points = 0\n",
    "\n",
    "        for topic_timeline in topic_timelines.values():\n",
    "\n",
    "            for change_point in topic_timeline.keys(): # Iterate through change points (keys)\n",
    "                print(f'User: {user_id}, topic: {topic_timeline}, change point {change_point}')\n",
    "                utts_num += 1\n",
    "\n",
    "                user_change_points += 1\n",
    "\n",
    "                # TIME: Window extraction\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    candidate_convos = window_extractor.get_conversations_around_change_point(\n",
    "                        change_point=change_point, corpus=corpus\n",
    "                    )\n",
    "                    window_time = time.time() - start_time\n",
    "                    print(f'⏱️ Window extraction: {window_time:.3f}s')\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping change point {change_point}: {e}\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # TIME: Path extraction\n",
    "                start_time = time.time()\n",
    "                op_path_pairs = []\n",
    "                for candidate_convo in candidate_convos:\n",
    "                    try:\n",
    "                        op_path_pairs.extend(op_path_pairer.extract_rooted_path_from_candidate_convos(\n",
    "                            [candidate_convo], user_id\n",
    "                        ))\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Skipping conversation {candidate_convo.id}: {e}\")\n",
    "                        continue\n",
    "                path_time = time.time() - start_time\n",
    "                print(f'⏱️ Path extraction: {path_time:.3f}s')\n",
    "\n",
    "\n",
    "                # TIME: Preprocessing\n",
    "                start_time = time.time()\n",
    "                preprocessed_pairs = pair_preprocessor.concatenate_path_in_all_pairs(op_path_pairs)\n",
    "                preprocess_time = time.time() - start_time\n",
    "                print(f'⏱️ Preprocessing: {preprocess_time:.3f}s')\n",
    "\n",
    "\n",
    "                # TIME: Feature extraction\n",
    "                start_time = time.time()\n",
    "                features_list = []\n",
    "                for op, paths in preprocessed_pairs:\n",
    "                    for k, concatenated_utts in paths.items():\n",
    "                        interplay_features = calculate_interplay_features(\n",
    "                            op.text, concatenated_utts, stop_words_set\n",
    "                        )\n",
    "                        features_list.append(interplay_features)\n",
    "                feature_time = time.time() - start_time\n",
    "                print(f'⏱️ Feature extraction: {feature_time:.3f}s')\n",
    "\n",
    "\n",
    "                # TIME: Scoring\n",
    "                start_time = time.time()\n",
    "                scores = []\n",
    "                for interplay_features in features_list:\n",
    "                    score = calculate_persuasion_score(interplay_features)\n",
    "                    scores.append(score)\n",
    "                scoring_time = time.time() - start_time\n",
    "                print(f'⏱️ Scoring: {scoring_time:.3f}s')\n",
    "                total_time = window_time + path_time + preprocess_time + feature_time + scoring_time\n",
    "                print(f'🔥 TOTAL for change point: {total_time:.3f}s\\n')\n",
    "\n",
    "                # Print total time for this change point\n",
    "                total_time = window_time + path_time + preprocess_time + feature_time + scoring_time\n",
    "                print(f'🔥 TOTAL for change point: {total_time:.3f}s\\n')\n",
    "\n",
    "                current_group_scores.extend(scores)\n",
    "\n",
    "        # TIME: End timing this user\n",
    "        user_total_time = time.time() - user_start_time\n",
    "        print(f'👤 USER {user_id} TOTAL: {user_total_time:.3f}s ({user_change_points} change points)')\n",
    "        print(f'📊 Average per change point: {user_total_time/max(1, user_change_points):.3f}s\\n')\n",
    "\n",
    "    # Calculate mean for this group\n",
    "    total = 0\n",
    "    num_of_scores = 0\n",
    "    for score in current_group_scores: # Iterate through individual scores\n",
    "        total += score\n",
    "        num_of_scores += 1\n",
    "\n",
    "    group_mean = total / num_of_scores if num_of_scores > 0 else 0 # Handle division by zero\n",
    "    group_means.append(group_mean) # Append mean to the list\n",
    "\n",
    "# Print the calculated group means\n",
    "print(f'Group Means: {group_means}')"
   ],
   "metadata": {
    "id": "iP__zCCBgPeO",
    "outputId": "8b1c0743-37de-4099-f67c-ffb4a5a0ba31",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "iP__zCCBgPeO",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for user_id, topic_timelines in timelines.items():\n",
    "    for topic_timeline in topic_timelines.values():\n",
    "        print(topic_timeline)\n",
    "        break\n",
    "    break"
   ],
   "metadata": {
    "id": "y23AmKyosPVN"
   },
   "id": "y23AmKyosPVN",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Maybe first collect each group, then have them in a list/tuple and run the coordinator on that.\n",
    "\n",
    "# So then that new function would take the features of this path and attatch a score to it.\n",
    "\n",
    "# And that's it. Now I only need to glue things together.\n",
    "\n",
    "# Should I make less topics?\n",
    "\n",
    "# I think I could make a simple model quick and go manual as a backup\n",
    "\n",
    "\n",
    "\n",
    "# Filter timelines, then\n",
    "# For each user in timelines:\n",
    "# Loops through all all topic_timelines and finds all change points (should be 1 for each topic_timeline)\n",
    "# Loops through all then takes these from a list and finds the conversations around that period (5-10)\n",
    "# Loops through all convos in that structure and creates a list of op,paths pairs\n",
    "# Extracts features and calculates the score\n",
    "\n",
    "\n",
    "# Then another function,\n",
    "# Does the same but for each user:\n",
    "# Loops through all topic_timelines and finds no change points (should be 1 for each topic_timeline)\n",
    "# Then does the same as the previous function\n",
    "\n",
    "# Then at the end I run a stat test for the two groups."
   ],
   "metadata": {
    "id": "x-wDR8rdmO7k"
   },
   "id": "x-wDR8rdmO7k"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
