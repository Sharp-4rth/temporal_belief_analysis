{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git"
   ],
   "metadata": {
    "id": "T9tORvNyIQ77",
    "outputId": "8d10196b-e79d-414c-daf9-edd707aa2acf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "T9tORvNyIQ77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "55edda2b57f0546e",
    "outputId": "b00c0e28-1c8e-42cf-e6c1-6d64c2e6346f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "# Need to restart after:\n",
    "!pip install convokit[llm]\n",
    "!pip install convokit"
   ],
   "id": "55edda2b57f0546e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "729be76f70432f3e",
    "outputId": "0d8aa6f8-4fc8-4ef4-ff33-663f68509c0e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir('/content/temporal_belief_analysis/notebooks')\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "# Absolute path to src directory\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ],
   "id": "729be76f70432f3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "eb72f903280d92dd",
    "outputId": "65b515f9-b6a2-40b4-cd3c-9005d963532a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "!pip install gdown\n",
    "import zipfile\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from convokit import Corpus, download\n",
    "import convokit\n",
    "from temporal_belief.core.timeline_building import TimelineBuilder"
   ],
   "id": "eb72f903280d92dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c8b97a9d177cd0bd",
    "outputId": "dbcb5ba1-9630-4275-8c19-dba5d544a06f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download and unzip with python:\n",
    "!gdown \"https://drive.google.com/file/d/10HNQHIsFe386oBu_o5s9JOuo-wiaofXC/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances150000_llm.zip\" --fuzzy\n",
    "zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances150000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")"
   ],
   "id": "c8b97a9d177cd0bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "95578f869b9ff523"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 22,
   "source": [
    "# Load corpus\n",
    "corpus = Corpus(filename=\"/content/temporal_belief_analysis/pd_corpus_with_stances150000_llm_rerun\")"
   ],
   "id": "95578f869b9ff523"
  },
  {
   "metadata": {
    "id": "3cafed28e28a2aed"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 104,
   "source": [
    "# Inputs:\n",
    "STANCE2ID = {\"neutral\":0, \"left-leaning\":1, \"right-leaning\":2}\n",
    "NORMALIZE = {\n",
    "    \"neutral\":\"neutral\",\n",
    "    \"left\":\"left-leaning\", \"left-leaning\":\"left-leaning\",\n",
    "    \"right\":\"right-leaning\",\"right-leaning\":\"right-leaning\",\n",
    "}\n",
    "VALID = set(STANCE2ID.keys())\n",
    "\n",
    "utterances, labels, utt_ids, user_ids, topics = [], [], [], [], []\n",
    "\n",
    "for u in corpus.iter_utterances():\n",
    "    txt = (u.text or \"\").strip()\n",
    "    stance = u.meta.get('detected_stance')\n",
    "    main_post_id = u.conversation_id\n",
    "    main_post = corpus.get_utterance(main_post_id)\n",
    "    conv = corpus.get_conversation(main_post_id)\n",
    "\n",
    "    conf_raw = u.meta.get('stance_confidence')\n",
    "    # normalize confidence\n",
    "    try:\n",
    "        conf = float(conf_raw)\n",
    "    except (TypeError, ValueError):\n",
    "        conf = None\n",
    "\n",
    "    # --------- SKIP problematic utterances ----------\n",
    "    # missing/empty main post or duplicate of main post\n",
    "    if not txt or txt in {'[removed]', '[deleted]', '.'}:\n",
    "        continue\n",
    "\n",
    "    if not main_post.text or txt == main_post.text:\n",
    "        continue\n",
    "\n",
    "    if 'stance_error' in u.meta.keys() and stance == 'neutral':\n",
    "        continue\n",
    "\n",
    "    if stance not in {'left-leaning', 'right-leaning', 'neutral'}:\n",
    "        continue\n",
    "\n",
    "    # confidence threshold depends on stance\n",
    "    if stance == 'neutral':\n",
    "        if conf is None or conf < 0.9:\n",
    "            continue\n",
    "    elif stance == 'left-leaning':\n",
    "        if conf is None or conf < 0.9:\n",
    "            continue\n",
    "    elif stance == 'right-leaning':\n",
    "        if conf is None or conf < 0.8:\n",
    "            continue\n",
    "\n",
    "    raw = u.meta.get(\"detected_stance\")\n",
    "\n",
    "    if not raw:\n",
    "        continue\n",
    "    lab = NORMALIZE.get(str(raw).strip().lower())  # map aliases\n",
    "    if lab not in VALID:                           # filters Unknown / other\n",
    "        continue\n",
    "\n",
    "    topic = u.meta.get(\"topic\")  # or compute from your mapping\n",
    "\n",
    "    utterances.append(txt)\n",
    "    labels.append(lab)\n",
    "    utt_ids.append(u.id)\n",
    "    user_ids.append(u.speaker.id if u.speaker else \"unknown\")\n",
    "    topics.append(topic)"
   ],
   "id": "3cafed28e28a2aed"
  },
  {
   "metadata": {
    "id": "11362b5f669438b6",
    "outputId": "01f18bbd-d452-4d0f-dff7-a6a46701dce8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final counts per label:\n",
      "Counter({'left-leaning': 6334, 'right-leaning': 6000, 'neutral': 2000})\n"
     ]
    }
   ],
   "execution_count": 106,
   "source": [
    "# --- Cap max utterances per label ---\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Set your per-label maximums here:\n",
    "MAX_PER_LABEL = {\n",
    "    \"neutral\": 2000,\n",
    "    \"left-leaning\": 9000,\n",
    "    \"right-leaning\": 6000,\n",
    "}\n",
    "\n",
    "# Shuffle indices first for randomness\n",
    "all_idx = np.arange(len(labels))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(all_idx)\n",
    "\n",
    "# Collect capped indices\n",
    "label_counts = defaultdict(int)\n",
    "keep_idx = []\n",
    "\n",
    "for i in all_idx:\n",
    "    lab = labels[i]\n",
    "    if lab in MAX_PER_LABEL and label_counts[lab] < MAX_PER_LABEL[lab]:\n",
    "        keep_idx.append(i)\n",
    "        label_counts[lab] += 1\n",
    "\n",
    "# Apply filtering\n",
    "utterances = [utterances[i] for i in keep_idx]\n",
    "labels     = [labels[i] for i in keep_idx]\n",
    "utt_ids    = [utt_ids[i] for i in keep_idx]\n",
    "user_ids   = [user_ids[i] for i in keep_idx]\n",
    "topics     = [topics[i] for i in keep_idx]\n",
    "\n",
    "print(\"Final counts per label:\")\n",
    "from collections import Counter\n",
    "print(Counter(labels))"
   ],
   "id": "11362b5f669438b6"
  },
  {
   "metadata": {
    "id": "b18006c0e326e315",
    "outputId": "2cfd6a46-142f-44a8-8e1b-de24bae74c1b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kept 14334 labeled utterances.\n"
     ]
    }
   ],
   "execution_count": 107,
   "source": [
    "# make a user-based split\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(42)\n",
    "uniq_users = np.array(sorted(set(user_ids)))\n",
    "rng.shuffle(uniq_users)\n",
    "cut = int(0.8 * len(uniq_users))\n",
    "train_users = set(uniq_users[:cut])\n",
    "val_users   = set(uniq_users[cut:])\n",
    "\n",
    "train_idx = np.array([i for i,u in enumerate(user_ids) if u in train_users])\n",
    "val_idx   = np.array([i for i,u in enumerate(user_ids) if u in val_users])\n",
    "\n",
    "# Sanity\n",
    "assert len(utterances) == len(labels) == len(utt_ids) == len(user_ids)\n",
    "\n",
    "print(f\"Kept {len(utterances)} labeled utterances.\")"
   ],
   "id": "b18006c0e326e315"
  },
  {
   "metadata": {
    "id": "5d752fd0afc4aa37",
    "outputId": "eae3d015-a510-4edb-92f7-a3ee831868a0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overall:\n",
      "  left-leaning :   6334 (44.2%)\n",
      "  neutral      :   2000 (14.0%)\n",
      "  right-leaning:   6000 (41.9%)\n",
      "\n",
      "Train split:\n",
      "  left-leaning :   5108 (44.6%)\n",
      "  neutral      :   1661 (14.5%)\n",
      "  right-leaning:   4687 (40.9%)\n",
      "\n",
      "Val split:\n",
      "  left-leaning :   1226 (42.6%)\n",
      "  neutral      :    339 (11.8%)\n",
      "  right-leaning:   1313 (45.6%)\n",
      "\n",
      "Class weights (index order per STANCE2ID): [2.389      0.75434166 0.7963333 ]\n"
     ]
    }
   ],
   "execution_count": 108,
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "order = ['left-leaning', 'neutral', 'right-leaning']  # desired print order\n",
    "\n",
    "# --- overall counts ---\n",
    "all_counts = Counter(labels)\n",
    "total = len(labels)\n",
    "print(\"Overall:\")\n",
    "for k in order:\n",
    "    c = all_counts.get(k, 0)\n",
    "    print(f\"  {k:13s}: {c:6d} ({c/total:.1%})\")\n",
    "\n",
    "# --- per-split counts (uses your train_idx / val_idx) ---\n",
    "def counts_for_idx(idx):\n",
    "    return Counter(labels[i] for i in idx)\n",
    "\n",
    "print(\"\\nTrain split:\")\n",
    "train_counts = counts_for_idx(train_idx)\n",
    "for k in order:\n",
    "    c = train_counts.get(k, 0)\n",
    "    print(f\"  {k:13s}: {c:6d} ({c/len(train_idx):.1%})\")\n",
    "\n",
    "print(\"\\nVal split:\")\n",
    "val_counts = counts_for_idx(val_idx)\n",
    "for k in order:\n",
    "    c = val_counts.get(k, 0)\n",
    "    print(f\"  {k:13s}: {c:6d} ({c/len(val_idx):.1%})\")\n",
    "\n",
    "print(\"\\nClass weights (index order per STANCE2ID):\", class_weights)"
   ],
   "id": "5d752fd0afc4aa37"
  },
  {
   "metadata": {
    "id": "9cbab9abfaaab061",
    "outputId": "231751a8-62d6-47a6-ad45-6b5452af62ef",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-3597298377.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1790/1790 09:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.210900</td>\n",
       "      <td>0.881690</td>\n",
       "      <td>0.582349</td>\n",
       "      <td>0.570145</td>\n",
       "      <td>0.597093</td>\n",
       "      <td>0.581284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.883045</td>\n",
       "      <td>0.586171</td>\n",
       "      <td>0.569958</td>\n",
       "      <td>0.592835</td>\n",
       "      <td>0.579884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>0.969574</td>\n",
       "      <td>0.601112</td>\n",
       "      <td>0.602691</td>\n",
       "      <td>0.579916</td>\n",
       "      <td>0.585798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>0.996736</td>\n",
       "      <td>0.609451</td>\n",
       "      <td>0.599941</td>\n",
       "      <td>0.578353</td>\n",
       "      <td>0.586785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>1.113419</td>\n",
       "      <td>0.604587</td>\n",
       "      <td>0.590604</td>\n",
       "      <td>0.586767</td>\n",
       "      <td>0.588639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('stance-clf-best-domain-adapted/tokenizer_config.json',\n",
       " 'stance-clf-best-domain-adapted/special_tokens_map.json',\n",
       " 'stance-clf-best-domain-adapted/vocab.txt',\n",
       " 'stance-clf-best-domain-adapted/bpe.codes',\n",
       " 'stance-clf-best-domain-adapted/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "execution_count": 110,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch, numpy as np\n",
    "from transformers import EarlyStoppingCallback\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler  # <-- add this\n",
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "# STANCE2ID = {\"neutral\":0, \"left-leaning\":1, \"right-leaning\":2}\n",
    "ID2STANCE = {v:k for k,v in STANCE2ID.items()}\n",
    "MAX_LEN = 128\n",
    "\n",
    "# utterances: list[str]; labels: list[str] aligned with utterances\n",
    "# Prefer split by USER so test utterances come from unseen users.\n",
    "utterances = utterances   # texts\n",
    "labels     = labels\n",
    "train_idx, val_idx = train_idx, val_idx # indices for your split (ideally user-based)\n",
    "\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"/content/temporal_belief_analysis/stance-clf-best\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"/content/temporal_belief_analysis/stance-clf-best\", num_labels=3, id2label=ID2STANCE, label2id=STANCE2ID\n",
    ")\n",
    "\n",
    "class StanceDS(Dataset):\n",
    "    def __init__(self, texts, labs):\n",
    "        self.enc = tok(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        self.y = torch.tensor([STANCE2ID[l] for l in labs], dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        item = {k:v[i] for k,v in self.enc.items()}\n",
    "        item[\"labels\"] = self.y[i]\n",
    "        return item\n",
    "\n",
    "train_ds = StanceDS([utterances[i] for i in train_idx], [labels[i] for i in train_idx])\n",
    "val_ds   = StanceDS([utterances[i] for i in val_idx],   [labels[i] for i in val_idx])\n",
    "\n",
    "train_counts = Counter(labels[i] for i in train_idx)\n",
    "num_classes = len(STANCE2ID)\n",
    "total_train = len(train_idx)\n",
    "\n",
    "# build inverse-freq weights\n",
    "weights_np = np.zeros(num_classes, dtype=np.float32)\n",
    "for stance, idx in STANCE2ID.items():\n",
    "    freq = train_counts.get(stance, 1)\n",
    "    weights_np[idx] = total_train / (num_classes * freq)\n",
    "\n",
    "# explicit boosts\n",
    "weights_np[STANCE2ID[\"neutral\"]]      *= 0.7\n",
    "weights_np[STANCE2ID[\"left-leaning\"]] *= 1.1\n",
    "weights_np[STANCE2ID[\"right-leaning\"]]*= 1.8\n",
    "\n",
    "# tensor + normalize\n",
    "class_weights = torch.tensor(weights_np, dtype=torch.float)\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "# class_weights = torch.clamp(class_weights, 0.7, 1.20)  # optional, mild\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # make sure we store a torch tensor\n",
    "        if isinstance(class_weights, np.ndarray):\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "        elif class_weights is not None and not isinstance(class_weights, torch.Tensor):\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            w = self.class_weights.to(device=logits.device, dtype=logits.dtype)\n",
    "        else:\n",
    "            w = None\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=w, label_smoothing=0.1)\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        inputs[\"labels\"] = labels\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    y_pred = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_precision\": p,\n",
    "        \"macro_recall\": r,\n",
    "        \"macro_f1\": f1,\n",
    "    }\n",
    "\n",
    "# init training argument\n",
    "args = TrainingArguments(\n",
    "    \"stance-clf\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,                 # allow up to 5 but early-stop\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",   # <-- change\n",
    "    greater_is_better=True,             # <-- change\n",
    "    warmup_ratio=0.08,\n",
    "    weight_decay=0.005,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=[],\n",
    "    logging_strategy=\"epoch\",       # <-- add this\n",
    "    logging_first_step=True\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    # class_weights=class_weights\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"stance-clf-best-domain-adapted\")\n",
    "tok.save_pretrained(\"stance-clf-best-domain-adapted\")"
   ],
   "id": "9cbab9abfaaab061"
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.state.best_model_checkpoint and trainer.state.best_metric"
   ],
   "metadata": {
    "id": "SG40Q-AZ38LV",
    "outputId": "1e5e6b23-5d58-4368-bdd6-9ada020f8553",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "SG40Q-AZ38LV",
   "execution_count": 112,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5886393061477343"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ]
  },
  {
   "metadata": {
    "id": "3191ddb3d088be61",
    "outputId": "f595114f-893e-4463-cd6f-4aa9b468dc50",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "      neutral       0.55      0.53      0.54       339\n",
      " left-leaning       0.58      0.59      0.59      1226\n",
      "right-leaning       0.64      0.64      0.64      1313\n",
      "\n",
      "     accuracy                           0.60      2878\n",
      "    macro avg       0.59      0.59      0.59      2878\n",
      " weighted avg       0.60      0.60      0.60      2878\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[181  95  63]\n",
      " [ 94 722 410]\n",
      " [ 54 422 837]]\n"
     ]
    }
   ],
   "execution_count": 111,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "out = trainer.predict(val_ds)\n",
    "y_pred = np.argmax(out.predictions, axis=1)\n",
    "y_true = out.label_ids\n",
    "\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=[\"neutral\",\"left-leaning\",\"right-leaning\"],\n",
    "    digits=2\n",
    "))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ],
   "id": "3191ddb3d088be61"
  },
  {
   "metadata": {
    "id": "5ff307c191cd278d"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "5ff307c191cd278d"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
