{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "# Need to restart after:\n",
        "!pip install convokit[llm]\n",
        "!pip install convokit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir('/content/temporal_belief_analysis/notebooks')\n",
        "print(\"Changed working directory to:\", os.getcwd())\n",
        "\n",
        "# Absolute path to src directory\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ],
      "metadata": {
        "id": "dAxzmPSXsoAb",
        "outputId": "931e4b56-78fa-4118-cb33-9caf0987207a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "dAxzmPSXsoAb",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/temporal_belief_analysis/notebooks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1235228885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/temporal_belief_analysis/notebooks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Changed working directory to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/temporal_belief_analysis/notebooks'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "!pip install gdown\n",
        "import zipfile\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from convokit import Corpus, download\n",
        "import convokit\n",
        "from temporal_belief.core.timeline_building import TimelineBuilder"
      ],
      "metadata": {
        "id": "njypokE6so1k",
        "outputId": "4a644894-f724-4ac5-f2ff-50281a06edc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "njypokE6so1k",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip with python (Dataloading):\n",
        "# !gdown \"https://drive.google.com/file/d/1N0U_jUJlOYjdaju2FaU8p87uB22YBxJ0/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/1DLFY6JLMZqNjwvNRZmhlV4-rnoQP_eyH/view?usp=sharing\" -O \"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/1nWaj5N8nsG7u5homv_kAh4CLPDv01M_Z/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_topics.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/15NMRXEkGRoGjK6TXFBHIMOPjkTyZ0keP/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances200000_llm.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/15nVf6Js0KsDxA9HaB0zCXhc8VdcC-Fy-/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances75000_llm.zip\" --fuzzy\n",
        "!gdown \"https://drive.google.com/file/d/1dOUvQmtjFrXq0hJvnOsP_ZLqdGEyOVNJ/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances100000_llm.zip\" --fuzzy\n",
        "\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_topics.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances200000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances75000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_llm.zip\").extractall(\"/content/temporal_belief_analysis\")"
      ],
      "metadata": {
        "id": "Er6Nh6PysqxG",
        "outputId": "dd858b37-532f-424c-f955-7aef280990cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Er6Nh6PysqxG",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dOUvQmtjFrXq0hJvnOsP_ZLqdGEyOVNJ\n",
            "From (redirected): https://drive.google.com/uc?id=1dOUvQmtjFrXq0hJvnOsP_ZLqdGEyOVNJ&confirm=t&uuid=69dd2b29-aada-4c9d-bfe0-88c684a8df1e\n",
            "To: /content/temporal_belief_analysis/pd_corpus_with_stances100000_llm.zip\n",
            "100% 834M/834M [00:09<00:00, 92.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus(filename=\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_llm\")"
      ],
      "metadata": {
        "id": "fQTXbA7Gss7h"
      },
      "id": "fQTXbA7Gss7h",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 0) Inputs: a ConvoKit Corpus ===\n",
        "# from convokit import Corpus\n",
        "# corpus = Corpus(\"path_or_url\")\n",
        "\n",
        "STANCE2ID = {\"neutral\":0, \"left-leaning\":1, \"right-leaning\":2}\n",
        "NORMALIZE = {\n",
        "    \"neutral\":\"neutral\",\n",
        "    \"left\":\"left-leaning\", \"left-leaning\":\"left-leaning\",\n",
        "    \"right\":\"right-leaning\",\"right-leaning\":\"right-leaning\",\n",
        "}\n",
        "VALID = set(STANCE2ID.keys())\n",
        "\n",
        "utterances, labels, utt_ids, user_ids, topics = [], [], [], [], []\n",
        "\n",
        "for u in corpus.iter_utterances():\n",
        "    txt = (u.text or \"\").strip()\n",
        "    stance = u.meta.get('detected_stance')\n",
        "    main_post_id = u.conversation_id\n",
        "    main_post = corpus.get_utterance(main_post_id)\n",
        "    conv = corpus.get_conversation(main_post_id)\n",
        "\n",
        "    conf_raw = u.meta.get('stance_confidence')\n",
        "    # normalize confidence\n",
        "    try:\n",
        "        conf = float(conf_raw)\n",
        "    except (TypeError, ValueError):\n",
        "        conf = None\n",
        "\n",
        "    # --------- SKIP problematic utterances ----------\n",
        "    # missing/empty main post or duplicate of main post\n",
        "    if not txt or txt in {'[removed]', '[deleted]', '.'}:\n",
        "        continue\n",
        "\n",
        "    if not main_post.text or txt == main_post.text:\n",
        "        continue\n",
        "\n",
        "    if 'stance_error' in u.meta.keys() and stance == 'neutral':\n",
        "        continue\n",
        "\n",
        "    if stance not in {'left-leaning', 'right-leaning', 'neutral'}:\n",
        "        continue\n",
        "\n",
        "    # confidence threshold depends on stance\n",
        "    if stance == 'neutral':\n",
        "        if conf is None or conf < 0.9:\n",
        "            continue\n",
        "    elif stance == 'left-leaning':\n",
        "        if conf is None or conf < 0.9:\n",
        "            continue\n",
        "    elif stance == 'right-leaning':\n",
        "        if conf is None or conf < 0.8:\n",
        "            continue\n",
        "\n",
        "    # --- EDIT HERE: pull topic if you have it, else use None/\"\" ---\n",
        "\n",
        "    raw = u.meta.get(\"detected_stance\")\n",
        "\n",
        "    # ------------------------------------------------\n",
        "\n",
        "    # Okay also check this from runpod\n",
        "\n",
        "    if not raw:\n",
        "        continue\n",
        "    lab = NORMALIZE.get(str(raw).strip().lower())  # map aliases\n",
        "    if lab not in VALID:                           # filters Unknown / other\n",
        "        continue\n",
        "\n",
        "    # --- EDIT HERE: pull topic if you have it, else use None/\"\" ---\n",
        "    topic = u.meta.get(\"topic\")  # or compute from your mapping\n",
        "    # --------------------------------------------------------------\n",
        "\n",
        "    utterances.append(txt)\n",
        "    labels.append(lab)\n",
        "    utt_ids.append(u.id)\n",
        "    user_ids.append(u.speaker.id if u.speaker else \"unknown\")\n",
        "    topics.append(topic)"
      ],
      "metadata": {
        "id": "ckuKVY9EtQ-l"
      },
      "id": "ckuKVY9EtQ-l",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cap max utterances per label ---\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Set your per-label maximums here:\n",
        "MAX_PER_LABEL = {\n",
        "    \"neutral\": 1250,\n",
        "    \"left-leaning\": 2250,\n",
        "    \"right-leaning\": 2500,\n",
        "}\n",
        "\n",
        "# Shuffle indices first for randomness\n",
        "all_idx = np.arange(len(labels))\n",
        "rng = np.random.default_rng(42)\n",
        "rng.shuffle(all_idx)\n",
        "\n",
        "# Collect capped indices\n",
        "label_counts = defaultdict(int)\n",
        "keep_idx = []\n",
        "\n",
        "for i in all_idx:\n",
        "    lab = labels[i]\n",
        "    if lab in MAX_PER_LABEL and label_counts[lab] < MAX_PER_LABEL[lab]:\n",
        "        keep_idx.append(i)\n",
        "        label_counts[lab] += 1\n",
        "\n",
        "# Apply filtering\n",
        "utterances = [utterances[i] for i in keep_idx]\n",
        "labels     = [labels[i] for i in keep_idx]\n",
        "utt_ids    = [utt_ids[i] for i in keep_idx]\n",
        "user_ids   = [user_ids[i] for i in keep_idx]\n",
        "topics     = [topics[i] for i in keep_idx]\n",
        "\n",
        "print(\"Final counts per label:\")\n",
        "from collections import Counter\n",
        "print(Counter(labels))"
      ],
      "metadata": {
        "id": "GALleUjaIC0i",
        "outputId": "1988c52c-6d60-4470-b257-5b5addf4f011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "GALleUjaIC0i",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2859991468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Shuffle indices first for randomness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mall_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a user-based split\n",
        "import numpy as np\n",
        "rng = np.random.default_rng(42)\n",
        "uniq_users = np.array(sorted(set(user_ids)))\n",
        "rng.shuffle(uniq_users)\n",
        "cut = int(0.8 * len(uniq_users))\n",
        "train_users = set(uniq_users[:cut])\n",
        "val_users   = set(uniq_users[cut:])\n",
        "\n",
        "train_idx = np.array([i for i,u in enumerate(user_ids) if u in train_users])\n",
        "val_idx   = np.array([i for i,u in enumerate(user_ids) if u in val_users])\n",
        "\n",
        "# Sanity\n",
        "assert len(utterances) == len(labels) == len(utt_ids) == len(user_ids)\n",
        "\n",
        "print(f\"Kept {len(utterances)} labeled utterances.\")"
      ],
      "metadata": {
        "id": "ywKTqttZKlG0",
        "outputId": "cc63a4a4-ef6f-467d-94f1-9b3cc3618711",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ywKTqttZKlG0",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept 4800 labeled utterances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "order = ['left-leaning', 'neutral', 'right-leaning']  # desired print order\n",
        "\n",
        "# --- overall counts ---\n",
        "all_counts = Counter(labels)\n",
        "total = len(labels)\n",
        "print(\"Overall:\")\n",
        "for k in order:\n",
        "    c = all_counts.get(k, 0)\n",
        "    print(f\"  {k:13s}: {c:6d} ({c/total:.1%})\")\n",
        "\n",
        "# --- per-split counts (uses your train_idx / val_idx) ---\n",
        "def counts_for_idx(idx):\n",
        "    return Counter(labels[i] for i in idx)\n",
        "\n",
        "print(\"\\nTrain split:\")\n",
        "train_counts = counts_for_idx(train_idx)\n",
        "for k in order:\n",
        "    c = train_counts.get(k, 0)\n",
        "    print(f\"  {k:13s}: {c:6d} ({c/len(train_idx):.1%})\")\n",
        "\n",
        "print(\"\\nVal split:\")\n",
        "val_counts = counts_for_idx(val_idx)\n",
        "for k in order:\n",
        "    c = val_counts.get(k, 0)\n",
        "    print(f\"  {k:13s}: {c:6d} ({c/len(val_idx):.1%})\")\n",
        "\n",
        "# --- optional: class weights aligned with STANCE2ID (for weighted loss) ---\n",
        "num_classes = len(STANCE2ID)\n",
        "class_weights = np.zeros(num_classes, dtype=np.float32)\n",
        "for stance, idx in STANCE2ID.items():\n",
        "    freq = all_counts.get(stance, 0)\n",
        "    class_weights[idx] = (total / (num_classes * max(freq, 1)))  # inverse frequency\n",
        "\n",
        "print(\"\\nClass weights (index order per STANCE2ID):\", class_weights)"
      ],
      "metadata": {
        "id": "N1nDYfhmteYu",
        "outputId": "49224844-9569-4093-b0cc-5bf061a303eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "N1nDYfhmteYu",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall:\n",
            "  left-leaning :   1800 (37.5%)\n",
            "  neutral      :   1000 (20.8%)\n",
            "  right-leaning:   2000 (41.7%)\n",
            "\n",
            "Train split:\n",
            "  left-leaning :   1557 (37.1%)\n",
            "  neutral      :    884 (21.1%)\n",
            "  right-leaning:   1757 (41.9%)\n",
            "\n",
            "Val split:\n",
            "  left-leaning :    243 (40.4%)\n",
            "  neutral      :    116 (19.3%)\n",
            "  right-leaning:    243 (40.4%)\n",
            "\n",
            "Class weights (index order per STANCE2ID): [1.6       0.8888889 0.8      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import torch, numpy as np\n",
        "from transformers import EarlyStoppingCallback\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler  # <-- add this\n",
        "from collections import Counter\n",
        "import torch\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from transformers import Trainer\n",
        "import torch.nn as nn\n",
        "\n",
        "# STANCE2ID = {\"neutral\":0, \"left-leaning\":1, \"right-leaning\":2}\n",
        "ID2STANCE = {v:k for k,v in STANCE2ID.items()}\n",
        "MAX_LEN = 128\n",
        "\n",
        "# ==== you provide ====\n",
        "# utterances: list[str]; labels: list[str] aligned with utterances\n",
        "# Prefer split by USER so test utterances come from unseen users.\n",
        "utterances = utterances   # texts\n",
        "labels     = labels\n",
        "train_idx, val_idx = train_idx, val_idx # indices for your split (ideally user-based)\n",
        "# =====================\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=3, id2label=ID2STANCE, label2id=STANCE2ID\n",
        ")\n",
        "\n",
        "class StanceDS(Dataset):\n",
        "    def __init__(self, texts, labs):\n",
        "        self.enc = tok(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
        "        self.y = torch.tensor([STANCE2ID[l] for l in labs], dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        item = {k:v[i] for k,v in self.enc.items()}\n",
        "        item[\"labels\"] = self.y[i]\n",
        "        return item\n",
        "\n",
        "train_ds = StanceDS([utterances[i] for i in train_idx], [labels[i] for i in train_idx])\n",
        "val_ds   = StanceDS([utterances[i] for i in val_idx],   [labels[i] for i in val_idx])\n",
        "\n",
        "# train_counts = Counter(labels[i] for i in train_idx)\n",
        "# num_classes = len(STANCE2ID)\n",
        "# total_train = len(train_idx)\n",
        "\n",
        "# weights_np = np.zeros(num_classes, dtype=np.float32)\n",
        "# for stance, idx in STANCE2ID.items():\n",
        "#     freq = train_counts.get(stance, 1)\n",
        "#     weights_np[idx] = total_train / (num_classes * freq)  # inverse frequency\n",
        "\n",
        "# class_weights = torch.tensor(weights_np, dtype=torch.float)\n",
        "# # after building torch tensor class_weights\n",
        "# class_weights = class_weights / class_weights.mean()\n",
        "# class_weights = torch.clamp(class_weights, 0.9, 1.2)  # softer spread\n",
        "\n",
        "# class WeightedTrainer(Trainer):\n",
        "#     def __init__(self, *args, class_weights=None, **kwargs):\n",
        "#         super().__init__(*args, **kwargs)\n",
        "#         # make sure we store a torch tensor\n",
        "#         if isinstance(class_weights, np.ndarray):\n",
        "#             class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "#         elif class_weights is not None and not isinstance(class_weights, torch.Tensor):\n",
        "#             class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "#         self.class_weights = class_weights\n",
        "\n",
        "#     # accept **kwargs for newer HF versions\n",
        "#     def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "#         labels = inputs.pop(\"labels\")\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.logits\n",
        "\n",
        "#         # ---- FIX: move weights to same device & dtype as logits ----\n",
        "#         if self.class_weights is not None:\n",
        "#             w = self.class_weights.to(device=logits.device, dtype=logits.dtype)\n",
        "#         else:\n",
        "#             w = None\n",
        "#         # ------------------------------------------------------------\n",
        "\n",
        "#         loss_fct = torch.nn.CrossEntropyLoss(weight=w, label_smoothing=0.00)\n",
        "#         loss = loss_fct(logits, labels)\n",
        "\n",
        "#         inputs[\"labels\"] = labels  # optional: put back for HF internals\n",
        "#         return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# class SamplerWeightedTrainer(WeightedTrainer):\n",
        "#     \"\"\"Same as WeightedTrainer, but oversamples RIGHT in the train dataloader.\"\"\"\n",
        "#     def get_train_dataloader(self):\n",
        "#         ds = self.train_dataset\n",
        "#         # ds.y must be a torch.LongTensor of label ids (you have this in StanceDS)\n",
        "#         y = ds.y\n",
        "#         # STANCE2ID order: [neutral=0, left=1, right=2]\n",
        "#         class_boost = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float)  # tune 1.5â€“3.0\n",
        "#         sample_weights = class_boost[y]\n",
        "#         sampler = WeightedRandomSampler(sample_weights,\n",
        "#                                         num_samples=len(sample_weights),\n",
        "#                                         replacement=True)\n",
        "#         return DataLoader(\n",
        "#             ds,\n",
        "#             batch_size=self.args.per_device_train_batch_size,\n",
        "#             sampler=sampler,\n",
        "#             collate_fn=self.data_collator,\n",
        "#             pin_memory=True,\n",
        "#             drop_last=self.args.dataloader_drop_last,\n",
        "#         )\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, y_true = eval_pred\n",
        "    y_pred = logits.argmax(axis=-1)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_precision\": p,\n",
        "        \"macro_recall\": r,\n",
        "        \"macro_f1\": f1,\n",
        "    }\n",
        "\n",
        "# init trainining argument\n",
        "args = TrainingArguments(\n",
        "    \"stance-clf\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=5,                 # allow up to 5 but early-stop\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",   # <-- change\n",
        "    greater_is_better=True,             # <-- change\n",
        "    warmup_ratio=0.01,\n",
        "    weight_decay=0.005,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to=[],\n",
        "    logging_strategy=\"epoch\",       # <-- add this\n",
        "    logging_first_step=True\n",
        ")\n",
        "\n",
        "# Init trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tok,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # optional\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(\"stance-clf-best\")\n",
        "tok.save_pretrained(\"stance-clf-best\")"
      ],
      "metadata": {
        "id": "XLPQaZKOt8I0",
        "outputId": "a1a456d8-2fd9-4d55-f25e-b672df29191e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "id": "XLPQaZKOt8I0",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1305527705.py:141: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='528' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [528/660 00:47 < 00:12, 10.99 it/s, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.962900</td>\n",
              "      <td>0.882614</td>\n",
              "      <td>0.543189</td>\n",
              "      <td>0.541578</td>\n",
              "      <td>0.576214</td>\n",
              "      <td>0.551556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.837000</td>\n",
              "      <td>0.865170</td>\n",
              "      <td>0.579734</td>\n",
              "      <td>0.576892</td>\n",
              "      <td>0.613902</td>\n",
              "      <td>0.585152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.726900</td>\n",
              "      <td>0.873546</td>\n",
              "      <td>0.573090</td>\n",
              "      <td>0.569504</td>\n",
              "      <td>0.603909</td>\n",
              "      <td>0.577025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.628000</td>\n",
              "      <td>0.908838</td>\n",
              "      <td>0.573090</td>\n",
              "      <td>0.570500</td>\n",
              "      <td>0.606913</td>\n",
              "      <td>0.579595</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('stance-clf-best/tokenizer_config.json',\n",
              " 'stance-clf-best/special_tokens_map.json',\n",
              " 'stance-clf-best/vocab.txt',\n",
              " 'stance-clf-best/added_tokens.json',\n",
              " 'stance-clf-best/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "out = trainer.predict(val_ds)\n",
        "y_pred = np.argmax(out.predictions, axis=1)\n",
        "y_true = out.label_ids\n",
        "\n",
        "print(classification_report(\n",
        "    y_true, y_pred,\n",
        "    target_names=[\"neutral\",\"left-leaning\",\"right-leaning\"],\n",
        "    digits=2\n",
        "))\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "AFBFDF0c6hn_",
        "outputId": "c5d3aded-9fa9-4e34-df88-caf20fd0efc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "id": "AFBFDF0c6hn_",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "      neutral       0.52      0.78      0.62       116\n",
            " left-leaning       0.59      0.56      0.57       243\n",
            "right-leaning       0.62      0.51      0.56       243\n",
            "\n",
            "     accuracy                           0.58       602\n",
            "    macro avg       0.58      0.61      0.59       602\n",
            " weighted avg       0.59      0.58      0.58       602\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[ 90  14  12]\n",
            " [ 44 136  63]\n",
            " [ 38  82 123]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLYm6CTGBErb"
      },
      "id": "RLYm6CTGBErb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}