{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Clone Repo\\n\",\n",
    "    \"!git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git\"\n",
    "   ],\n",
    "   \"id\": \"a6134511ae0d06fa\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Need to restart after:\\n\",\n",
    "    \"!pip install convokit[llm]\\n\",\n",
    "    \"!pip install convokit\\n\",\n",
    "    \"!pip install statsmodels\"\n",
    "   ],\n",
    "   \"id\": \"ca9d6e7e2597e920\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"os.chdir('/content/temporal_belief_analysis/notebooks')\\n\",\n",
    "    \"print(\\\"Changed working directory to:\\\", os.getcwd())\\n\",\n",
    "    \"# Absolute path to src directory\\n\",\n",
    "    \"src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\\n\",\n",
    "    \"if src_path not in sys.path:\\n\",\n",
    "    \"    sys.path.insert(0, src_path)\"\n",
    "   ],\n",
    "   \"id\": \"8da55431d4e44ab8\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"import time\\n\",\n",
    "    \"!pip install gdown\\n\",\n",
    "    \"import zipfile\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from convokit import Corpus, download\\n\",\n",
    "    \"import convokit\\n\",\n",
    "    \"from temporal_belief.core.timeline_building import TimelineBuilder\\n\",\n",
    "    \"from temporal_belief.core.CUSUM_change_detection import ChangeDetector\\n\",\n",
    "    \"from temporal_belief.core.window_extraction import WindowExtractor\\n\",\n",
    "    \"from temporal_belief.core.op_path_pairing import OpPathPairer\\n\",\n",
    "    \"from temporal_belief.data.preprocessors import ChangeDetectorPreprocessor\\n\",\n",
    "    \"from temporal_belief.data.preprocessors import PairPreprocessor\\n\",\n",
    "    \"from temporal_belief.data.preprocessors import ExtractFeatures\\n\",\n",
    "    \"from temporal_belief.data.preprocessors import GroupPreprocessor\\n\",\n",
    "    \"from temporal_belief.core.interplay import Interplay\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"nltk.download('stopwords')\"\n",
    "   ],\n",
    "   \"id\": \"58e4db9f6ee972cd\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"# Download 'r/PoliticalDiscussion' corpus with stance labels to root, unzip with python:\\n\",\n",
    "    \"!gdown \\\"https://drive.google.com/file/d/1AIrstrzE259fcVyxJQW4-RwvAkoUyK1x/view?usp=sharing\\\" -O \\\"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\\\" --fuzzy\\n\",\n",
    "    \"zipfile.ZipFile(\\\"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\\\").extractall(\\\"/content/temporal_belief_analysis\\\")\"\n",
    "   ],\n",
    "   \"id\": \"d80eb0bf377b9a19\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"CORPUS_PATH = \\\"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned\\\"\\n\",\n",
    "    \"corpus = Corpus(filename=CORPUS_PATH)\"\n",
    "   ],\n",
    "   \"id\": \"d35b2192da8b17fe\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"timeline_builder = TimelineBuilder(corpus)\\n\",\n",
    "    \"timelines = timeline_builder.build_timelines()\\n\",\n",
    "    \"change_detector = ChangeDetector()\\n\",\n",
    "    \"window_extractor = WindowExtractor(corpus, timelines)\\n\",\n",
    "    \"window_extractor.build_global_user_conversations_index()\\n\",\n",
    "    \"op_path_pairer = OpPathPairer(corpus, timelines)\\n\",\n",
    "    \"pair_preprocessor = PairPreprocessor()\\n\",\n",
    "    \"groups_preprocessor = GroupPreprocessor()\\n\",\n",
    "    \"groups = change_detector.get_two_groups(timelines)\\n\",\n",
    "    \"feature_extractor = ExtractFeatures()\\n\",\n",
    "    \"persuasion_analyzer = Interplay()\"\n",
    "   ],\n",
    "   \"id\": \"c6ac08ef2dd321bc\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"groups_tuple = (groups['with_changes'], groups['no_changes'])\\n\",\n",
    "    \"groups_tuple = groups_preprocessor.filter_groups(groups, groups_tuple)\\n\",\n",
    "    \"target_utterances = groups_preprocessor.get_target(groups_tuple)\"\n",
    "   ],\n",
    "   \"id\": \"a27bafbebffd6590\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": [\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"stop_words_set = set(stopwords.words('english'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"group_means = []\\n\",\n",
    "    \"group_scores = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For each group\\n\",\n",
    "    \"for group_idx, group in enumerate(tqdm(groups_tuple, desc=\\\"Processing groups\\\")):\\n\",\n",
    "    \"    # Initialize dictionary for this group's scores (one score per utterance)\\n\",\n",
    "    \"    current_group_scores = {\\n\",\n",
    "    \"        'interplay': [],\\n\",\n",
    "    \"        'politeness': [],\\n\",\n",
    "    \"        'argument_complexity': [],\\n\",\n",
    "    \"        'evidence_markers': [],\\n\",\n",
    "    \"        'hedging': []\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"\\n\",\n",
    "    \"    utterances_processed = 0\\n\",\n",
    "    \"    target_reached = False  # Flag to control all nested loops\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for user_id, topic_timelines in group.items():\\n\",\n",
    "    \"        if target_reached:  # Check flag at user level\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"\\n\",\n",
    "    \"        user_start_time = time.time()\\n\",\n",
    "    \"        user_change_points = 0\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for topic_timeline in topic_timelines.values():\\n\",\n",
    "    \"            if target_reached:  # Check flag at topic level\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"\\n\",\n",
    "    \"            for change_point in topic_timeline.keys():  # Each utterance/change point\\n\",\n",
    "    \"                if utterances_processed >= target_utterances:\\n\",\n",
    "    \"                    target_reached = True  # Set flag instead of just breaking\\n\",\n",
    "    \"                    break\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # utts_num += 1\\n\",\n",
    "    \"                user_change_points += 1\\n\",\n",
    "    \"                utterances_processed += 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Window extraction\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"                try:\\n\",\n",
    "    \"                    candidate_convos = window_extractor.get_conversations_around_change_point(\\n\",\n",
    "    \"                        change_point=change_point, corpus=corpus, test=True\\n\",\n",
    "    \"                    )\\n\",\n",
    "    \"                    window_time = time.time() - start_time\\n\",\n",
    "    \"                    print(f'Window extraction: {window_time:.3f}s')\\n\",\n",
    "    \"                except ValueError as e:\\n\",\n",
    "    \"                    print(f\\\"Skipping change point {change_point}: {e}\\\")\\n\",\n",
    "    \"                    continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Path extraction\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"                timeout_duration = 0.25\\n\",\n",
    "    \"                op_path_pairs = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"                for candidate_convo in candidate_convos:\\n\",\n",
    "    \"                    if time.time() - start_time > timeout_duration:\\n\",\n",
    "    \"                        print(f\\\"Path extraction timeout reached ({timeout_duration}s)\\\")\\n\",\n",
    "    \"                        break\\n\",\n",
    "    \"\\n\",\n",
    "    \"                    try:\\n\",\n",
    "    \"                        op_path_pairs.extend(op_path_pairer.extract_rooted_path_from_candidate_convos(\\n\",\n",
    "    \"                            [candidate_convo], user_id\\n\",\n",
    "    \"                        ))\\n\",\n",
    "    \"                    except ValueError as e:\\n\",\n",
    "    \"                        print(f\\\"Skipping conversation {candidate_convo.id}: {e}\\\")\\n\",\n",
    "    \"                        continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"                path_time = time.time() - start_time\\n\",\n",
    "    \"                print(f'Path extraction: {path_time:.3f}s')\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Preprocessing\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"                preprocessed_pairs = pair_preprocessor.concatenate_path_in_all_pairs(op_path_pairs)\\n\",\n",
    "    \"                preprocess_time = time.time() - start_time\\n\",\n",
    "    \"                print(f'Preprocessing: {preprocess_time:.3f}s')\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Feature extraction - collect ALL scores for this utterance\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"                utterance_interplay_scores = []\\n\",\n",
    "    \"                utterance_politeness_scores = []\\n\",\n",
    "    \"                utterance_complexity_scores = []\\n\",\n",
    "    \"                utterance_evidence_scores = []\\n\",\n",
    "    \"                utterance_hedging_scores = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"                for op, paths in preprocessed_pairs:\\n\",\n",
    "    \"                    for k, concatenated_utts in paths.items():\\n\",\n",
    "    \"                        # Extract features\\n\",\n",
    "    \"                        interplay_features = persuasion_analyzer.calculate_interplay_features(\\n\",\n",
    "    \"                            op.text, concatenated_utts, stop_words_set\\n\",\n",
    "    \"                        )\\n\",\n",
    "    \"                        politeness_features = feature_extractor.get_politeness_features(concatenated_utts)\\n\",\n",
    "    \"                        complexity_features = feature_extractor.extract_argument_complexity_features(concatenated_utts)\\n\",\n",
    "    \"                        evidence_features = feature_extractor.extract_evidence_features(concatenated_utts)\\n\",\n",
    "    \"                        hedging_features = feature_extractor.extract_hedging_features(concatenated_utts)\\n\",\n",
    "    \"\\n\",\n",
    "    \"                        # Calculate scores\\n\",\n",
    "    \"                        interplay_score = persuasion_analyzer.calculate_persuasion_score(interplay_features)\\n\",\n",
    "    \"                        politeness_score = sum(politeness_features.values())\\n\",\n",
    "    \"                        complexity_score = feature_extractor.calculate_complexity_score(complexity_features)\\n\",\n",
    "    \"                        evidence_score = feature_extractor.calculate_evidence_score(evidence_features)\\n\",\n",
    "    \"                        hedging_score = feature_extractor.calculate_hedging_score_from_features(hedging_features)\\n\",\n",
    "    \"\\n\",\n",
    "    \"                        # Collect all scores for this utterance\\n\",\n",
    "    \"                        utterance_interplay_scores.append(interplay_score)\\n\",\n",
    "    \"                        utterance_politeness_scores.append(politeness_score)\\n\",\n",
    "    \"                        utterance_complexity_scores.append(complexity_score)\\n\",\n",
    "    \"                        utterance_evidence_scores.append(evidence_score)\\n\",\n",
    "    \"                        utterance_hedging_scores.append(hedging_score)\\n\",\n",
    "    \"\\n\",\n",
    "    \"                feature_time = time.time() - start_time\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Take mean across all paths for this single utterance\\n\",\n",
    "    \"                start_time = time.time()\\n\",\n",
    "    \"                if utterance_interplay_scores:  # Only if we have scores\\n\",\n",
    "    \"                    # One score per utterance (mean of all conversation paths)\\n\",\n",
    "    \"                    utterance_mean_interplay = np.mean(utterance_interplay_scores)\\n\",\n",
    "    \"                    utterance_mean_politeness = np.mean(utterance_politeness_scores)\\n\",\n",
    "    \"                    utterance_mean_complexity = np.mean(utterance_complexity_scores)\\n\",\n",
    "    \"                    utterance_mean_evidence = np.mean(utterance_evidence_scores)\\n\",\n",
    "    \"                    utterance_mean_hedging = np.mean(utterance_hedging_scores)\\n\",\n",
    "    \"\\n\",\n",
    "    \"                    # Add ONE score per utterance to group scores\\n\",\n",
    "    \"                    current_group_scores['interplay'].append(utterance_mean_interplay)\\n\",\n",
    "    \"                    current_group_scores['politeness'].append(utterance_mean_politeness)\\n\",\n",
    "    \"                    current_group_scores['argument_complexity'].append(utterance_mean_complexity)\\n\",\n",
    "    \"                    current_group_scores['evidence_markers'].append(utterance_mean_evidence)\\n\",\n",
    "    \"                    current_group_scores['hedging'].append(utterance_mean_hedging)\\n\",\n",
    "    \"\\n\",\n",
    "    \"                    print(f\\\"Utterance {change_point}: {len(utterance_interplay_scores)} paths -> 1 mean score\\\")\\n\",\n",
    "    \"                    print(f\\\"Group {group_idx + 1}: {utterances_processed}/{target_utterances} utterances processed\\\")\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    print(f\\\"Utterance {change_point}: No valid paths found, skipping\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"                scoring_time = time.time() - start_time\\n\",\n",
    "    \"                print(f'Scoring: {scoring_time:.3f}s')\\n\",\n",
    "    \"\\n\",\n",
    "    \"                # Print total time for this change point\\n\",\n",
    "    \"                total_time = window_time + path_time + preprocess_time + feature_time + scoring_time\\n\",\n",
    "    \"                print(f'TOTAL for utterance: {total_time:.3f}s\\\\n')\\n\",\n",
    "    \"\\n\",\n",
    "    \"        user_total_time = time.time() - user_start_time\\n\",\n",
    "    \"        if user_change_points > 0:  # Only print if user had utterances\\n\",\n",
    "    \"            print(f'USER {user_id}: {user_total_time:.3f}s ({user_change_points} utterances)')\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Calculate means for each predictor for this group\\n\",\n",
    "    \"    group_mean = {}\\n\",\n",
    "    \"    for predictor_name, scores in current_group_scores.items():\\n\",\n",
    "    \"        if scores:\\n\",\n",
    "    \"            group_mean[predictor_name] = np.mean(scores)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            group_mean[predictor_name] = 0\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"\\\\nGroup {group_idx + 1} final sample sizes:\\\")\\n\",\n",
    "    \"    for predictor_name, scores in current_group_scores.items():\\n\",\n",
    "    \"        print(f\\\"  {predictor_name}: n={len(scores)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    print(f\\\"Group {group_idx + 1}: Processed exactly {utterances_processed} utterances\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"    group_means.append(group_mean)\\n\",\n",
    "    \"    group_scores.append(current_group_scores)\"\n",
    "   ],\n",
    "   \"id\": \"5efbbbf0a1d8f24b\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": \"groups_preprocessor.run_statistical_comparison(group_scores)\",\n",
    "   \"id\": \"1ba2bc1fa33d0696\"\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {},\n",
    "   \"cell_type\": \"code\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": null,\n",
    "   \"source\": \"\",\n",
    "   \"id\": \"68b11687c9dc32be\"\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "e3e5f75ecafc15c0"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
