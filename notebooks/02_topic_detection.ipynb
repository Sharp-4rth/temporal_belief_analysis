{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc7c77-ce3e-42a6-881c-b745a7baf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EH8DdCK0Or0F",
   "metadata": {
    "collapsed": true,
    "id": "EH8DdCK0Or0F",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For colab:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbdb0ca-91d4-4a79-bb7e-107fbba420be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Absolute path to src directory\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "klCvx9YUvKv6",
   "metadata": {
    "collapsed": true,
    "id": "klCvx9YUvKv6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get latest version\n",
    "%cd temporal_belief_analysis\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfca0265e767d",
   "metadata": {
    "id": "8ecfca0265e767d"
   },
   "outputs": [],
   "source": [
    "# For colab:\n",
    "!pip install convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9mcx9qMoUA68",
   "metadata": {
    "id": "9mcx9qMoUA68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /root/.convokit/saved-corpora/subreddit-PoliticalDiscussion\n"
     ]
    }
   ],
   "source": [
    "# For colab:\n",
    "import unsloth\n",
    "import unsloth_zoo\n",
    "from convokit import Corpus, download\n",
    "import convokit\n",
    "from temporal_belief.models.bart_classifier import BARTZeroShotClassifier\n",
    "from temporal_belief.utils.config import POLITICAL_TOPICS, ProjectConfig\n",
    "corpus = Corpus(filename=download(\"subreddit-PoliticalDiscussion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3d1e33-a74a-4bcf-a1a8-c0c0a43b2e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we are talking about true AI, the company or country that creates it first sure as hell is not going to sell it. I'm a little confused why you think they would? That type of tech is a game changer and the first to create it is going to be the winner. This isn't a cell phone. War, business, investing, data interpretation, an intelligence that can make a better version of it self and give even better results... You have an advantage the rest of the world could only dream of.\n"
     ]
    }
   ],
   "source": [
    "# Check dataset was loaded:\n",
    "print(corpus.random_utterance().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8n26m1TaPh9b",
   "metadata": {
    "id": "8n26m1TaPh9b"
   },
   "outputs": [],
   "source": [
    "# For local:\n",
    "from src.temporal_belief.models.bart_classifier import BARTZeroShotClassifier\n",
    "from src.temporal_belief.utils.config import POLITICAL_TOPICS, ProjectConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "vPF-AwIWUxJe",
   "metadata": {
    "id": "vPF-AwIWUxJe"
   },
   "outputs": [],
   "source": [
    "\"\"\"Topic detection functionality for conversation analysis.\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TopicDetector:\n",
    "    \"\"\"Detect topics in ConvoKit conversations using BART.\"\"\"\n",
    "\n",
    "    def __init__(self, topics: Optional[List[str]] = None,\n",
    "                 config: ProjectConfig = None):\n",
    "        \"\"\"Initialize topic detector.\"\"\"\n",
    "        self.config = config or ProjectConfig()\n",
    "        self.classifier = BARTZeroShotClassifier(self.config.bart_model_name)\n",
    "        self.topics = topics or POLITICAL_TOPICS\n",
    "        logger.info(f\"Initialized topic detector with {len(self.topics)} topics\")\n",
    "\n",
    "    def detect_conversation_topic(self, conversation) -> Dict[str, Any]:\n",
    "        \"\"\"Detect topic for a single conversation.\"\"\"\n",
    "        utterances = list(conversation.iter_utterances())\n",
    "        \n",
    "        # Safe attribute access\n",
    "        title = conversation.meta.get('title', '')\n",
    "        \n",
    "        # Safe utterance handling\n",
    "        first_utterance = utterances[0] if utterances else None\n",
    "        original_post = first_utterance.text if first_utterance else ''\n",
    "        \n",
    "        if not original_post and not title:\n",
    "            logger.warning(f\"No utterances or title found in conversation {conversation.id}\")\n",
    "            return {\"topic\": \"unknown\", \"confidence\": 0.0}\n",
    "\n",
    "        # Truncate long texts to prevent memory issues\n",
    "        combined_text = f\"Title: {title}. Original Post: {original_post}\"[:2000]\n",
    "        result = self.classifier.classify_text(combined_text, self.topics)\n",
    "\n",
    "        return {\n",
    "            \"topic\": result[\"label\"],\n",
    "            \"confidence\": result[\"confidence\"],\n",
    "            \"all_scores\": result[\"all_scores\"],\n",
    "            \"text_length\": len(original_post),\n",
    "            \"num_utterances\": len(utterances)\n",
    "        }\n",
    "\n",
    "    def process_corpus(self, corpus, batch_size: int = 50,  # Balanced batch size\n",
    "                    save_path: Optional[str] = None) -> None:\n",
    "        \"\"\"Process entire corpus for topic detection.\"\"\"\n",
    "        conversations = list(corpus.iter_conversations())\n",
    "        logger.info(f\"Processing {len(conversations)} conversations for topic detection\")\n",
    "\n",
    "        for i in tqdm(range(0, len(conversations), batch_size),\n",
    "                      desc=\"Processing conversations\"):\n",
    "            batch = conversations[i:i + batch_size]\n",
    "\n",
    "            # Prepare all texts for batch processing\n",
    "            batch_texts = []\n",
    "            valid_conversations = []\n",
    "\n",
    "            for conv in batch:\n",
    "                try:\n",
    "                    # Safe attribute access\n",
    "                    title = conv.meta.get('title', '')\n",
    "                    utterances = list(conv.iter_utterances())\n",
    "                    \n",
    "                    # Safe utterance handling\n",
    "                    first_utterance = utterances[0] if utterances else None\n",
    "                    original_post = first_utterance.text if first_utterance else ''\n",
    "                    \n",
    "                    if not original_post and not title:\n",
    "                        logger.warning(f\"No utterances or title found in conversation {conv.id}\")\n",
    "                        # Set metadata for empty conversations\n",
    "                        conv.add_meta(\"detected_topic\", \"unknown\")\n",
    "                        conv.add_meta(\"topic_confidence\", 0.0)\n",
    "                        conv.add_meta(\"topic_scores\", {})\n",
    "                        continue\n",
    "\n",
    "                    # Truncate long texts\n",
    "                    combined_text = f\"{title}. {original_post}\"[:2000]\n",
    "                    batch_texts.append(combined_text)\n",
    "                    valid_conversations.append(conv)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to prepare conversation {conv.id}: {e}\")\n",
    "                    conv.add_meta(\"detected_topic\", \"unknown\")\n",
    "                    conv.add_meta(\"topic_confidence\", 0.0)\n",
    "                    conv.add_meta(\"topic_scores\", {})\n",
    "\n",
    "            # Process entire batch at once\n",
    "            if batch_texts:\n",
    "                try:\n",
    "                    print(f\"üöÄ Attempting batch of {len(batch_texts)} texts...\")\n",
    "                    import time\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    batch_results = self.classifier.classify_batch(batch_texts, self.topics)\n",
    "                    \n",
    "                    end = time.time()\n",
    "                    print(f\"‚úÖ Batch completed in {end-start:.2f}s ({(end-start)/len(batch_texts):.3f}s per text)\")\n",
    "\n",
    "                    # Apply results back to conversations\n",
    "                    for conv, result in zip(valid_conversations, batch_results):\n",
    "                        conv.add_meta(\"detected_topic\", result[\"label\"])\n",
    "                        conv.add_meta(\"topic_confidence\", result[\"confidence\"])\n",
    "                        conv.add_meta(\"topic_scores\", result[\"all_scores\"])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Batch processing failed: {e}\")\n",
    "                    logger.error(f\"Batch classification failed: {e}\")\n",
    "                    \n",
    "                    # Fallback to individual processing\n",
    "                    for conv in valid_conversations:\n",
    "                        try:\n",
    "                            topic_result = self.detect_conversation_topic(conv)\n",
    "                            conv.add_meta(\"detected_topic\", topic_result[\"topic\"])\n",
    "                            conv.add_meta(\"topic_confidence\", topic_result[\"confidence\"])\n",
    "                            conv.add_meta(\"topic_scores\", topic_result[\"all_scores\"])\n",
    "                        except Exception as e2:\n",
    "                            logger.error(f\"Individual fallback failed for {conv.id}: {e2}\")\n",
    "                            conv.add_meta(\"detected_topic\", \"unknown\")\n",
    "                            conv.add_meta(\"topic_confidence\", 0.0)\n",
    "                            conv.add_meta(\"topic_scores\", {})\n",
    "\n",
    "        if save_path:\n",
    "            corpus.dump(save_path)\n",
    "            logger.info(f\"Saved processed corpus to {save_path}\")\n",
    "\n",
    "        logger.info(\"Topic detection processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "T1ow8xBrcr1e",
   "metadata": {
    "id": "T1ow8xBrcr1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /root/.convokit/saved-corpora/reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "corpus_small = Corpus(filename=download(\"reddit-corpus-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "id": "K8oYk7YLM_oW",
   "metadata": {
    "id": "K8oYk7YLM_oW"
   },
   "source": [
    "# Testing 'process_corpus()'\n",
    "SAVE_PATH = \"/workspace/temporal_belief_analysis/pd_corpus_with_topics\"\n",
    "topic_detector = TopicDetector()\n",
    "topic_detector.process_corpus(corpus, save_path=SAVE_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bad1190-19c0-48fc-ab44-70e17f93aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small.dump(\"/workspace/temporal_belief_analysis/corpus_small_save_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fe3b6-add2-4e28-9ba1-c0c35589cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new cell - test if kernel responds\n",
    "print(\"Kernel alive check\")\n",
    "import time\n",
    "print(f\"Current time: {time.time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38623a2c-6af8-4edd-b2f7-044708597fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First conversation ID: nz1xu\n",
      "Has topic metadata: True\n",
      "Topic: media and political commentary\n",
      "Confidence: 0.3874607980251312\n"
     ]
    }
   ],
   "source": [
    "# Check if metadata gets added\n",
    "conversations = list(corpus.iter_conversations())\n",
    "\n",
    "# Check first conversation\n",
    "first_conv = conversations[1]\n",
    "print(f\"First conversation ID: {first_conv.id}\")\n",
    "print(f\"Has topic metadata: {'detected_topic' in first_conv.meta}\")\n",
    "if 'detected_topic' in first_conv.meta:\n",
    "    print(f\"Topic: {first_conv.meta['detected_topic']}\")\n",
    "    print(f\"Confidence: {first_conv.meta['topic_confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PxXbHWRrMbDK",
   "metadata": {
    "collapsed": true,
    "id": "PxXbHWRrMbDK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing 'detect_conversation_topic()' and 'dump()'\n",
    "i = 0\n",
    "convos_small = list(corpus_small.iter_conversations())\n",
    "topic_detector = TopicDetector()\n",
    "for i in range(3):\n",
    "  utterances = list(convos_small[i].iter_utterances())\n",
    "  title = convos_small[i].meta['title']\n",
    "  og_post = utterances[0].text\n",
    "  print(100*'-')\n",
    "  print(f\"Title: {title} \\n\")\n",
    "  print(f\"OG post: {og_post} \\n\")\n",
    "  topic = topic_detector.detect_conversation_topic(convos_small[i])\n",
    "  print(f\"Detected topic: {topic['topic']} \\n\")\n",
    "  print(f\"Confidence: {topic['confidence']} \\n\")\n",
    "  convos_small[i].add_meta(\"detected_topic\", topic[\"topic\"])\n",
    "  convos_small[i].add_meta(\"topic_confidence\", topic[\"confidence\"])\n",
    "  convos_small[i].add_meta(\"topic_scores\", topic[\"all_scores\"])\n",
    "  i += 1\n",
    "\n",
    "corpus_small.dump(\"/content/drive/MyDrive/MScProject/Corpora/corpus_small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JsdRB0uXdcQp",
   "metadata": {
    "id": "JsdRB0uXdcQp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
