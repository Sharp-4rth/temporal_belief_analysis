{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc7c77-ce3e-42a6-881c-b745a7baf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EH8DdCK0Or0F",
   "metadata": {
    "id": "EH8DdCK0Or0F"
   },
   "outputs": [],
   "source": [
    "# For colab:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecbdb0ca-91d4-4a79-bb7e-107fbba420be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Absolute path to src directory\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "klCvx9YUvKv6",
   "metadata": {
    "id": "klCvx9YUvKv6"
   },
   "outputs": [],
   "source": [
    "# Get latest version\n",
    "%cd temporal_belief_analysis\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfca0265e767d",
   "metadata": {
    "id": "8ecfca0265e767d"
   },
   "outputs": [],
   "source": [
    "# For colab:\n",
    "!pip install convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9mcx9qMoUA68",
   "metadata": {
    "id": "9mcx9qMoUA68"
   },
   "outputs": [],
   "source": [
    "# For colab:\n",
    "import unsloth\n",
    "import unsloth_zoo\n",
    "from convokit import Corpus, download\n",
    "import convokit\n",
    "from temporal_belief.models.bart_classifier import BARTZeroShotClassifier\n",
    "from temporal_belief.utils.config import POLITICAL_TOPICS, ProjectConfig\n",
    "# corpus = Corpus(filename=download(\"subreddit-PoliticalDiscussion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d1e33-a74a-4bcf-a1a8-c0c0a43b2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset was loaded:\n",
    "print(corpus.random_utterance().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vPF-AwIWUxJe",
   "metadata": {
    "id": "vPF-AwIWUxJe"
   },
   "outputs": [],
   "source": [
    "\"\"\"Topic detection functionality for conversation analysis.\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TopicDetector:\n",
    "    \"\"\"Detect topics in ConvoKit conversations using BART.\"\"\"\n",
    "\n",
    "    def __init__(self, topics: Optional[List[str]] = None,\n",
    "                 config: ProjectConfig = None):\n",
    "        \"\"\"Initialize topic detector.\"\"\"\n",
    "        self.config = config or ProjectConfig()\n",
    "        self.classifier = BARTZeroShotClassifier(self.config.bart_model_name)\n",
    "        self.topics = topics or POLITICAL_TOPICS\n",
    "        logger.info(f\"Initialized topic detector with {len(self.topics)} topics\")\n",
    "\n",
    "    def detect_conversation_topic(self, conversation) -> Dict[str, Any]:\n",
    "        \"\"\"Detect topic for a single conversation.\"\"\"\n",
    "        utterances = list(conversation.iter_utterances())\n",
    "        \n",
    "        # Safe attribute access\n",
    "        title = conversation.meta.get('title', '')\n",
    "        \n",
    "        # Safe utterance handling\n",
    "        first_utterance = utterances[0] if utterances else None\n",
    "        original_post = first_utterance.text if first_utterance else ''\n",
    "        \n",
    "        if not original_post and not title:\n",
    "            logger.warning(f\"No utterances or title found in conversation {conversation.id}\")\n",
    "            return {\"topic\": \"unknown\", \"confidence\": 0.0}\n",
    "\n",
    "        # Truncate long texts to prevent memory issues\n",
    "        combined_text = f\"Title: {title}. Original Post: {original_post}\"[:2000]\n",
    "        result = self.classifier.classify_text(combined_text, self.topics)\n",
    "\n",
    "        return {\n",
    "            \"topic\": result[\"label\"],\n",
    "            \"confidence\": result[\"confidence\"],\n",
    "            \"all_scores\": result[\"all_scores\"],\n",
    "            \"text_length\": len(original_post),\n",
    "            \"num_utterances\": len(utterances)\n",
    "        }\n",
    "\n",
    "    def process_corpus(self, corpus, batch_size: int = 50,  # Balanced batch size\n",
    "                    save_path: Optional[str] = None) -> None:\n",
    "        \"\"\"Process entire corpus for topic detection.\"\"\"\n",
    "        conversations = list(corpus.iter_conversations())\n",
    "        logger.info(f\"Processing {len(conversations)} conversations for topic detection\")\n",
    "\n",
    "        for i in tqdm(range(0, len(conversations), batch_size),\n",
    "                      desc=\"Processing conversations\"):\n",
    "            batch = conversations[i:i + batch_size]\n",
    "\n",
    "            # Prepare all texts for batch processing\n",
    "            batch_texts = []\n",
    "            valid_conversations = []\n",
    "\n",
    "            for conv in batch:\n",
    "                try:\n",
    "                    # Safe attribute access\n",
    "                    title = conv.meta.get('title', '')\n",
    "                    utterances = list(conv.iter_utterances())\n",
    "                    \n",
    "                    # Safe utterance handling\n",
    "                    first_utterance = utterances[0] if utterances else None\n",
    "                    original_post = first_utterance.text if first_utterance else ''\n",
    "                    \n",
    "                    if not original_post and not title:\n",
    "                        logger.warning(f\"No utterances or title found in conversation {conv.id}\")\n",
    "                        # Set metadata for empty conversations\n",
    "                        conv.add_meta(\"detected_topic\", \"unknown\")\n",
    "                        conv.add_meta(\"topic_confidence\", 0.0)\n",
    "                        conv.add_meta(\"topic_scores\", {})\n",
    "                        continue\n",
    "\n",
    "                    # Truncate long texts\n",
    "                    combined_text = f\"{title}. {original_post}\"[:2000]\n",
    "                    batch_texts.append(combined_text)\n",
    "                    valid_conversations.append(conv)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to prepare conversation {conv.id}: {e}\")\n",
    "                    conv.add_meta(\"detected_topic\", \"unknown\")\n",
    "                    conv.add_meta(\"topic_confidence\", 0.0)\n",
    "                    conv.add_meta(\"topic_scores\", {})\n",
    "\n",
    "            # Process entire batch at once\n",
    "            if batch_texts:\n",
    "                try:\n",
    "                    print(f\"üöÄ Attempting batch of {len(batch_texts)} texts...\")\n",
    "                    import time\n",
    "                    start = time.time()\n",
    "                    \n",
    "                    batch_results = self.classifier.classify_batch(batch_texts, self.topics)\n",
    "                    \n",
    "                    end = time.time()\n",
    "                    print(f\"‚úÖ Batch completed in {end-start:.2f}s ({(end-start)/len(batch_texts):.3f}s per text)\")\n",
    "\n",
    "                    # Apply results back to conversations\n",
    "                    for conv, result in zip(valid_conversations, batch_results):\n",
    "                        conv.add_meta(\"detected_topic\", result[\"label\"])\n",
    "                        conv.add_meta(\"topic_confidence\", result[\"confidence\"])\n",
    "                        conv.add_meta(\"topic_scores\", result[\"all_scores\"])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Batch processing failed: {e}\")\n",
    "                    logger.error(f\"Batch classification failed: {e}\")\n",
    "                    \n",
    "                    # Fallback to individual processing\n",
    "                    for conv in valid_conversations:\n",
    "                        try:\n",
    "                            topic_result = self.detect_conversation_topic(conv)\n",
    "                            conv.add_meta(\"detected_topic\", topic_result[\"topic\"])\n",
    "                            conv.add_meta(\"topic_confidence\", topic_result[\"confidence\"])\n",
    "                            conv.add_meta(\"topic_scores\", topic_result[\"all_scores\"])\n",
    "                        except Exception as e2:\n",
    "                            logger.error(f\"Individual fallback failed for {conv.id}: {e2}\")\n",
    "                            conv.add_meta(\"detected_topic\", \"unknown\")\n",
    "                            conv.add_meta(\"topic_confidence\", 0.0)\n",
    "                            conv.add_meta(\"topic_scores\", {})\n",
    "\n",
    "        if save_path:\n",
    "            corpus.dump(save_path)\n",
    "            logger.info(f\"Saved processed corpus to {save_path}\")\n",
    "\n",
    "        logger.info(\"Topic detection processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K8oYk7YLM_oW",
   "metadata": {
    "id": "K8oYk7YLM_oW"
   },
   "outputs": [],
   "source": [
    "# Testing 'process_corpus()'\n",
    "SAVE_PATH = \"/workspace/temporal_belief_analysis/pd_corpus_with_topics2\"\n",
    "topic_detector = TopicDetector()\n",
    "topic_detector.process_corpus(corpus, save_path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad1190-19c0-48fc-ab44-70e17f93aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_small.dump(\"/workspace/temporal_belief_analysis/corpus_small_save_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96fe3b6-add2-4e28-9ba1-c0c35589cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new cell - test if kernel responds\n",
    "print(\"Kernel alive check\")\n",
    "import time\n",
    "print(f\"Current time: {time.time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38623a2c-6af8-4edd-b2f7-044708597fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if metadata gets added\n",
    "conversations = list(corpus.iter_conversations())\n",
    "\n",
    "# Check first conversation\n",
    "first_conv = conversations[1]\n",
    "print(f\"First conversation ID: {first_conv.id}\")\n",
    "print(f\"Has topic metadata: {'detected_topic' in first_conv.meta}\")\n",
    "if 'detected_topic' in first_conv.meta:\n",
    "    print(f\"Topic: {first_conv.meta['detected_topic']}\")\n",
    "    print(f\"Confidence: {first_conv.meta['topic_confidence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PxXbHWRrMbDK",
   "metadata": {
    "id": "PxXbHWRrMbDK"
   },
   "outputs": [],
   "source": [
    "# Testing 'detect_conversation_topic()' and 'dump()'\n",
    "corpus_small = Corpus(filename=download(\"reddit-corpus-small\"))\n",
    "\n",
    "i = 0\n",
    "convos_small = list(corpus_small.iter_conversations())\n",
    "topic_detector = TopicDetector()\n",
    "for i in range(3):\n",
    "  utterances = list(convos_small[i].iter_utterances())\n",
    "  title = convos_small[i].meta['title']\n",
    "  og_post = utterances[0].text\n",
    "  # print(100*'-')\n",
    "  # print(f\"Title: {title} \\n\")\n",
    "  # print(f\"OG post: {og_post} \\n\")\n",
    "  topic = topic_detector.detect_conversation_topic(convos_small[i])\n",
    "  # print(f\"Detected topic: {topic['topic']} \\n\")\n",
    "  # print(f\"Confidence: {topic['confidence']} \\n\")\n",
    "  convos_small[i].add_meta(\"detected_topic\", topic[\"topic\"])\n",
    "  convos_small[i].add_meta(\"topic_confidence\", topic[\"confidence\"])\n",
    "  convos_small[i].add_meta(\"topic_scores\", topic[\"all_scores\"])\n",
    "  i += 1\n",
    "\n",
    "# corpus_small.dump_info(obj_type=\"corpus\", fields=[\"meta\"])\n",
    "corpus_small.dump(\"/workspace/temporal_belief_analysis/processed_corpus_small_other\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "695b3110-8e5f-4bec-a37b-6bf1390a40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = Corpus(filename=\"/workspace/temporal_belief_analysis/processed_corpus_small_other\")\n",
    "\n",
    "# processed_convos = list(processed_corpus.iter_conversations())\n",
    "# convos_small = list(corpus_small.iter_conversations())\n",
    "# print(f'Processed: {processed_convos[0].meta}\\n')\n",
    "# print(f'Not processed: {convos_small[0].meta}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c647fd0e-e52d-49b3-be3e-2514a1100377",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/temporal_belief_analysis/pd_corpus_with_topics2'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m corpus_with_topics \u001B[38;5;241m=\u001B[39m \u001B[43mCorpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/workspace/temporal_belief_analysis/pd_corpus_with_topics2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# convos_with_topics = list(processed_corpus.iter_conversations())\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/convokit/model/corpus.py:172\u001B[0m, in \u001B[0;36mCorpus.__init__\u001B[0;34m(self, filename, utterances, db_collection_prefix, db_host, preload_vectors, utterance_start_index, utterance_end_index, merge_lines, exclude_utterance_meta, exclude_conversation_meta, exclude_speaker_meta, exclude_overall_meta, disable_type_check, backend, backend_mapper)\u001B[0m\n\u001B[1;32m    170\u001B[0m     speakers_data \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mdict\u001B[39m)\n\u001B[1;32m    171\u001B[0m     convos_data \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mdict\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m     utterances \u001B[38;5;241m=\u001B[39m \u001B[43mload_from_utterance_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutterance_start_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutterance_end_index\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mutterances \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspeakers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n",
      "File \u001B[0;32m/usr/local/lib/python3.10/dist-packages/convokit/model/corpus_helpers.py:313\u001B[0m, in \u001B[0;36mload_from_utterance_file\u001B[0;34m(filename, utterance_start_index, utterance_end_index)\u001B[0m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_from_utterance_file\u001B[39m(filename, utterance_start_index, utterance_end_index):\n\u001B[1;32m    310\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;124;03m    where filename is \"utterances.json\" or \"utterances.jsonl\" for example\u001B[39;00m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 313\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    314\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    315\u001B[0m             ext \u001B[38;5;241m=\u001B[39m filename\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/workspace/temporal_belief_analysis/pd_corpus_with_topics2'"
     ]
    }
   ],
   "source": [
    "corpus_with_topics = Corpus(filename=\"/workspace/temporal_belief_analysis/pd_corpus_with_topics2\")\n",
    "# convos_with_topics = list(processed_corpus.iter_conversations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea46bc4-7644-4834-bd6f-21fc055cda23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
