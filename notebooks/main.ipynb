{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Sharp-4rth/temporal_belief_analysis.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sjSohXnld3",
        "outputId": "9f1aa5e7-dfc3-4da5-fcf9-e66916f2c622"
      },
      "id": "F8sjSohXnld3",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'temporal_belief_analysis'...\n",
            "remote: Enumerating objects: 527, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
            "remote: Total 527 (delta 79), reused 54 (delta 28), pack-reused 389 (from 1)\u001b[K\n",
            "Receiving objects: 100% (527/527), 3.41 MiB | 5.32 MiB/s, done.\n",
            "Resolving deltas: 100% (327/327), done.\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "f31f60c33c386f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae50bdd-3bb9-4485-ce95-d16c822bbae0"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit[llm]\n",
            "  Downloading convokit-3.4.1.tar.gz (212 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/212.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/212.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (3.10.0)\n",
            "Requirement already satisfied: scipy>1.14 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (1.16.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (2.0.2)\n",
            "Collecting msgpack-numpy>=0.4.3.2 (from convokit[llm])\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: spacy>=3.8.2 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (3.8.7)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (1.6.1)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (3.9.1)\n",
            "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (0.3.8)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (1.5.1)\n",
            "Collecting clean-text>=0.6.0 (from convokit[llm])\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit[llm])\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (4.67.1)\n",
            "Collecting pymongo>=4.0 (from convokit[llm])\n",
            "  Downloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (6.0.2)\n",
            "Collecting dnspython>=1.16.0 (from convokit[llm])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (8.3.6)\n",
            "Collecting h5py==3.12.1 (from convokit[llm])\n",
            "  Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numexpr>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (2.11.0)\n",
            "Requirement already satisfied: ruff>=0.4.8 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (0.12.9)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (1.4.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (4.0.0)\n",
            "Requirement already satisfied: torch>=0.12 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (1.10.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (0.17.0)\n",
            "Collecting bitsandbytes (from convokit[llm])\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (4.55.2)\n",
            "Collecting unsloth (from convokit[llm])\n",
            "  Downloading unsloth-2025.8.9-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.12.2 (from convokit[llm])\n",
            "  Downloading trl-0.21.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tensorflow>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (2.19.0)\n",
            "Requirement already satisfied: tf-keras<3.0.0,>=2.17.0 in /usr/local/lib/python3.12/dist-packages (from convokit[llm]) (2.19.0)\n",
            "Collecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit[llm])\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit[llm])\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit[llm]) (2.9.0.post0)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from msgpack-numpy>=0.4.3.2->convokit[llm]) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.4->convokit[llm]) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.4->convokit[llm]) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->convokit[llm]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->convokit[llm]) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->convokit[llm]) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit[llm]) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (3.10.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.18.0->convokit[llm]) (0.5.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.0->convokit[llm]) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.0->convokit[llm]) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.12->convokit[llm]) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->convokit[llm]) (5.9.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate->convokit[llm]) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate->convokit[llm]) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->convokit[llm]) (18.1.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->convokit[llm]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->convokit[llm]) (0.70.16)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers->convokit[llm]) (0.21.4)\n",
            "Collecting unsloth_zoo>=2025.8.8 (from unsloth->convokit[llm])\n",
            "  Downloading unsloth_zoo-2025.8.8-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth->convokit[llm])\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tyro (from unsloth->convokit[llm])\n",
            "  Downloading tyro-0.9.28-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets (from convokit[llm])\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth->convokit[llm]) (0.2.1)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth->convokit[llm]) (0.45.1)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth->convokit[llm]) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth->convokit[llm]) (0.34.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth->convokit[llm]) (0.23.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (3.12.15)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit[llm]) (0.2.13)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate->convokit[llm]) (1.1.7)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit[llm]) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit[llm]) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.18.0->convokit[llm]) (0.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit[llm]) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit[llm]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit[llm]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit[llm]) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit[llm]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit[llm]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit[llm]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit[llm]) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.12->convokit[llm]) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit[llm]) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit[llm]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.18.0->convokit[llm]) (3.1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit[llm]) (1.5.4)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.8.8->unsloth->convokit[llm]) (0.10.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.8.8->unsloth->convokit[llm])\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.8.8->unsloth->convokit[llm])\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit[llm]) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit[llm]) (7.3.0.post1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth->convokit[llm]) (8.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=3.8.2->convokit[llm]) (3.0.2)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth->convokit[llm]) (0.17.0)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth->convokit[llm])\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth->convokit[llm]) (4.4.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit[llm]) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit[llm]) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.18.0->convokit[llm]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.18.0->convokit[llm]) (2.19.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth->convokit[llm]) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.18.0->convokit[llm]) (0.1.2)\n",
            "Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Downloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.21.0-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth-2025.8.9-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.8.8-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.8/184.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.28-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.4.1-py3-none-any.whl size=255922 sha256=e1096c697d17339b53231c43d8803b8f8338e4b69efda855e8e4176c1f5fbd5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/1b/82/19dc268440f10effa084bf272c07d9d5d3cd4f6db1472b0be6\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171031 sha256=6d8c23ba5fedbdbf553388d3299047b89ebaad5a0b1a6a558fb8fff698192356\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/8c/e0/294d2e4ea0e55792bfc99b6b263e4a0511443da7b69af67688\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, shtab, msgspec, msgpack-numpy, h5py, ftfy, dnspython, pymongo, clean-text, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, convokit, unsloth\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.14.0\n",
            "    Uninstalling h5py-3.14.0:\n",
            "      Successfully uninstalled h5py-3.14.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed bitsandbytes-0.47.0 clean-text-0.6.0 convokit-3.4.1 cut_cross_entropy-25.1.1 datasets-3.6.0 dnspython-2.7.0 emoji-1.7.0 ftfy-6.3.1 h5py-3.12.1 msgpack-numpy-0.4.8 msgspec-0.19.0 pymongo-4.14.1 shtab-1.7.2 trl-0.21.0 tyro-0.9.28 unidecode-1.4.0 unsloth-2025.8.9 unsloth_zoo-2025.8.8 xformers-0.0.32.post2\n",
            "Requirement already satisfied: convokit in /usr/local/lib/python3.12/dist-packages (3.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (3.10.0)\n",
            "Requirement already satisfied: scipy>1.14 in /usr/local/lib/python3.12/dist-packages (from convokit) (1.16.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (2.2.2)\n",
            "Requirement already satisfied: numpy>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (2.0.2)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.12/dist-packages (from convokit) (0.4.8)\n",
            "Requirement already satisfied: spacy>=3.8.2 in /usr/local/lib/python3.12/dist-packages (from convokit) (3.8.7)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (1.6.1)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.12/dist-packages (from convokit) (3.9.1)\n",
            "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.12/dist-packages (from convokit) (0.3.8)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from convokit) (1.5.1)\n",
            "Requirement already satisfied: clean-text>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (0.6.0)\n",
            "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from convokit) (1.4.0)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (4.67.1)\n",
            "Requirement already satisfied: pymongo>=4.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (4.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from convokit) (6.0.2)\n",
            "Requirement already satisfied: dnspython>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (2.7.0)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (8.3.6)\n",
            "Requirement already satisfied: h5py==3.12.1 in /usr/local/lib/python3.12/dist-packages (from convokit) (3.12.1)\n",
            "Requirement already satisfied: numexpr>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from convokit) (2.11.0)\n",
            "Requirement already satisfied: ruff>=0.4.8 in /usr/local/lib/python3.12/dist-packages (from convokit) (0.12.9)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.12/dist-packages (from convokit) (1.4.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from convokit) (3.6.0)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from clean-text>=0.6.0->convokit) (1.7.0)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from clean-text>=0.6.0->convokit) (6.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->convokit) (2.9.0.post0)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.4->convokit) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.4->convokit) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->convokit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.0->convokit) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->convokit) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy>=3.8.2->convokit) (3.5.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.0->convokit) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.0->convokit) (0.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->convokit) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->convokit) (18.1.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->convokit) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->convokit) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->convokit) (0.34.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (3.12.15)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->convokit) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->convokit) (1.1.7)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.8.2->convokit) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.8.2->convokit) (2025.8.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy>=3.8.2->convokit) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->convokit) (1.20.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.8.2->convokit) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.8.2->convokit) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.8.2->convokit) (0.1.2)\n"
          ]
        }
      ],
      "execution_count": 2,
      "source": [
        "# Need to restart after:\n",
        "!pip install convokit[llm]\n",
        "!pip install convokit"
      ],
      "id": "f31f60c33c386f57"
    },
    {
      "metadata": {
        "id": "524995afbe8f262c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c56d880-5685-409d-af52-727fe228804f"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Changed working directory to: /content/temporal_belief_analysis/notebooks\n"
          ]
        }
      ],
      "execution_count": 1,
      "source": [
        "import sys\n",
        "import os\n",
        "os.chdir('/content/temporal_belief_analysis/notebooks')\n",
        "print(\"Changed working directory to:\", os.getcwd())\n",
        "\n",
        "# Absolute path to src directory\n",
        "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)"
      ],
      "id": "524995afbe8f262c"
    },
    {
      "metadata": {
        "id": "719bb8c1568bba9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af024530-b7c7-4d18-badf-d0169e7cf088"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "!pip install gdown\n",
        "import zipfile\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from convokit import Corpus, download\n",
        "import convokit\n",
        "from temporal_belief.core.timeline_building import TimelineBuilder\n",
        "from temporal_belief.core.change_detection import ChangeDetector\n",
        "from temporal_belief.core.window_extraction import WindowExtractor\n",
        "from temporal_belief.core.op_path_pairing import OpPathPairer\n",
        "from temporal_belief.data.preprocessors import ChangeDetectorPreprocessor\n",
        "from temporal_belief.data.preprocessors import PairPreprocessor\n",
        "from temporal_belief.core.interplay import Interplay\n",
        "nltk.download('stopwords')"
      ],
      "id": "719bb8c1568bba9a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "An error occurred: Unsloth currently only works on NVIDIA GPUs and Intel GPUs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/convokit/__init__.py:36: UserWarning: If you are using ConvoKit with Google Colab, incorrect versions of some packages (ex. scipy) may be imported while runtime start. To fix the issue, restart the session and run all codes again. Thank you!\n",
            "  warnings.warn(\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32cc484b68d1ecdc",
        "outputId": "36deb9a9-ca1d-4dd6-afa9-7dac52186ee4"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1AIrstrzE259fcVyxJQW4-RwvAkoUyK1x\n",
            "From (redirected): https://drive.google.com/uc?id=1AIrstrzE259fcVyxJQW4-RwvAkoUyK1x&confirm=t&uuid=cf2419fc-0025-4155-93ac-cba05fe956d3\n",
            "To: /content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\n",
            "100% 1.07G/1.07G [00:11<00:00, 93.0MB/s]\n"
          ]
        }
      ],
      "execution_count": 3,
      "source": [
        "# Download and unzip with python (Dataloading):\n",
        "# !gdown \"https://drive.google.com/file/d/1N0U_jUJlOYjdaju2FaU8p87uB22YBxJ0/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/1DLFY6JLMZqNjwvNRZmhlV4-rnoQP_eyH/view?usp=sharing\" -O \"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\" --fuzzy\n",
        "# !gdown \"https://drive.google.com/file/d/1nWaj5N8nsG7u5homv_kAh4CLPDv01M_Z/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_topics.zip\" --fuzzy\n",
        "!gdown \"https://drive.google.com/file/d/1AIrstrzE259fcVyxJQW4-RwvAkoUyK1x/view?usp=sharing\" -O \"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\" --fuzzy\n",
        "\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances100000_chronological.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/merged_corpus_checkpoint_5.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "# zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_topics.zip\").extractall(\"/content/temporal_belief_analysis\")\n",
        "zipfile.ZipFile(\"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned.zip\").extractall(\"/content/temporal_belief_analysis\")"
      ],
      "id": "32cc484b68d1ecdc"
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS_PATH = \"/content/temporal_belief_analysis/pd_corpus_with_stances_fine_tuned\""
      ],
      "metadata": {
        "id": "Q98fql-ioDgx"
      },
      "id": "Q98fql-ioDgx",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Any\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "\n",
        "MERGED_TOPIC = {\n",
        "    # Economy\n",
        "    'economic policy': 'Economy & Tax',\n",
        "    'taxation and government spending': 'Economy & Tax',\n",
        "\n",
        "    # Healthcare\n",
        "    'healthcare policy': 'Healthcare',\n",
        "\n",
        "    # Civil rights / justice / education / voting\n",
        "    'civil rights and social issues': 'Civil Rights, Justice & Education',\n",
        "    'criminal justice and policing': 'Civil Rights, Justice & Education',\n",
        "    'voting rights and elections': 'Civil Rights, Justice & Education',\n",
        "    'education policy': 'Civil Rights, Justice & Education',\n",
        "\n",
        "    # Hot-button singles\n",
        "    'gun rights and control': 'Guns',\n",
        "    'abortion and reproductive rights': 'Abortion',\n",
        "    'immigration policy': 'Immigration',\n",
        "    'climate change and energy policy': 'Climate & Energy',\n",
        "\n",
        "    # Foreign / defense\n",
        "    'foreign policy and defense': 'Foreign & Defense',\n",
        "\n",
        "    # Meta / process / actors\n",
        "    'political figures and campaigns': 'Process & Actors (Meta)',\n",
        "    'congressional politics': 'Process & Actors (Meta)',\n",
        "    'electoral politics': 'Process & Actors (Meta)',\n",
        "    'political parties and ideology': 'Process & Actors (Meta)',\n",
        "    'media and political commentary': 'Process & Actors (Meta)',\n",
        "}\n",
        "\n",
        "\n",
        "class TimelineBuilder:\n",
        "    \"\"\"Simple timeline builder for user belief tracking.\n",
        "\n",
        "    Builds structure: {user_id: {topic: {utterance_id: stance}}}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus, min_posts_per_topic: int = 0, min_topics_per_user: int = 0):\n",
        "        self.corpus = corpus\n",
        "        self.min_posts_per_topic = min_posts_per_topic\n",
        "        self.min_topics_per_user = min_topics_per_user\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def build_timelines(self, include_all=True) -> Dict[str, Dict[str, Dict[str, str]]]:\n",
        "        \"\"\"Build user timelines from corpus with stance metadata.\n",
        "\n",
        "        Returns:\n",
        "            {user_id: {topic: {utterance_id: stance}}}\n",
        "        \"\"\"\n",
        "        # Group by user and topic\n",
        "        user_topic_posts = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "        for utterance in self.corpus.iter_utterances():\n",
        "            # Skip if no stance metadata on utterance\n",
        "            if include_all == False:\n",
        "                if not utterance.meta or 'detected_stance' not in utterance.meta:\n",
        "                    continue\n",
        "\n",
        "            # Get topic from conversation metadata\n",
        "            conversation = utterance.get_conversation()\n",
        "            if not conversation or not conversation.meta or 'detected_topic' not in conversation.meta:\n",
        "                continue\n",
        "\n",
        "            if not utterance.timestamp:\n",
        "                continue\n",
        "\n",
        "            user_id = utterance.speaker.id\n",
        "            old_topic = conversation.meta['detected_topic']\n",
        "            topic = MERGED_TOPIC.get(old_topic, old_topic)\n",
        "            stance = utterance.meta.get('detected_stance', 'Unknown')\n",
        "\n",
        "            user_topic_posts[user_id][topic].append({\n",
        "                'utterance_id': utterance.id,\n",
        "                'timestamp': utterance.timestamp,\n",
        "                'stance': stance\n",
        "            })\n",
        "\n",
        "        # Filter and sort\n",
        "        timelines = {}\n",
        "        for user_id, topic_posts in user_topic_posts.items():\n",
        "            user_timeline = {}\n",
        "\n",
        "            for topic, posts in topic_posts.items():\n",
        "                if len(posts) >= self.min_posts_per_topic:\n",
        "                    # Sort chronologically\n",
        "                    posts.sort(key=lambda x: x['timestamp'])\n",
        "\n",
        "                    # Create topic timeline\n",
        "                    topic_timeline = {}\n",
        "                    for post in posts:\n",
        "                        topic_timeline[post['utterance_id']] = post['stance']\n",
        "\n",
        "                    user_timeline[topic] = topic_timeline\n",
        "\n",
        "            # Only include users with enough topics\n",
        "            if len(user_timeline) >= self.min_topics_per_user:\n",
        "                timelines[user_id] = user_timeline\n",
        "\n",
        "        self.logger.info(f\"Built timelines for {len(timelines)} users\")\n",
        "        return timelines"
      ],
      "metadata": {
        "id": "9q_403AbXLTt"
      },
      "id": "9q_403AbXLTt",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "\n",
        "class ChangeDetector:\n",
        "    \"\"\"CUSUM-based change detection for political stance shifts.\n",
        "\n",
        "    Focuses on detecting changes between 'left-leaning' and 'right-leaning' positions,\n",
        "    ignoring neutral stances. Uses cumulative sum control charts to identify\n",
        "    significant shifts in political orientation over time.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, threshold=3.0, drift=0.5, min_change_separation=5):\n",
        "        \"\"\"Initialize CUSUM detector with control parameters.\n",
        "\n",
        "        Args:\n",
        "            threshold: Detection threshold for CUSUM statistic (higher = less sensitive)\n",
        "            drift: Reference drift value for change detection (typically 0.5-1.0)\n",
        "            min_change_separation: Minimum posts between detected changes\n",
        "        \"\"\"\n",
        "        self.threshold = threshold\n",
        "        self.drift = drift\n",
        "        self.min_change_separation = min_change_separation\n",
        "\n",
        "        # Map stances to numeric values for CUSUM\n",
        "        self.stance_values = {\n",
        "            'left-leaning': -1.0,\n",
        "            'neutral': 0.0,        # Will be filtered out\n",
        "            'right-leaning': 1.0\n",
        "        }\n",
        "\n",
        "        self.all_change_points = []\n",
        "        self.all_no_change_points = []\n",
        "\n",
        "        # Logging setup\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _to_probs(self, item):\n",
        "        \"\"\"Convert various input formats to probability tuple (pL, pN, pR).\"\"\"\n",
        "        if isinstance(item, str):\n",
        "            if item == 'left-leaning':  return (1.0, 0.0, 0.0)\n",
        "            if item == 'neutral':       return (0.0, 1.0, 0.0)\n",
        "            if item == 'right-leaning': return (0.0, 0.0, 1.0)\n",
        "            return (0.0, 1.0, 0.0)\n",
        "        if isinstance(item, dict):\n",
        "            return (float(item.get('pL', 0.0)), float(item.get('pN', 0.0)), float(item.get('pR', 0.0)))\n",
        "        if isinstance(item, (list, tuple)) and len(item) == 3:\n",
        "            pL, pN, pR = item\n",
        "            return (float(pL), float(pN), float(pR))\n",
        "        return (0.0, 1.0, 0.0)\n",
        "\n",
        "    def _get_political_signal(self, prob_tuple, conf_threshold=0.6):\n",
        "        \"\"\"Extract political signal from probability tuple, ignoring neutral.\n",
        "\n",
        "        Args:\n",
        "            prob_tuple: (pL, pN, pR) probability tuple\n",
        "            conf_threshold: Minimum confidence to consider stance reliable\n",
        "\n",
        "        Returns:\n",
        "            Float value: -1.0 (left), +1.0 (right), or None (neutral/uncertain)\n",
        "        \"\"\"\n",
        "        pL, pN, pR = prob_tuple\n",
        "\n",
        "        # Only consider if we have sufficient confidence in left or right\n",
        "        if pL >= conf_threshold:\n",
        "            return -1.0  # left-leaning\n",
        "        elif pR >= conf_threshold:\n",
        "            return 1.0   # right-leaning\n",
        "        else:\n",
        "            return None  # neutral or uncertain - ignore for CUSUM\n",
        "\n",
        "    def detect_cusum_changes(self, topic_timeline, conf_threshold=0.6):\n",
        "        \"\"\"Detect political stance changes using CUSUM algorithm.\n",
        "\n",
        "        Args:\n",
        "            topic_timeline: List of (utterance_id, stance_data) tuples\n",
        "            conf_threshold: Minimum confidence for reliable stance detection\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with change_points and no_change_points lists\n",
        "        \"\"\"\n",
        "        if not topic_timeline:\n",
        "            return {'change_points': [], 'no_change_points': []}\n",
        "\n",
        "        # Extract political signals, filtering out neutral/uncertain\n",
        "        signals = []\n",
        "        valid_utterances = []\n",
        "\n",
        "        for utt_id, stance_data in topic_timeline:\n",
        "            prob_tuple = self._to_probs(stance_data)\n",
        "            signal = self._get_political_signal(prob_tuple, conf_threshold)\n",
        "\n",
        "            if signal is not None:\n",
        "                signals.append(signal)\n",
        "                valid_utterances.append(utt_id)\n",
        "\n",
        "        if len(signals) < 3:\n",
        "            self.logger.warning(f\"Insufficient political signals for CUSUM: {len(signals)}\")\n",
        "            return {'change_points': [], 'no_change_points': [utt_id for utt_id, _ in topic_timeline]}\n",
        "\n",
        "        # CUSUM change detection\n",
        "        change_indices = self._cusum_detect_changes(signals)\n",
        "\n",
        "        # Convert indices back to utterance IDs\n",
        "        change_points = [valid_utterances[idx] for idx in change_indices if idx < len(valid_utterances)]\n",
        "\n",
        "        # All other utterances are no-change points\n",
        "        change_set = set(change_points)\n",
        "        no_change_points = [utt_id for utt_id, _ in topic_timeline if utt_id not in change_set]\n",
        "\n",
        "        # Store for aggregate statistics\n",
        "        self.all_change_points.extend(change_points)\n",
        "        self.all_no_change_points.extend(no_change_points)\n",
        "\n",
        "        return {\n",
        "            'change_points': change_points,\n",
        "            'no_change_points': no_change_points\n",
        "        }\n",
        "\n",
        "    def _cusum_detect_changes(self, signals):\n",
        "        \"\"\"Core CUSUM algorithm for detecting mean shifts in political stance.\n",
        "\n",
        "        Args:\n",
        "            signals: List of political stance values (-1.0 or +1.0)\n",
        "\n",
        "        Returns:\n",
        "            List of indices where significant changes were detected\n",
        "        \"\"\"\n",
        "        if len(signals) < 2:\n",
        "            return []\n",
        "\n",
        "        signals = np.array(signals)\n",
        "        n = len(signals)\n",
        "        change_points = []\n",
        "\n",
        "        # Calculate overall mean for reference\n",
        "        overall_mean = np.mean(signals)\n",
        "\n",
        "        # Initialize CUSUM statistics\n",
        "        cusum_pos = 0.0  # Positive CUSUM (detecting upward shifts)\n",
        "        cusum_neg = 0.0  # Negative CUSUM (detecting downward shifts)\n",
        "\n",
        "        for i in range(1, n):\n",
        "            # Calculate deviations from reference mean\n",
        "            deviation = signals[i] - overall_mean\n",
        "\n",
        "            # Update CUSUM statistics\n",
        "            cusum_pos = max(0, cusum_pos + deviation - self.drift)\n",
        "            cusum_neg = max(0, cusum_neg - deviation - self.drift)\n",
        "\n",
        "            # Check for threshold crossings\n",
        "            change_detected = False\n",
        "\n",
        "            if cusum_pos > self.threshold:\n",
        "                # Positive shift detected (towards right-leaning)\n",
        "                change_points.append(i)\n",
        "                cusum_pos = 0.0  # Reset after detection\n",
        "                change_detected = True\n",
        "                self.logger.debug(f\"CUSUM: Positive shift detected at index {i}\")\n",
        "\n",
        "            elif cusum_neg > self.threshold:\n",
        "                # Negative shift detected (towards left-leaning)\n",
        "                change_points.append(i)\n",
        "                cusum_neg = 0.0  # Reset after detection\n",
        "                change_detected = True\n",
        "                self.logger.debug(f\"CUSUM: Negative shift detected at index {i}\")\n",
        "\n",
        "            # Enforce minimum separation between changes\n",
        "            if change_detected and len(change_points) > 1:\n",
        "                if i - change_points[-2] < self.min_change_separation:\n",
        "                    change_points.pop()  # Remove this change point\n",
        "                    self.logger.debug(f\"CUSUM: Removed change point at {i} due to minimum separation\")\n",
        "\n",
        "        return change_points\n",
        "\n",
        "    def detect_cusum_changes_advanced(self, topic_timeline, conf_threshold=0.6,\n",
        "                                    adaptive_threshold=True):\n",
        "        \"\"\"Advanced CUSUM with adaptive thresholding and confidence weighting.\n",
        "\n",
        "        Args:\n",
        "            topic_timeline: List of (utterance_id, stance_data) tuples\n",
        "            conf_threshold: Minimum confidence for reliable stance detection\n",
        "            adaptive_threshold: Whether to adapt threshold based on signal variance\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with change_points and no_change_points lists\n",
        "        \"\"\"\n",
        "        if not topic_timeline:\n",
        "            return {'change_points': [], 'no_change_points': []}\n",
        "\n",
        "        # Extract weighted political signals\n",
        "        signals = []\n",
        "        confidences = []\n",
        "        valid_utterances = []\n",
        "\n",
        "        for utt_id, stance_data in topic_timeline:\n",
        "            prob_tuple = self._to_probs(stance_data)\n",
        "            signal = self._get_political_signal(prob_tuple, conf_threshold)\n",
        "\n",
        "            if signal is not None:\n",
        "                signals.append(signal)\n",
        "                # Extract confidence from stance_data if available\n",
        "                confidence = self._extract_confidence(stance_data)\n",
        "                confidences.append(confidence)\n",
        "                valid_utterances.append(utt_id)\n",
        "\n",
        "        if len(signals) < 3:\n",
        "            return {'change_points': [], 'no_change_points': [utt_id for utt_id, _ in topic_timeline]}\n",
        "\n",
        "        # Adaptive threshold based on signal variance\n",
        "        threshold = self.threshold\n",
        "        if adaptive_threshold:\n",
        "            signal_std = np.std(signals)\n",
        "            threshold = max(self.threshold, 2.0 * signal_std)\n",
        "            self.logger.debug(f\"CUSUM: Adaptive threshold set to {threshold:.2f}\")\n",
        "\n",
        "        # Confidence-weighted CUSUM\n",
        "        change_indices = self._cusum_detect_changes_weighted(signals, confidences, threshold)\n",
        "\n",
        "        change_points = [valid_utterances[idx] for idx in change_indices if idx < len(valid_utterances)]\n",
        "        change_set = set(change_points)\n",
        "        no_change_points = [utt_id for utt_id, _ in topic_timeline if utt_id not in change_set]\n",
        "\n",
        "        self.all_change_points.extend(change_points)\n",
        "        self.all_no_change_points.extend(no_change_points)\n",
        "\n",
        "        return {\n",
        "            'change_points': change_points,\n",
        "            'no_change_points': no_change_points\n",
        "        }\n",
        "\n",
        "    def _cusum_detect_changes_weighted(self, signals, confidences, threshold):\n",
        "        \"\"\"CUSUM with confidence weighting for more reliable change detection.\"\"\"\n",
        "        signals = np.array(signals)\n",
        "        confidences = np.array(confidences)\n",
        "        n = len(signals)\n",
        "        change_points = []\n",
        "\n",
        "        # Confidence-weighted mean\n",
        "        weighted_mean = np.average(signals, weights=confidences)\n",
        "\n",
        "        # Initialize CUSUM with confidence weighting\n",
        "        cusum_pos = 0.0\n",
        "        cusum_neg = 0.0\n",
        "\n",
        "        for i in range(1, n):\n",
        "            # Weight deviation by confidence\n",
        "            deviation = (signals[i] - weighted_mean) * confidences[i]\n",
        "\n",
        "            # Update CUSUM statistics\n",
        "            cusum_pos = max(0, cusum_pos + deviation - self.drift)\n",
        "            cusum_neg = max(0, cusum_neg - deviation - self.drift)\n",
        "\n",
        "            # Detection with separation enforcement\n",
        "            if cusum_pos > threshold or cusum_neg > threshold:\n",
        "                if not change_points or i - change_points[-1] >= self.min_change_separation:\n",
        "                    change_points.append(i)\n",
        "                    cusum_pos = 0.0\n",
        "                    cusum_neg = 0.0\n",
        "\n",
        "                    direction = \"right\" if cusum_pos > cusum_neg else \"left\"\n",
        "                    self.logger.debug(f\"CUSUM: {direction} shift detected at index {i}, confidence={confidences[i]:.2f}\")\n",
        "\n",
        "        return change_points\n",
        "\n",
        "    def _extract_confidence(self, stance_data):\n",
        "        \"\"\"Extract confidence score from stance data.\"\"\"\n",
        "        if isinstance(stance_data, dict):\n",
        "            return stance_data.get('confidence', 1.0)\n",
        "        elif isinstance(stance_data, (list, tuple)) and len(stance_data) == 3:\n",
        "            # Use max probability as confidence\n",
        "            return max(stance_data)\n",
        "        else:\n",
        "            return 1.0  # Default confidence\n",
        "\n",
        "    def _get_political_signal(self, prob_tuple, conf_threshold=0.6):\n",
        "        \"\"\"Extract political signal, ignoring neutral positions.\"\"\"\n",
        "        pL, pN, pR = prob_tuple\n",
        "\n",
        "        # Only consider confident left/right positions\n",
        "        if pL >= conf_threshold:\n",
        "            return -1.0  # left-leaning\n",
        "        elif pR >= conf_threshold:\n",
        "            return 1.0   # right-leaning\n",
        "        else:\n",
        "            return None  # neutral/uncertain - ignore\n",
        "\n",
        "    def get_two_groups(self, timelines, method='cusum', conf_threshold=0.6,\n",
        "                      advanced=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Group users into with/without changes using CUSUM detection.\n",
        "\n",
        "        Args:\n",
        "            timelines: Dictionary of {user_id: {topic: timeline}} data\n",
        "            method: Detection method ('cusum' or 'cusum_advanced')\n",
        "            conf_threshold: Minimum confidence for reliable stance detection\n",
        "            advanced: Whether to use confidence-weighted CUSUM\n",
        "            **kwargs: Additional parameters (threshold, drift, etc.)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with 'with_changes' and 'no_changes' user groups\n",
        "        \"\"\"\n",
        "        with_changes = {}\n",
        "        no_changes = {}\n",
        "\n",
        "        # Update detector parameters from kwargs\n",
        "        if 'threshold' in kwargs:\n",
        "            self.threshold = kwargs['threshold']\n",
        "        if 'drift' in kwargs:\n",
        "            self.drift = kwargs['drift']\n",
        "        if 'min_change_separation' in kwargs:\n",
        "            self.min_change_separation = kwargs['min_change_separation']\n",
        "\n",
        "        # Select detection method\n",
        "        if advanced:\n",
        "            detect_func = lambda tl: self.detect_cusum_changes_advanced(\n",
        "                tl, conf_threshold=conf_threshold, **kwargs\n",
        "            )\n",
        "        else:\n",
        "            detect_func = lambda tl: self.detect_cusum_changes(\n",
        "                tl, conf_threshold=conf_threshold\n",
        "            )\n",
        "\n",
        "        self.logger.info(f\"Starting CUSUM change detection with threshold={self.threshold}, \"\n",
        "                        f\"drift={self.drift}, advanced={advanced}\")\n",
        "\n",
        "        for user_id, topic_timelines in timelines.items():\n",
        "            user_has_changes = False\n",
        "\n",
        "            for topic_name, topic_timeline in topic_timelines.items():\n",
        "                # Convert to list format expected by detection methods\n",
        "                topic_timeline_list = list(topic_timeline.items())\n",
        "\n",
        "                # Run CUSUM change detection\n",
        "                changes = detect_func(topic_timeline_list)\n",
        "\n",
        "                if changes['change_points']:\n",
        "                    user_has_changes = True\n",
        "                    if user_id not in with_changes:\n",
        "                        with_changes[user_id] = {}\n",
        "\n",
        "                    # Store change points with their stance data\n",
        "                    with_changes[user_id][topic_name] = {\n",
        "                        utt_id: topic_timeline[utt_id]\n",
        "                        for utt_id in changes['change_points']\n",
        "                    }\n",
        "\n",
        "            # Users without any detected changes\n",
        "            if not user_has_changes:\n",
        "                no_changes[user_id] = topic_timelines\n",
        "\n",
        "        # Log summary statistics\n",
        "        self.logger.info(f\"CUSUM Results: {len(with_changes)} users with changes, \"\n",
        "                        f\"{len(no_changes)} users without changes\")\n",
        "        self.logger.info(f\"Total change points detected: {len(self.all_change_points)}\")\n",
        "\n",
        "        return {\n",
        "            'with_changes': with_changes,\n",
        "            'no_changes': no_changes,\n",
        "            'summary': {\n",
        "                'users_with_changes': len(with_changes),\n",
        "                'users_without_changes': len(no_changes),\n",
        "                'total_change_points': len(self.all_change_points),\n",
        "                'detection_parameters': {\n",
        "                    'threshold': self.threshold,\n",
        "                    'drift': self.drift,\n",
        "                    'min_separation': self.min_change_separation,\n",
        "                    'conf_threshold': conf_threshold\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_change_patterns(self, with_changes_data):\n",
        "        \"\"\"Analyze patterns in detected political stance changes.\n",
        "\n",
        "        Args:\n",
        "            with_changes_data: Users with detected changes from get_two_groups()\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing change pattern analysis\n",
        "        \"\"\"\n",
        "        all_changes = []\n",
        "\n",
        "        for user_id, topics in with_changes_data.items():\n",
        "            for topic_name, change_points in topics.items():\n",
        "                for utt_id, stance_data in change_points.items():\n",
        "                    prob_tuple = self._to_probs(stance_data)\n",
        "                    signal = self._get_political_signal(prob_tuple)\n",
        "\n",
        "                    if signal is not None:\n",
        "                        all_changes.append({\n",
        "                            'user_id': user_id,\n",
        "                            'topic': topic_name,\n",
        "                            'utterance_id': utt_id,\n",
        "                            'direction': 'left_shift' if signal < 0 else 'right_shift',\n",
        "                            'magnitude': abs(signal),\n",
        "                            'confidence': self._extract_confidence(stance_data)\n",
        "                        })\n",
        "\n",
        "        if not all_changes:\n",
        "            return {'total_changes': 0}\n",
        "\n",
        "        # Analyze patterns\n",
        "        change_directions = [c['direction'] for c in all_changes]\n",
        "        change_magnitudes = [c['magnitude'] for c in all_changes]\n",
        "        change_confidences = [c['confidence'] for c in all_changes]\n",
        "\n",
        "        direction_counts = Counter(change_directions)\n",
        "\n",
        "        return {\n",
        "            'total_changes': len(all_changes),\n",
        "            'direction_distribution': dict(direction_counts),\n",
        "            'average_magnitude': np.mean(change_magnitudes),\n",
        "            'average_confidence': np.mean(change_confidences),\n",
        "            'left_shifts': direction_counts.get('left_shift', 0),\n",
        "            'right_shifts': direction_counts.get('right_shift', 0),\n",
        "            'most_common_direction': direction_counts.most_common(1)[0] if direction_counts else None\n",
        "        }\n",
        "\n",
        "    def tune_cusum_parameters(self, validation_timeline, known_changes=None):\n",
        "        \"\"\"Tune CUSUM parameters for optimal performance on validation data.\n",
        "\n",
        "        Args:\n",
        "            validation_timeline: Timeline with known change points for tuning\n",
        "            known_changes: List of known change points for comparison\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with optimal parameters and performance metrics\n",
        "        \"\"\"\n",
        "        # Parameter grid for tuning\n",
        "        threshold_values = [1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0]\n",
        "        drift_values = [0.3, 0.5, 0.7, 1.0]\n",
        "\n",
        "        best_params = None\n",
        "        best_score = -1.0\n",
        "        results = []\n",
        "\n",
        "        for threshold in threshold_values:\n",
        "            for drift in drift_values:\n",
        "                # Temporarily set parameters\n",
        "                original_threshold = self.threshold\n",
        "                original_drift = self.drift\n",
        "\n",
        "                self.threshold = threshold\n",
        "                self.drift = drift\n",
        "\n",
        "                # Test detection\n",
        "                detected = self.detect_cusum_changes(validation_timeline)\n",
        "\n",
        "                # Calculate performance metrics\n",
        "                if known_changes:\n",
        "                    precision, recall, f1 = self._calculate_detection_metrics(\n",
        "                        detected['change_points'], known_changes\n",
        "                    )\n",
        "                    score = f1\n",
        "                else:\n",
        "                    # Use change detection rate as proxy metric\n",
        "                    score = len(detected['change_points']) / max(1, len(validation_timeline))\n",
        "\n",
        "                results.append({\n",
        "                    'threshold': threshold,\n",
        "                    'drift': drift,\n",
        "                    'score': score,\n",
        "                    'change_points': len(detected['change_points'])\n",
        "                })\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'threshold': threshold, 'drift': drift}\n",
        "\n",
        "                # Restore original parameters\n",
        "                self.threshold = original_threshold\n",
        "                self.drift = original_drift\n",
        "\n",
        "        # Set best parameters\n",
        "        if best_params:\n",
        "            self.threshold = best_params['threshold']\n",
        "            self.drift = best_params['drift']\n",
        "\n",
        "        self.logger.info(f\"CUSUM tuning complete. Best params: {best_params}, Score: {best_score:.3f}\")\n",
        "\n",
        "        return {\n",
        "            'best_parameters': best_params,\n",
        "            'best_score': best_score,\n",
        "            'all_results': results\n",
        "        }\n",
        "\n",
        "    def _calculate_detection_metrics(self, detected_changes, known_changes):\n",
        "        \"\"\"Calculate precision, recall, and F1 for change detection.\"\"\"\n",
        "        detected_set = set(detected_changes)\n",
        "        known_set = set(known_changes)\n",
        "\n",
        "        true_positives = len(detected_set & known_set)\n",
        "        false_positives = len(detected_set - known_set)\n",
        "        false_negatives = len(known_set - detected_set)\n",
        "\n",
        "        precision = true_positives / max(1, true_positives + false_positives)\n",
        "        recall = true_positives / max(1, true_positives + false_negatives)\n",
        "        f1 = 2 * precision * recall / max(1, precision + recall)\n",
        "\n",
        "        return precision, recall, f1\n",
        "\n",
        "    def get_change_statistics(self):\n",
        "        \"\"\"Get aggregate statistics across all processed timelines.\"\"\"\n",
        "        total_points = len(self.all_change_points) + len(self.all_no_change_points)\n",
        "        change_rate = len(self.all_change_points) / max(1, total_points)\n",
        "\n",
        "        return {\n",
        "            'total_change_points': len(self.all_change_points),\n",
        "            'total_no_change_points': len(self.all_no_change_points),\n",
        "            'overall_change_rate': change_rate,\n",
        "            'detection_parameters': {\n",
        "                'threshold': self.threshold,\n",
        "                'drift': self.drift,\n",
        "                'min_separation': self.min_change_separation\n",
        "            }\n",
        "        }\n",
        "\n",
        "# # Example usage and testing\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Setup logging\n",
        "#     logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "#     # Initialize CUSUM detector\n",
        "#     detector = CUSUMChangeDetector(threshold=3.0, drift=0.5, min_change_separation=3)\n",
        "\n",
        "#     # Example timeline data\n",
        "#     example_timeline = [\n",
        "#         ('post_1', {'pL': 0.8, 'pN': 0.1, 'pR': 0.1, 'confidence': 0.9}),  # left\n",
        "#         ('post_2', {'pL': 0.7, 'pN': 0.2, 'pR': 0.1, 'confidence': 0.8}),  # left\n",
        "#         ('post_3', {'pL': 0.6, 'pN': 0.3, 'pR': 0.1, 'confidence': 0.7}),  # left\n",
        "#         ('post_4', {'pL': 0.2, 'pN': 0.3, 'pR': 0.5, 'confidence': 0.6}),  # uncertain\n",
        "#         ('post_5', {'pL': 0.1, 'pN': 0.2, 'pR': 0.7, 'confidence': 0.8}),  # right - CHANGE!\n",
        "#         ('post_6', {'pL': 0.1, 'pN': 0.1, 'pR': 0.8, 'confidence': 0.9}),  # right\n",
        "#         ('post_7', {'pL': 0.1, 'pN': 0.2, 'pR': 0.7, 'confidence': 0.8}),  # right\n",
        "#     ]\n",
        "\n",
        "#     # Test CUSUM detection\n",
        "#     print(\"Testing CUSUM Change Detection\")\n",
        "#     print(\"=\" * 40)\n",
        "\n",
        "#     # Basic CUSUM\n",
        "#     result_basic = detector.detect_cusum_changes(example_timeline)\n",
        "#     print(f\"Basic CUSUM - Changes: {result_basic['change_points']}\")\n",
        "\n",
        "#     # Advanced CUSUM\n",
        "#     result_advanced = detector.detect_cusum_changes_advanced(example_timeline)\n",
        "#     print(f\"Advanced CUSUM - Changes: {result_advanced['change_points']}\")\n",
        "\n",
        "#     # Example with multiple users\n",
        "#     example_timelines = {\n",
        "#         'user_001': {'politics': dict(example_timeline)},\n",
        "#         'user_002': {'politics': {\n",
        "#             'post_a': 'left-leaning',\n",
        "#             'post_b': 'left-leaning',\n",
        "#             'post_c': 'neutral',\n",
        "#             'post_d': 'right-leaning'  # Simple change\n",
        "#         }}\n",
        "#     }\n",
        "\n",
        "#     # Test group classification\n",
        "#     groups = detector.get_two_groups(example_timelines, conf_threshold=0.6, advanced=True)\n",
        "#     print(f\"\\nGroup Analysis:\")\n",
        "#     print(f\"Users with changes: {list(groups['with_changes'].keys())}\")\n",
        "#     print(f\"Users without changes: {list(groups['no_changes'].keys())}\")\n",
        "\n",
        "#     # Change pattern analysis\n",
        "#     if groups['with_changes']:\n",
        "#         patterns = detector.analyze_change_patterns(groups['with_changes'])\n",
        "#         print(f\"\\nChange Patterns:\")\n",
        "#         print(f\"Total changes detected: {patterns['total_changes']}\")\n",
        "#         print(f\"Direction distribution: {patterns['direction_distribution']}\")\n",
        "#         print(f\"Average confidence: {patterns['average_confidence']:.3f}\")\n",
        "\n",
        "#     # Overall statistics\n",
        "#     stats = detector.get_change_statistics()\n",
        "#     print(f\"\\nOverall Statistics:\")\n",
        "#     print(f\"Change rate: {stats['overall_change_rate']:.3f}\")\n",
        "#     print(f\"Detection threshold: {stats['detection_parameters']['threshold']}\")"
      ],
      "metadata": {
        "id": "Z2SDYDxSSN7v"
      },
      "id": "Z2SDYDxSSN7v",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowExtractor:\n",
        "    \"\"\" Find the conversations around the change point \"\"\"\n",
        "\n",
        "    def __init__(self, corpus, timelines):\n",
        "        self.corpus = corpus\n",
        "        self.timelines = timelines\n",
        "        self.user_conversations_cache = {}  # Add cache\n",
        "\n",
        "    def build_global_user_conversations_index(self):\n",
        "        \"\"\"Build sorted conversations for ALL users upfront\"\"\"\n",
        "        print(\"Building global user conversations index...\")\n",
        "        user_conversations = {}\n",
        "\n",
        "        convos = list(corpus.iter_conversations())\n",
        "        for convo in convos:\n",
        "            # Get all speakers in this conversation\n",
        "            speakers = {utt.speaker.id for utt in convo.iter_utterances()}\n",
        "\n",
        "            # Add this conversation to each speaker's list\n",
        "            for speaker_id in speakers:\n",
        "                if speaker_id not in user_conversations:\n",
        "                    user_conversations[speaker_id] = []\n",
        "                user_conversations[speaker_id].append(convo)\n",
        "\n",
        "        # Sort each user's conversations once\n",
        "        for speaker_id in user_conversations:\n",
        "            user_conversations[speaker_id].sort(\n",
        "                key=lambda convo: min(utt.timestamp for utt in convo.iter_utterances())\n",
        "            )\n",
        "\n",
        "        print(f\"Index built for {len(user_conversations)} users!\")\n",
        "\n",
        "        self.user_conversations_cache = user_conversations\n",
        "\n",
        "    def get_user_conversations_chronological_old(self, corpus, speaker_id):\n",
        "        \"\"\"Get all conversations for a user in chronological order.\"\"\"\n",
        "\n",
        "        # Check cache first\n",
        "        if speaker_id in self.user_conversations_cache:\n",
        "            return self.user_conversations_cache[speaker_id]\n",
        "\n",
        "        # Get all conversations where the speaker participated\n",
        "        user_conversations = [convo for convo in corpus.iter_conversations()\n",
        "                              if speaker_id in [utt.speaker.id for utt in convo.iter_utterances()]]\n",
        "\n",
        "        # Sort conversations by their earliest timestamp\n",
        "        user_conversations.sort(key=lambda convo: min(utt.timestamp for utt in convo.iter_utterances()))\n",
        "\n",
        "        # Cache the result\n",
        "        self.user_conversations_cache[speaker_id] = user_conversations\n",
        "\n",
        "        return user_conversations\n",
        "\n",
        "    def get_user_conversations_chronological(self, corpus, speaker_id):\n",
        "        return self.user_conversations_cache.get(speaker_id, [])\n",
        "\n",
        "    def get_conversations_around_change_point(self, corpus, change_point, test=False, window=10):\n",
        "        # Get first change (probably only one I need)\n",
        "        utterance = corpus.get_utterance(change_point)\n",
        "\n",
        "        # Find the convo this utterance belongs to:\n",
        "        conversation = utterance.get_conversation()\n",
        "\n",
        "        # Put all user's convos in a list\n",
        "        speaker_id = utterance.speaker.id\n",
        "        if test is True:\n",
        "            user_conversations = self.get_user_conversations_chronological_old(corpus, speaker_id)\n",
        "        else:\n",
        "            user_conversations = self.get_user_conversations_chronological(corpus, speaker_id)\n",
        "            print(f\"Cache: {user_conversations}\")\n",
        "\n",
        "        candidate_convos = []\n",
        "        # find the index of the convo, and return the convo id of the 3 prior convos\n",
        "        for i, convo in enumerate(user_conversations):\n",
        "            if conversation.id == user_conversations[i].id:\n",
        "                # Check if there are at least two conversations before the current one\n",
        "                # To this:\n",
        "                if i >= window:\n",
        "                    # Get the 'window' number of conversations before the current one\n",
        "                    candidate_convos.extend(user_conversations[i-10:i])\n",
        "                else:\n",
        "                    # If there are fewer than 10 conversations before, get all of them\n",
        "                    candidate_convos.extend(user_conversations[:i])\n",
        "\n",
        "                # Append the current conversation with the change point\n",
        "                candidate_convos.append(conversation)\n",
        "                break  # Found the conversation, no need to continue the loop\n",
        "\n",
        "        return candidate_convos"
      ],
      "metadata": {
        "id": "PTTNrLLqFp57"
      },
      "id": "PTTNrLLqFp57",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f771600a48d1cc04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7f930f-a350-4df6-a164-77d8e4239296"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "model_directory: ~/.convokit/saved-models\n",
            "default_backend: mem\n"
          ]
        }
      ],
      "execution_count": 10,
      "source": [
        "corpus = Corpus(filename=CORPUS_PATH)"
      ],
      "id": "f771600a48d1cc04"
    },
    {
      "metadata": {
        "id": "683adccaa471a6a1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 11,
      "source": [
        "timeline_builder = TimelineBuilder(corpus)\n",
        "timelines = timeline_builder.build_timelines()"
      ],
      "id": "683adccaa471a6a1"
    },
    {
      "cell_type": "code",
      "source": [
        "# User to test:\n",
        "user_id = \"HardCoreModerate\"\n",
        "topic = \"economic policy\""
      ],
      "metadata": {
        "id": "vRywhDYhtdyt"
      },
      "id": "vRywhDYhtdyt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "change_detector = ChangeDetector()"
      ],
      "metadata": {
        "id": "mEbltsUbUbjf"
      },
      "id": "mEbltsUbUbjf",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "id": "791631c501d996c0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "change_detector = ChangeDetector()\n",
        "topic_timeline = timelines[user_id][topic]\n",
        "topic_timeline_list = list(topic_timeline.items())\n",
        "change_points = change_detector.detect_cusum_changes(topic_timeline_list)['change_points']\n",
        "# print(change_points)"
      ],
      "id": "791631c501d996c0"
    },
    {
      "metadata": {
        "id": "a88d55750fc6e31c"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "window_extractor = WindowExtractor(corpus, timelines)\n",
        "candidate_convos = window_extractor.get_conversations_around_change_point(change_point=change_points[0], corpus=corpus, test=True)\n",
        "print(f\"Candidate convos: {[convo for convo in candidate_convos]}\")"
      ],
      "id": "a88d55750fc6e31c"
    },
    {
      "cell_type": "code",
      "source": [
        "window_extractor.build_global_user_conversations_index()"
      ],
      "metadata": {
        "id": "fDJxwSvUt6Jf"
      },
      "id": "fDJxwSvUt6Jf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class OpPathPairer:\n",
        "    \"\"\" Pair OP utterances with a path of responses by a user/challenger\"\"\"\n",
        "    def __init__(self, corpus, timelines):\n",
        "        self.corpus = corpus\n",
        "        self.timelines = timelines\n",
        "\n",
        "    def _trim_paths(self, op_utterance):\n",
        "        try:\n",
        "            conversation = op_utterance.get_conversation()\n",
        "        except Exception as e:\n",
        "            print(f\"Can't access convo from utterance, error{e}\")\n",
        "\n",
        "        paths = conversation.get_root_to_leaf_paths()\n",
        "\n",
        "        trimmed_paths = []\n",
        "        for path in paths:\n",
        "            if op_utterance in path:\n",
        "                # Find where op_utterance is in this path\n",
        "                op_index = path.index(op_utterance)\n",
        "                # Slice from that index onwards\n",
        "                trimmed_path = path[op_index + 1:]\n",
        "                trimmed_paths.append(trimmed_path)\n",
        "\n",
        "        return trimmed_paths\n",
        "\n",
        "    def _filter_paths(self, trimmed_paths, op_speaker_id):\n",
        "        \"\"\"Filter paths to create rooted path-units, excluding OP utterances\"\"\"\n",
        "        filtered_paths = {}\n",
        "\n",
        "        for path_index, path in enumerate(trimmed_paths):\n",
        "            for utt in path:\n",
        "                # Skip if this utterance is from the OP\n",
        "                if utt.speaker.id == op_speaker_id:\n",
        "                    continue\n",
        "\n",
        "                key = f\"{utt.speaker.id}_path_{path_index}\"\n",
        "                if key not in filtered_paths:\n",
        "                    filtered_paths[key] = []\n",
        "                filtered_paths[key].append(utt)\n",
        "\n",
        "        return filtered_paths\n",
        "\n",
        "    def extract_rooted_paths(self, op_utterance):\n",
        "        trimmed_path = self._trim_paths(op_utterance)\n",
        "        # Pass the OP's speaker ID to filter method\n",
        "        filtered_path = self._filter_paths(trimmed_path, op_utterance.speaker.id)\n",
        "\n",
        "        return filtered_path\n",
        "\n",
        "    # Find the op_utterances from a convo and add them to a list\n",
        "    def extract_op_utterances_from_convo(self, candidate_convo, user_id):\n",
        "        paths = candidate_convo.get_root_to_leaf_paths()\n",
        "        op_utterances = []\n",
        "        for path in paths:\n",
        "            for utt in path:\n",
        "                if utt.speaker.id == user_id and utt not in op_utterances:\n",
        "                    op_utterances.append(utt)\n",
        "                    break\n",
        "\n",
        "        return op_utterances\n",
        "\n",
        "    # Get all op_utterances accross every candidate convo\n",
        "    def extract_op_utterances_from_all_convos(self, candidate_convos, user_id):\n",
        "        all_op_utterances = []\n",
        "        for candidate_convo in candidate_convos:\n",
        "            op_utterances = self.extract_op_utterances_from_convo(candidate_convo, user_id)\n",
        "            all_op_utterances.extend(op_utterances)\n",
        "\n",
        "        return all_op_utterances\n",
        "\n",
        "    # Get the paths of an op_utterance from the op_utterances list\n",
        "    def extract_rooted_path_from_candidate_convos(self, candidate_convos, user_id):\n",
        "        all_op_utterances = self.extract_op_utterances_from_all_convos(candidate_convos, user_id)\n",
        "\n",
        "        # debug:\n",
        "        # for op_utt in all_op_utterances:\n",
        "        #     print(f'my input user_id: {user_id}')\n",
        "        #     speaker_id = self.corpus.get_utterance(op_utt.id).speaker.id\n",
        "        #     print(f'Utt_id: {op_utt.id} and user_id: {speaker_id} in the list of all op utterances.')\n",
        "\n",
        "        all_ops_n_paths = []\n",
        "        for op_utt in all_op_utterances:\n",
        "            # So rooted paths is a dict. Should I convert to list?\n",
        "            rooted_paths = self.extract_rooted_paths(op_utt)\n",
        "\n",
        "            op_n_paths = (op_utt, rooted_paths)\n",
        "            all_ops_n_paths.append(op_n_paths)\n",
        "\n",
        "        return all_ops_n_paths"
      ],
      "metadata": {
        "id": "-dFLALew96UX"
      },
      "id": "-dFLALew96UX",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ce48e0fc8365e82"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 16,
      "source": [
        "# This should be a list of tuples, where the second part is the text of the concatenated utterances of a user\n",
        "op_path_pairer = OpPathPairer(corpus, timelines)\n",
        "# op_path_pairs = op_path_pairer.extract_rooted_path_from_candidate_convos(candidate_convos, user_id)\n",
        "pair_preprocessor = PairPreprocessor()\n",
        "# preprocessed_pairs = pair_preprocessor.concatenate_path_in_all_pairs(op_path_pairs)\n",
        "# print(preprocessed_pairs)\n",
        "\n",
        "# for pair in preprocessed_pairs:\n",
        "#     print(100*'===')\n",
        "#     op = pair[0]\n",
        "#     paths = pair[1]\n",
        "#     print(f\"OP: {op.speaker.id}, Text: {op.text}\")\n",
        "#     for id, text in paths.items():\n",
        "#         print(100*'---')\n",
        "#         print(f\"ID: {id}, Text: {text}\")\n",
        "#     print(100*'§§§§§§')"
      ],
      "id": "1ce48e0fc8365e82"
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of gobal vars and stuff. clean it up."
      ],
      "metadata": {
        "id": "GhllqDEZGCdJ"
      },
      "id": "GhllqDEZGCdJ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_to_leaf_paths = candidate_convos[0].get_root_to_leaf_paths()\n",
        "\n",
        "for path in root_to_leaf_paths:\n",
        "    for utt in path:\n",
        "        print(f\"Speaker: {utt.speaker.id}, Text: {utt.text}\")"
      ],
      "metadata": {
        "id": "0EnjNRw73Gur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "13d70a22-3597-4b10-dbda-fef70d992363"
      },
      "id": "0EnjNRw73Gur",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'candidate_convos' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3761034016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroot_to_leaf_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_convos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_root_to_leaf_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mroot_to_leaf_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mutt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Speaker: {utt.speaker.id}, Text: {utt.text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'candidate_convos' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "395b7d2bc77fbe69"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 15,
      "source": [
        "# Load English stop words\n",
        "stop_words_set = set(stopwords.words('english'))\n",
        "persuasion_analyzer = Interplay()"
      ],
      "id": "395b7d2bc77fbe69"
    },
    {
      "cell_type": "code",
      "source": [
        "# use the groups\n",
        "groups = change_detector.get_two_groups(timelines)\n",
        "groups_tuple = (groups['with_changes'], groups['no_changes'])"
      ],
      "metadata": {
        "id": "4GaoLBn0DTt4"
      },
      "id": "4GaoLBn0DTt4",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlotNHdVF32I",
        "outputId": "831afe9e-5090-4a5c-bc14-d4d929845de8"
      },
      "id": "jlotNHdVF32I",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "QahT4AxeLftK"
      },
      "id": "QahT4AxeLftK",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from convokit import PolitenessStrategies, Utterance, Speaker\n",
        "\n",
        "# Load spaCy model (do this once at the top of your script)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "    raise\n",
        "\n",
        "# Initialize PolitenessStrategies\n",
        "politeness_analyzer = PolitenessStrategies()\n",
        "\n",
        "def get_politeness_features_proper(concatenated_path_text):\n",
        "    \"\"\"\n",
        "    Get politeness strategy scores from challenger's concatenated reply chain\n",
        "    \"\"\"\n",
        "    if len(concatenated_path_text) > 5000:  # Adjust threshold as needed\n",
        "        concatenated_path_text = concatenated_path_text[:1500] + \"...\"\n",
        "\n",
        "    # print('Text:', concatenated_path_text)\n",
        "    # Create a dummy speaker\n",
        "    temp_speaker = Speaker(id=\"temp_speaker\")\n",
        "\n",
        "    # Create utterance with the speaker\n",
        "    temp_utterance = Utterance(\n",
        "        id=\"temp_id\",\n",
        "        speaker=temp_speaker,\n",
        "        text=concatenated_path_text\n",
        "    )\n",
        "\n",
        "    # Transform utterance to get politeness scores\n",
        "    politeness_analyzer.transform_utterance(temp_utterance, spacy_nlp=nlp)\n",
        "\n",
        "    # Access the politeness strategies from the utterance's metadata\n",
        "    politeness_strategies_dict = temp_utterance.meta.get('politeness_strategies', {})\n",
        "\n",
        "    return {\n",
        "        'politeness_gratitude': politeness_strategies_dict.get('feature_politeness_==Gratitude==', 0),\n",
        "        'politeness_apologizing': politeness_strategies_dict.get('feature_politeness_==Apologizing==', 0),\n",
        "        'politeness_please': politeness_strategies_dict.get('feature_politeness_==Please==', 0),\n",
        "        'politeness_indirect_greeting': politeness_strategies_dict.get('feature_politeness_==Indirect_(greeting)==', 0),\n",
        "        'politeness_please_start': politeness_strategies_dict.get('feature_politeness_==Please_start==', 0),\n",
        "        'politeness_hashedge': politeness_strategies_dict.get('feature_politeness_==HASHEDGE==', 0),\n",
        "        'politeness_deference': politeness_strategies_dict.get('feature_politeness_==Deference==', 0),\n",
        "    }"
      ],
      "metadata": {
        "id": "wlZGeyC-_YXY"
      },
      "id": "wlZGeyC-_YXY",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_politeness_features(concatenated_path_text):\n",
        "    \"\"\"Fast regex-based approximation - good enough for thesis analysis\"\"\"\n",
        "    text_lower = concatenated_path_text.lower()\n",
        "\n",
        "    return {\n",
        "        'politeness_gratitude': len(re.findall(r'\\b(thank|thanks|grateful|appreciate|gratitude)\\b', text_lower)),\n",
        "        'politeness_apologizing': len(re.findall(r'\\b(sorry|apolog|excuse me|my bad|my mistake)\\b', text_lower)),\n",
        "        'politeness_please': len(re.findall(r'\\bplease\\b', text_lower)),\n",
        "        'politeness_indirect_greeting': len(re.findall(r'\\b(hello|hi|hey|greetings)\\b', text_lower)),\n",
        "        'politeness_please_start': 1 if re.match(r'^\\s*please\\b', text_lower) else 0,\n",
        "        'politeness_hashedge': len(re.findall(r'\\b(maybe|perhaps|might|could|would|possibly|probably|seems|appears)\\b', text_lower)),\n",
        "        'politeness_deference': len(re.findall(r'\\b(sir|madam|mr\\.|mrs\\.|ms\\.|dr\\.|professor|respectfully)\\b', text_lower)),\n",
        "    }"
      ],
      "metadata": {
        "id": "_4HtZS5lFxSG"
      },
      "id": "_4HtZS5lFxSG",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction functions (return features, not scores)\n",
        "def extract_argument_complexity_features(text):\n",
        "    words = text.split()\n",
        "    sentences = [s for s in text.split('.') if s.strip()]\n",
        "    subordinating = ['because', 'since', 'although', 'while', 'whereas', 'if']\n",
        "\n",
        "    return {\n",
        "        'word_count': len(words),\n",
        "        'unique_words': len(set(words)),\n",
        "        'sentence_count': len(sentences),\n",
        "        'subordinating_count': sum(text.lower().count(word) for word in subordinating)\n",
        "    }\n",
        "\n",
        "def extract_evidence_features(text):\n",
        "    import re\n",
        "    evidence_patterns = [\n",
        "        r'http[s]?://\\S+',\n",
        "        r'according to',\n",
        "        r'research shows',\n",
        "        r'studies indicate',\n",
        "        r'data suggests',\n",
        "        r'statistics show',\n",
        "        r'survey found',\n",
        "        r'report states'\n",
        "    ]\n",
        "\n",
        "    evidence_counts = {}\n",
        "    for i, pattern in enumerate(evidence_patterns):\n",
        "        evidence_counts[f'evidence_type_{i}'] = len(re.findall(pattern, text.lower()))\n",
        "\n",
        "    return evidence_counts\n",
        "\n",
        "def extract_hedging_features(text):\n",
        "    hedges = [\n",
        "        'might', 'could', 'perhaps', 'possibly', 'probably', 'likely',\n",
        "        'seems', 'appears', 'suggests', 'indicates', 'tends to',\n",
        "        'generally', 'usually', 'often', 'sometimes', 'may'\n",
        "    ]\n",
        "\n",
        "    hedge_counts = {}\n",
        "    for hedge in hedges:\n",
        "        hedge_counts[f'hedge_{hedge}'] = text.lower().count(hedge)\n",
        "\n",
        "    return {\n",
        "        'hedge_counts': hedge_counts,\n",
        "        'total_words': len(text.split())\n",
        "    }\n",
        "\n",
        "# Scoring functions (take features, return single score)\n",
        "def calculate_complexity_score(features):\n",
        "    if features['word_count'] == 0:\n",
        "        return 0\n",
        "\n",
        "    lexical_diversity = features['unique_words'] / features['word_count']\n",
        "    avg_sentence_length = features['word_count'] / max(1, features['sentence_count'])\n",
        "    subordinating_ratio = features['subordinating_count'] / features['word_count']\n",
        "\n",
        "    return lexical_diversity + (avg_sentence_length / 100) + subordinating_ratio\n",
        "\n",
        "def calculate_evidence_score(features):\n",
        "    return sum(features.values())\n",
        "\n",
        "def calculate_hedging_score_from_features(features):\n",
        "    total_hedges = sum(features['hedge_counts'].values())\n",
        "    return total_hedges / max(1, features['total_words'])"
      ],
      "metadata": {
        "id": "mZc4iJKgLMp0"
      },
      "id": "mZc4iJKgLMp0",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "668ebf37f0d3de52",
        "outputId": "99f9f01f-6561-47f3-8adf-41fd053f5e90"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing groups:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.013s\n",
            "⏱️ Preprocessing: 0.000s\n",
            "⏱️ Feature extraction (enhanced): 0.003s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.016s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.047s\n",
            "⏱️ Preprocessing: 0.000s\n",
            "⏱️ Feature extraction (enhanced): 0.008s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.056s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.011s\n",
            "⏱️ Preprocessing: 0.000s\n",
            "⏱️ Feature extraction (enhanced): 0.003s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.014s\n",
            "\n",
            "👤 USER seltaeb4 TOTAL: 0.087s (3 change points)\n",
            "📊 Average per change point: 0.029s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.032s\n",
            "⏱️ Preprocessing: 0.002s\n",
            "⏱️ Feature extraction (enhanced): 0.110s\n",
            "⏱️ Scoring: 0.001s\n",
            "🔥 TOTAL for change point: 0.145s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.168s\n",
            "⏱️ Preprocessing: 0.012s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing groups:  50%|█████     | 1/2 [00:00<00:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏱️ Feature extraction (enhanced): 0.377s\n",
            "⏱️ Scoring: 0.001s\n",
            "🔥 TOTAL for change point: 0.558s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.011s\n",
            "⏱️ Preprocessing: 0.001s\n",
            "⏱️ Feature extraction (enhanced): 0.025s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.038s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.042s\n",
            "⏱️ Preprocessing: 0.001s\n",
            "⏱️ Feature extraction (enhanced): 0.021s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.064s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "Skipping conversation 1jcj4v: Conversation failed integrity check. It is either missing an utterance in the reply-to chain and/or has multiple root nodes. Run check_integrity() to diagnose issues.\n",
            "⏱️ Path extraction: 0.026s\n",
            "⏱️ Preprocessing: 0.001s\n",
            "⏱️ Feature extraction (enhanced): 0.020s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.046s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.029s\n",
            "⏱️ Preprocessing: 0.000s\n",
            "⏱️ Feature extraction (enhanced): 0.014s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.043s\n",
            "\n",
            "👤 USER HardCoreModerate TOTAL: 0.895s (6 change points)\n",
            "📊 Average per change point: 0.149s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.000s\n",
            "⏱️ Preprocessing: 0.000s\n",
            "⏱️ Feature extraction (enhanced): 0.002s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.002s\n",
            "\n",
            "👤 USER VegemiteDesu TOTAL: 0.002s (1 change points)\n",
            "📊 Average per change point: 0.002s\n",
            "\n",
            "⏱️ Window extraction: 0.000s\n",
            "⏱️ Path extraction: 0.001s\n",
            "⏱️ Preprocessing: 0.000s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing groups: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏱️ Feature extraction (enhanced): 0.024s\n",
            "⏱️ Scoring: 0.000s\n",
            "🔥 TOTAL for change point: 0.026s\n",
            "\n",
            "👤 USER amade183 TOTAL: 0.026s (1 change points)\n",
            "📊 Average per change point: 0.026s\n",
            "\n",
            "\n",
            "=== GROUP COMPARISON ===\n",
            "\n",
            "Group 1 Means:\n",
            "  interplay: 0.8327\n",
            "  politeness: 2.7514\n",
            "  argument_complexity: 0.8918\n",
            "  evidence_markers: 0.9904\n",
            "  hedging: 0.0044\n",
            "\n",
            "Group 2 Means:\n",
            "  interplay: 0.7555\n",
            "  politeness: 0.6286\n",
            "  argument_complexity: 1.0112\n",
            "  evidence_markers: 0.4095\n",
            "  hedging: 0.0037\n",
            "\n",
            "=== GROUP 1 vs GROUP 2 COMPARISON ===\n",
            "interplay:\n",
            "  Group 1: 0.8327\n",
            "  Group 2: 0.7555\n",
            "  Difference: 0.0772 (+10.2%)\n",
            "\n",
            "politeness:\n",
            "  Group 1: 2.7514\n",
            "  Group 2: 0.6286\n",
            "  Difference: 2.1229 (+337.7%)\n",
            "\n",
            "argument_complexity:\n",
            "  Group 1: 0.8918\n",
            "  Group 2: 1.0112\n",
            "  Difference: -0.1194 (-11.8%)\n",
            "\n",
            "evidence_markers:\n",
            "  Group 1: 0.9904\n",
            "  Group 2: 0.4095\n",
            "  Difference: 0.5809 (+141.9%)\n",
            "\n",
            "hedging:\n",
            "  Group 1: 0.0044\n",
            "  Group 2: 0.0037\n",
            "  Difference: 0.0007 (+18.5%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 52,
      "source": [
        "from tqdm import tqdm\n",
        "from convokit import PolitenessStrategies\n",
        "import re\n",
        "\n",
        "# Convos has been set to test\n",
        "# Init\n",
        "i = 0\n",
        "group_means = [] # Initialize as a list to append means\n",
        "group_scores = []\n",
        "utts_num = 0\n",
        "\n",
        "# For each group\n",
        "for group_idx, group in enumerate(tqdm(groups_tuple, desc=\"Processing groups\")):\n",
        "    # Initialize dictionary for this group's scores\n",
        "    current_group_scores = {\n",
        "        'interplay': [],\n",
        "        'politeness': [],\n",
        "        'argument_complexity': [],\n",
        "        'evidence_markers': [],\n",
        "        'hedging': []\n",
        "    }\n",
        "\n",
        "    count = 0\n",
        "    for user_id, topic_timelines in group.items():\n",
        "        # Process only 2 users for debugging\n",
        "        if count < 2:\n",
        "\n",
        "            user_start_time = time.time()\n",
        "            user_change_points = 0\n",
        "\n",
        "            for topic_timeline in topic_timelines.values():\n",
        "\n",
        "                for change_point in topic_timeline.keys():  # Iterate through change points (keys)\n",
        "                    utts_num += 1\n",
        "\n",
        "                    user_change_points += 1\n",
        "\n",
        "                    # TIME: Window extraction\n",
        "                    start_time = time.time()\n",
        "                    try:\n",
        "                        candidate_convos = window_extractor.get_conversations_around_change_point(\n",
        "                            change_point=change_point, corpus=corpus, test=True\n",
        "                        )\n",
        "                        window_time = time.time() - start_time\n",
        "                        print(f'⏱️ Window extraction: {window_time:.3f}s')\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Skipping change point {change_point}: {e}\")\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    # TIME: Path extraction\n",
        "                    start_time = time.time()\n",
        "                    op_path_pairs = []\n",
        "                    for candidate_convo in candidate_convos:\n",
        "                        try:\n",
        "                            op_path_pairs.extend(op_path_pairer.extract_rooted_path_from_candidate_convos(\n",
        "                                [candidate_convo], user_id\n",
        "                            ))\n",
        "                        except ValueError as e:\n",
        "                            print(f\"Skipping conversation {candidate_convo.id}: {e}\")\n",
        "                            continue\n",
        "                    path_time = time.time() - start_time\n",
        "                    print(f'⏱️ Path extraction: {path_time:.3f}s')\n",
        "\n",
        "\n",
        "                    # TIME: Preprocessing\n",
        "                    start_time = time.time()\n",
        "                    preprocessed_pairs = pair_preprocessor.concatenate_path_in_all_pairs(op_path_pairs)\n",
        "                    preprocess_time = time.time() - start_time\n",
        "                    print(f'⏱️ Preprocessing: {preprocess_time:.3f}s')\n",
        "\n",
        "\n",
        "                    # TIME: Feature extraction (ENHANCED)\n",
        "                    start_time = time.time()\n",
        "                    interplay_features_list = []\n",
        "                    politeness_features_list = []\n",
        "                    # NEW: Feature lists for new predictors\n",
        "                    argument_complexity_features_list = []\n",
        "                    evidence_features_list = []\n",
        "                    hedging_features_list = []\n",
        "\n",
        "                    for op, paths in preprocessed_pairs:\n",
        "                        for k, concatenated_utts in paths.items():\n",
        "                            # Existing feature extraction\n",
        "                            interplay_features = persuasion_analyzer.calculate_interplay_features(\n",
        "                                op.text, concatenated_utts, stop_words_set\n",
        "                            )\n",
        "                            interplay_features_list.append(interplay_features)\n",
        "\n",
        "                            # Fixed politeness feature extraction\n",
        "                            politeness_features = get_politeness_features(concatenated_utts)\n",
        "                            politeness_features_list.append(politeness_features)\n",
        "\n",
        "                            # NEW: Extract features (not scores) for new predictors\n",
        "                            complexity_features = extract_argument_complexity_features(concatenated_utts)\n",
        "                            argument_complexity_features_list.append(complexity_features)\n",
        "\n",
        "                            evidence_features = extract_evidence_features(concatenated_utts)\n",
        "                            evidence_features_list.append(evidence_features)\n",
        "\n",
        "                            hedging_features = extract_hedging_features(concatenated_utts)\n",
        "                            hedging_features_list.append(hedging_features)\n",
        "\n",
        "                    feature_time = time.time() - start_time\n",
        "                    print(f'⏱️ Feature extraction (enhanced): {feature_time:.3f}s')\n",
        "\n",
        "                    # TIME: Score interplay (existing)\n",
        "                    start_time = time.time()\n",
        "                    interplay_scores = []\n",
        "                    for interplay_features in interplay_features_list:\n",
        "                        score = persuasion_analyzer.calculate_persuasion_score(interplay_features)\n",
        "                        interplay_scores.append(score)\n",
        "\n",
        "                    # Score politeness features (existing)\n",
        "                    politeness_scores = []\n",
        "                    for politeness_features in politeness_features_list:\n",
        "                        politeness_total = sum(politeness_features.values())\n",
        "                        politeness_scores.append(politeness_total)\n",
        "\n",
        "                    # NEW: Score the new predictors\n",
        "                    argument_complexity_scores = []\n",
        "                    for complexity_features in argument_complexity_features_list:\n",
        "                        score = calculate_complexity_score(complexity_features)\n",
        "                        argument_complexity_scores.append(score)\n",
        "\n",
        "                    evidence_scores = []\n",
        "                    for evidence_features in evidence_features_list:\n",
        "                        score = calculate_evidence_score(evidence_features)\n",
        "                        evidence_scores.append(score)\n",
        "\n",
        "                    hedging_scores = []\n",
        "                    for hedging_features in hedging_features_list:\n",
        "                        score = calculate_hedging_score_from_features(hedging_features)\n",
        "                        hedging_scores.append(score)\n",
        "\n",
        "                    scoring_time = time.time() - start_time\n",
        "                    print(f'⏱️ Scoring: {scoring_time:.3f}s')\n",
        "\n",
        "                    # Add all scores to current group (NEW STRUCTURE)\n",
        "                    current_group_scores['interplay'].extend(interplay_scores)\n",
        "                    current_group_scores['politeness'].extend(politeness_scores)\n",
        "                    current_group_scores['argument_complexity'].extend(argument_complexity_scores)\n",
        "                    current_group_scores['evidence_markers'].extend(evidence_scores)\n",
        "                    current_group_scores['hedging'].extend(hedging_scores)\n",
        "\n",
        "                    # Print total time for this change point\n",
        "                    total_time = window_time + path_time + preprocess_time + feature_time + scoring_time\n",
        "                    print(f'🔥 TOTAL for change point: {total_time:.3f}s\\n')\n",
        "                    break\n",
        "\n",
        "            # TIME: End timing this user\n",
        "            user_total_time = time.time() - user_start_time\n",
        "            print(f'👤 USER {user_id} TOTAL: {user_total_time:.3f}s ({user_change_points} change points)')\n",
        "            print(f'📊 Average per change point: {user_total_time/max(1, user_change_points):.3f}s\\n')\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        if count >= 10:\n",
        "            break\n",
        "\n",
        "    # Calculate means for each predictor for this group (ENHANCED)\n",
        "    group_mean = {}\n",
        "    for predictor_name, scores in current_group_scores.items():\n",
        "        if scores:  # Check if we have scores\n",
        "            group_mean[predictor_name] = sum(scores) / len(scores)\n",
        "        else:\n",
        "            group_mean[predictor_name] = 0\n",
        "\n",
        "    # Append this group's means\n",
        "    group_means.append(group_mean)\n",
        "    group_scores.append(current_group_scores)\n",
        "\n",
        "# Print the calculated group means for each predictor (ENHANCED)\n",
        "print(f'\\n=== GROUP COMPARISON ===')\n",
        "for group_idx, group_mean in enumerate(group_means):\n",
        "    print(f'\\nGroup {group_idx + 1} Means:')\n",
        "    for predictor, mean_score in group_mean.items():\n",
        "        print(f'  {predictor}: {mean_score:.4f}')\n",
        "\n",
        "# Print comparison between groups\n",
        "if len(group_means) >= 2:\n",
        "    print(f'\\n=== GROUP 1 vs GROUP 2 COMPARISON ===')\n",
        "    for predictor in group_means[0].keys():\n",
        "        group1_mean = group_means[0][predictor]\n",
        "        group2_mean = group_means[1][predictor]\n",
        "        difference = group1_mean - group2_mean\n",
        "        percent_diff = (difference / group2_mean * 100) if group2_mean != 0 else 0\n",
        "        print(f'{predictor}:')\n",
        "        print(f'  Group 1: {group1_mean:.4f}')\n",
        "        print(f'  Group 2: {group2_mean:.4f}')\n",
        "        print(f'  Difference: {difference:.4f} ({percent_diff:+.1f}%)')\n",
        "        print()"
      ],
      "id": "668ebf37f0d3de52"
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== NEW: STATISTICAL ANALYSIS SECTION ==========\n",
        "\n",
        "def perform_statistical_tests(group1_scores, group2_scores, predictor_name, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Perform comprehensive statistical tests between two groups for a given predictor.\n",
        "\n",
        "    Args:\n",
        "        group1_scores: List of scores for group 1\n",
        "        group2_scores: List of scores for group 2\n",
        "        predictor_name: Name of the predictor being tested\n",
        "        alpha: Significance level (default 0.05)\n",
        "\n",
        "    Returns:\n",
        "        dict: Results of all statistical tests\n",
        "    \"\"\"\n",
        "    if not group1_scores or not group2_scores:\n",
        "        return {\n",
        "            'valid': False,\n",
        "            'reason': 'Empty score lists'\n",
        "        }\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    g1 = np.array(group1_scores)\n",
        "    g2 = np.array(group2_scores)\n",
        "\n",
        "    # Basic descriptive statistics\n",
        "    results = {\n",
        "        'valid': True,\n",
        "        'predictor': predictor_name,\n",
        "        'group1_n': len(g1),\n",
        "        'group2_n': len(g2),\n",
        "        'group1_mean': np.mean(g1),\n",
        "        'group2_mean': np.mean(g2),\n",
        "        'group1_std': np.std(g1, ddof=1),\n",
        "        'group2_std': np.std(g2, ddof=1),\n",
        "        'group1_median': np.median(g1),\n",
        "        'group2_median': np.median(g2),\n",
        "        'mean_difference': np.mean(g1) - np.mean(g2),\n",
        "    }\n",
        "\n",
        "    # Effect size (Cohen's d)\n",
        "    pooled_std = np.sqrt(((len(g1) - 1) * results['group1_std']**2 +\n",
        "                         (len(g2) - 1) * results['group2_std']**2) /\n",
        "                        (len(g1) + len(g2) - 2))\n",
        "    results['cohens_d'] = results['mean_difference'] / pooled_std if pooled_std > 0 else 0\n",
        "\n",
        "    # Interpret effect size\n",
        "    abs_d = abs(results['cohens_d'])\n",
        "    if abs_d < 0.2:\n",
        "        effect_size_interpretation = \"negligible\"\n",
        "    elif abs_d < 0.5:\n",
        "        effect_size_interpretation = \"small\"\n",
        "    elif abs_d < 0.8:\n",
        "        effect_size_interpretation = \"medium\"\n",
        "    else:\n",
        "        effect_size_interpretation = \"large\"\n",
        "    results['effect_size_interpretation'] = effect_size_interpretation\n",
        "\n",
        "    # Test for equal variances (Levene's test)\n",
        "    try:\n",
        "        levene_stat, levene_p = levene(g1, g2)\n",
        "        results['levene_statistic'] = levene_stat\n",
        "        results['levene_p_value'] = levene_p\n",
        "        results['equal_variances'] = levene_p > alpha\n",
        "    except Exception as e:\n",
        "        results['levene_error'] = str(e)\n",
        "        results['equal_variances'] = True  # Assume equal variances if test fails\n",
        "\n",
        "    # Independent samples t-test\n",
        "    try:\n",
        "        # Use equal_var parameter based on Levene's test\n",
        "        equal_var = results.get('equal_variances', True)\n",
        "        t_stat, t_p = ttest_ind(g1, g2, equal_var=equal_var)\n",
        "        results['t_statistic'] = t_stat\n",
        "        results['t_p_value'] = t_p\n",
        "        results['t_significant'] = t_p < alpha\n",
        "\n",
        "        # Calculate degrees of freedom\n",
        "        if equal_var:\n",
        "            results['t_df'] = len(g1) + len(g2) - 2\n",
        "        else:\n",
        "            # Welch's t-test degrees of freedom\n",
        "            s1_sq, s2_sq = results['group1_std']**2, results['group2_std']**2\n",
        "            n1, n2 = len(g1), len(g2)\n",
        "            results['t_df'] = ((s1_sq/n1 + s2_sq/n2)**2) / ((s1_sq/n1)**2/(n1-1) + (s2_sq/n2)**2/(n2-1))\n",
        "\n",
        "    except Exception as e:\n",
        "        results['t_test_error'] = str(e)\n",
        "\n",
        "    # Mann-Whitney U test (non-parametric alternative)\n",
        "    try:\n",
        "        u_stat, u_p = mannwhitneyu(g1, g2, alternative='two-sided')\n",
        "        results['mannwhitney_u_statistic'] = u_stat\n",
        "        results['mannwhitney_p_value'] = u_p\n",
        "        results['mannwhitney_significant'] = u_p < alpha\n",
        "    except Exception as e:\n",
        "        results['mannwhitney_error'] = str(e)\n",
        "\n",
        "    # 95% Confidence interval for the difference in means\n",
        "    try:\n",
        "        # Pooled standard error\n",
        "        n1, n2 = len(g1), len(g2)\n",
        "        pooled_se = pooled_std * np.sqrt(1/n1 + 1/n2)\n",
        "\n",
        "        # Critical t-value\n",
        "        df = results.get('t_df', n1 + n2 - 2)\n",
        "        t_critical = stats.t.ppf(1 - alpha/2, df)\n",
        "\n",
        "        # Confidence interval\n",
        "        margin_of_error = t_critical * pooled_se\n",
        "        results['ci_lower'] = results['mean_difference'] - margin_of_error\n",
        "        results['ci_upper'] = results['mean_difference'] + margin_of_error\n",
        "    except Exception as e:\n",
        "        results['ci_error'] = str(e)\n",
        "\n",
        "    return results\n",
        "\n",
        "def format_statistical_results(results):\n",
        "    \"\"\"Format statistical results for readable output.\"\"\"\n",
        "    if not results['valid']:\n",
        "        return f\"❌ {results['predictor']}: {results['reason']}\"\n",
        "\n",
        "    output = f\"\\n📊 {results['predictor'].upper().replace('_', ' ')} ANALYSIS:\\n\"\n",
        "    output += f\"{'='*50}\\n\"\n",
        "\n",
        "    # Descriptive statistics\n",
        "    output += f\"Sample sizes: Group 1: n={results['group1_n']}, Group 2: n={results['group2_n']}\\n\"\n",
        "    output += f\"Group 1: M={results['group1_mean']:.4f} (SD={results['group1_std']:.4f}), Mdn={results['group1_median']:.4f}\\n\"\n",
        "    output += f\"Group 2: M={results['group2_mean']:.4f} (SD={results['group2_std']:.4f}), Mdn={results['group2_median']:.4f}\\n\"\n",
        "    output += f\"Mean difference: {results['mean_difference']:.4f}\\n\"\n",
        "\n",
        "    # Effect size\n",
        "    output += f\"Effect size (Cohen's d): {results['cohens_d']:.4f} ({results['effect_size_interpretation']})\\n\"\n",
        "\n",
        "    # Variance equality test\n",
        "    if 'levene_p_value' in results:\n",
        "        equal_var_str = \"✅ Equal\" if results['equal_variances'] else \"❌ Unequal\"\n",
        "        output += f\"Levene's test: F={results['levene_statistic']:.4f}, p={results['levene_p_value']:.4f} ({equal_var_str} variances)\\n\"\n",
        "\n",
        "    # t-test results\n",
        "    if 't_p_value' in results:\n",
        "        significance = \"✅ SIGNIFICANT\" if results['t_significant'] else \"❌ Not significant\"\n",
        "        test_type = \"Welch's t-test\" if not results.get('equal_variances', True) else \"Student's t-test\"\n",
        "        output += f\"{test_type}: t({results['t_df']:.1f})={results['t_statistic']:.4f}, p={results['t_p_value']:.4f} {significance}\\n\"\n",
        "\n",
        "    # Mann-Whitney U test\n",
        "    if 'mannwhitney_p_value' in results:\n",
        "        significance = \"✅ SIGNIFICANT\" if results['mannwhitney_significant'] else \"❌ Not significant\"\n",
        "        output += f\"Mann-Whitney U: U={results['mannwhitney_u_statistic']:.1f}, p={results['mannwhitney_p_value']:.4f} {significance}\\n\"\n",
        "\n",
        "    # Confidence interval\n",
        "    if 'ci_lower' in results:\n",
        "        output += f\"95% CI for difference: [{results['ci_lower']:.4f}, {results['ci_upper']:.4f}]\\n\"\n",
        "\n",
        "    return output\n",
        "\n",
        "# Print the calculated group means for each predictor (ENHANCED)\n",
        "print(f'\\n=== GROUP COMPARISON ===')\n",
        "for group_idx, group_mean in enumerate(group_means):\n",
        "    print(f'\\nGroup {group_idx + 1} Means:')\n",
        "    for predictor, mean_score in group_mean.items():\n",
        "        print(f'  {predictor}: {mean_score:.4f}')\n",
        "\n",
        "# Print comparison between groups\n",
        "if len(group_means) >= 2:\n",
        "    print(f'\\n=== GROUP 1 vs GROUP 2 COMPARISON ===')\n",
        "    for predictor in group_means[0].keys():\n",
        "        group1_mean = group_means[0][predictor]\n",
        "        group2_mean = group_means[1][predictor]\n",
        "        difference = group1_mean - group2_mean\n",
        "        percent_diff = (difference / group2_mean * 100) if group2_mean != 0 else 0\n",
        "        print(f'{predictor}:')\n",
        "        print(f'  Group 1: {group1_mean:.4f}')\n",
        "        print(f'  Group 2: {group2_mean:.4f}')\n",
        "        print(f'  Difference: {difference:.4f} ({percent_diff:+.1f}%)')\n",
        "        print()\n",
        "\n",
        "# ========== NEW: COMPREHENSIVE STATISTICAL ANALYSIS ==========\n",
        "\n",
        "if len(group_scores) >= 2:\n",
        "    print(f'\\n🔬 STATISTICAL SIGNIFICANCE TESTING')\n",
        "    print(f'=' * 60)\n",
        "\n",
        "    # Store all test results for summary\n",
        "    all_test_results = []\n",
        "    significant_predictors = []\n",
        "\n",
        "    # Test each predictor\n",
        "    for predictor in group_scores[0].keys():\n",
        "        group1_scores = group_scores[0][predictor]\n",
        "        group2_scores = group_scores[1][predictor]\n",
        "\n",
        "        # Perform statistical tests\n",
        "        test_results = perform_statistical_tests(group1_scores, group2_scores, predictor)\n",
        "        all_test_results.append(test_results)\n",
        "\n",
        "        # Print formatted results\n",
        "        print(format_statistical_results(test_results))\n",
        "\n",
        "        # Track significant predictors\n",
        "        if test_results.get('t_significant', False):\n",
        "            significant_predictors.append(predictor)\n",
        "\n",
        "    # Multiple comparison correction (Bonferroni)\n",
        "    print(f'\\n🎯 MULTIPLE COMPARISON CORRECTION')\n",
        "    print(f'=' * 40)\n",
        "    n_tests = len([r for r in all_test_results if r['valid']])\n",
        "    bonferroni_alpha = 0.05 / n_tests if n_tests > 0 else 0.05\n",
        "    print(f\"Number of tests: {n_tests}\")\n",
        "    print(f\"Bonferroni-corrected α: {bonferroni_alpha:.4f}\")\n",
        "\n",
        "    bonferroni_significant = []\n",
        "    for result in all_test_results:\n",
        "        if result['valid'] and 't_p_value' in result:\n",
        "            is_significant = result['t_p_value'] < bonferroni_alpha\n",
        "            status = \"✅ SIGNIFICANT\" if is_significant else \"❌ Not significant\"\n",
        "            print(f\"{result['predictor']}: p={result['t_p_value']:.4f} {status}\")\n",
        "            if is_significant:\n",
        "                bonferroni_significant.append(result['predictor'])\n",
        "\n",
        "    # Summary of findings\n",
        "    print(f'\\n📋 SUMMARY OF FINDINGS')\n",
        "    print(f'=' * 30)\n",
        "    print(f\"Total predictors tested: {n_tests}\")\n",
        "    print(f\"Significant at α=0.05: {len(significant_predictors)} ({len(significant_predictors)/n_tests*100:.1f}%)\")\n",
        "    print(f\"Significant after Bonferroni correction: {len(bonferroni_significant)} ({len(bonferroni_significant)/n_tests*100:.1f}%)\")\n",
        "\n",
        "    if significant_predictors:\n",
        "        print(f\"\\nSignificant predictors (uncorrected): {', '.join(significant_predictors)}\")\n",
        "    if bonferroni_significant:\n",
        "        print(f\"Significant predictors (Bonferroni): {', '.join(bonferroni_significant)}\")\n",
        "\n",
        "    # Effect size summary\n",
        "    print(f'\\n📏 EFFECT SIZES SUMMARY')\n",
        "    print(f'=' * 25)\n",
        "    for result in all_test_results:\n",
        "        if result['valid']:\n",
        "            direction = \"Group 1 > Group 2\" if result['mean_difference'] > 0 else \"Group 2 > Group 1\"\n",
        "            print(f\"{result['predictor']}: d={result['cohens_d']:.3f} ({result['effect_size_interpretation']}, {direction})\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️  Need at least 2 groups for statistical comparison\")\n",
        "\n",
        "print(f'\\n🏁 Analysis completed. Total utterances processed: {utts_num}')"
      ],
      "metadata": {
        "id": "-01zn5RzLYD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7332b6af-ade9-4bc4-dc05-aa9c583c805c"
      },
      "id": "-01zn5RzLYD1",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GROUP COMPARISON ===\n",
            "\n",
            "Group 1 Means:\n",
            "  interplay: 0.8327\n",
            "  politeness: 0.7008\n",
            "  argument_complexity: 0.8918\n",
            "  evidence_markers: 0.9904\n",
            "  hedging: 0.0044\n",
            "\n",
            "Group 2 Means:\n",
            "  interplay: 0.7555\n",
            "  politeness: 0.4190\n",
            "  argument_complexity: 1.0112\n",
            "  evidence_markers: 0.4095\n",
            "  hedging: 0.0037\n",
            "\n",
            "=== GROUP 1 vs GROUP 2 COMPARISON ===\n",
            "interplay:\n",
            "  Group 1: 0.8327\n",
            "  Group 2: 0.7555\n",
            "  Difference: 0.0772 (+10.2%)\n",
            "\n",
            "politeness:\n",
            "  Group 1: 0.7008\n",
            "  Group 2: 0.4190\n",
            "  Difference: 0.2817 (+67.2%)\n",
            "\n",
            "argument_complexity:\n",
            "  Group 1: 0.8918\n",
            "  Group 2: 1.0112\n",
            "  Difference: -0.1194 (-11.8%)\n",
            "\n",
            "evidence_markers:\n",
            "  Group 1: 0.9904\n",
            "  Group 2: 0.4095\n",
            "  Difference: 0.5809 (+141.9%)\n",
            "\n",
            "hedging:\n",
            "  Group 1: 0.0044\n",
            "  Group 2: 0.0037\n",
            "  Difference: 0.0007 (+18.5%)\n",
            "\n",
            "\n",
            "🔬 STATISTICAL SIGNIFICANCE TESTING\n",
            "============================================================\n",
            "\n",
            "📊 INTERPLAY ANALYSIS:\n",
            "==================================================\n",
            "Sample sizes: Group 1: n=1046, Group 2: n=105\n",
            "Group 1: M=0.8327 (SD=0.0828), Mdn=0.8226\n",
            "Group 2: M=0.7555 (SD=0.0779), Mdn=0.7694\n",
            "Mean difference: 0.0772\n",
            "Effect size (Cohen's d): 0.9369 (large)\n",
            "\n",
            "\n",
            "📊 POLITENESS ANALYSIS:\n",
            "==================================================\n",
            "Sample sizes: Group 1: n=1046, Group 2: n=105\n",
            "Group 1: M=0.7008 (SD=0.6410), Mdn=1.0000\n",
            "Group 2: M=0.4190 (SD=0.5509), Mdn=0.0000\n",
            "Mean difference: 0.2817\n",
            "Effect size (Cohen's d): 0.4448 (small)\n",
            "\n",
            "\n",
            "📊 ARGUMENT COMPLEXITY ANALYSIS:\n",
            "==================================================\n",
            "Sample sizes: Group 1: n=1046, Group 2: n=105\n",
            "Group 1: M=0.8918 (SD=0.2127), Mdn=0.9500\n",
            "Group 2: M=1.0112 (SD=0.0994), Mdn=1.0200\n",
            "Mean difference: -0.1194\n",
            "Effect size (Cohen's d): -0.5825 (medium)\n",
            "\n",
            "\n",
            "📊 EVIDENCE MARKERS ANALYSIS:\n",
            "==================================================\n",
            "Sample sizes: Group 1: n=1046, Group 2: n=105\n",
            "Group 1: M=0.9904 (SD=2.2723), Mdn=0.0000\n",
            "Group 2: M=0.4095 (SD=0.8626), Mdn=0.0000\n",
            "Mean difference: 0.5809\n",
            "Effect size (Cohen's d): 0.2662 (small)\n",
            "\n",
            "\n",
            "📊 HEDGING ANALYSIS:\n",
            "==================================================\n",
            "Sample sizes: Group 1: n=1046, Group 2: n=105\n",
            "Group 1: M=0.0044 (SD=0.0130), Mdn=0.0000\n",
            "Group 2: M=0.0037 (SD=0.0076), Mdn=0.0000\n",
            "Mean difference: 0.0007\n",
            "Effect size (Cohen's d): 0.0545 (negligible)\n",
            "\n",
            "\n",
            "🎯 MULTIPLE COMPARISON CORRECTION\n",
            "========================================\n",
            "Number of tests: 5\n",
            "Bonferroni-corrected α: 0.0100\n",
            "\n",
            "📋 SUMMARY OF FINDINGS\n",
            "==============================\n",
            "Total predictors tested: 5\n",
            "Significant at α=0.05: 0 (0.0%)\n",
            "Significant after Bonferroni correction: 0 (0.0%)\n",
            "\n",
            "📏 EFFECT SIZES SUMMARY\n",
            "=========================\n",
            "interplay: d=0.937 (large, Group 1 > Group 2)\n",
            "politeness: d=0.445 (small, Group 1 > Group 2)\n",
            "argument_complexity: d=-0.583 (medium, Group 2 > Group 1)\n",
            "evidence_markers: d=0.266 (small, Group 1 > Group 2)\n",
            "hedging: d=0.055 (negligible, Group 1 > Group 2)\n",
            "\n",
            "🏁 Analysis completed. Total utterances processed: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bIXojYlryRqw"
      },
      "id": "bIXojYlryRqw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}